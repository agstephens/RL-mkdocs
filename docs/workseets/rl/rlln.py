'''
    this library implement a *linear function approximation* for well-known 
    RL algorithms. It works by inheriting from the classes in the 
    rl.tabular library. We added v prefix to the MRP and MDP base classes to 
    differentiate them from their ancestor but we could have kept the same names.
    As usual we start by defining an MRP class for prediction, then MDP for control
    then make other rl algorithms inherit forn them as needed.
'''

from rl.rl import *
from env.gridln import *
from env.mountainln import *

from math import floor
# ======================================= prediction master class==========================================
class vMRP(MRP):
        
    # set up the weights, must be done whenever we train
    def init(self):
        self.w = np.ones(self.env.nF)*self.v0
        self.V = self.V_ # this allows us to use a very similar syntax for our updates
        self.S_= None
        
    #-------------------------------------------buffer related-------------------------------------
    # allocate a suitable buffer
    def allocate(self): 
        super().allocate()
        self.s = np.ones ((self.max_t, self.env.nF), dtype=np.uint32) *(self.env.nS+10)    
    
    #---------------------------------------- retrieve Vs ------------------------------------------
    def V_(self, s=None):
        return self.w.dot(s) if s is not None else self.w.dot(self.env.S_()) 
        
    def Î”V(self,s): # gradient: we should have used âˆ‡ but jupyter does not like it
        return s


# ======================================= prediction algorithms==========================================
class MC(vMRP):
    def __init__(self,  **kw):
        super().__init__(**kw)
        self.store = True 
        
    def init(self):
        super().init() # this is needed to bring w to the scope of the child class
        self.store = True 
        
    # ----------------------------- ðŸŒ˜ offline, MC learning: end-of-episode learning ----------------------    
    def offline(self):
        # obtain the return for the latest episode
        Gt = 0
        for t in range(self.t, -1, -1):
            s = self.s[t]
            rn = self.r[t+1]
            
            Gt = self.Î³*Gt + rn
            self.w += self.Î±*(Gt - self.V(s))*self.Î”V(s)

class TDf(vMRP):
    def init(self):
        super().init()
        self.store = True        
    # ----------------------------- ðŸŒ˜ offline TD learning ----------------------------   
    def offline(self):
        for t in range(self.t, -1, -1):
            s = self.s[t]
            sn = self.s[t+1]
            rn = self.r[t+1]
            done = self.done[t+1]
            
            self.w += self.Î±*(rn + (1-done)*self.Î³*self.V(sn) - self.V(s))*self.Î”V(s)

class TD(vMRP):
    # ----------------------------- ðŸŒ– online learning ----------------------    
    def online(self, s, rn,sn, done, *args): 
        self.w += self.Î±*(rn + (1-done)*self.Î³*self.V(sn) - self.V(s))*self.Î”V(s)

class TDn(vMRP):

    def init(self):
        super().init()
        self.store = True # there is a way to save storage by using t%(self.n+1) but we left it for clarity

    # ----------------------------- ðŸŒ– online learning ----------------------    
    def online(self,*args):
        Ï„ = self.t - (self.n-1);  n=self.n
        if Ï„<0: return
        
        # we take the min so that we do not exceed the episode limit (last step+1)
        Ï„n = Ï„+n ; Ï„n=min(Ï„n, self.t+1 - self.skipstep)
        Ï„1 = Ï„+1
        
        sÏ„ = self.s[Ï„ ]
        sn = self.s[Ï„n]
        done = self.done[Ï„n]
        
        # n steps Ï„+1,..., Ï„+n inclusive of both ends
        self.w += self.Î±*(self.G(Ï„1,Ï„n)+ (1-done)*self.Î³**n *self.V(sn) - self.V(sÏ„))*self.Î”V(sÏ„)
        
class TDnf(vMRP):

    def init(self):
        super().init()
        self.store = True # offline method we need to store anyway

    # ----------------------------- ðŸŒ˜ offline TD learning ----------------------------   
    def offline(self):
        n=self.n        
        for t in range(self.t+n): # T+n to reach T+n-1
            Ï„  = t - (n-1)
            if Ï„<0: continue
        
            # we take the min so that we do not exceed the episode limit (last step+1)
            Ï„1 = Ï„+1
            Ï„n = Ï„+n ; Ï„n=min(Ï„n, self.t+1)
            
            sÏ„ = self.s[Ï„ ]
            sn = self.s[Ï„n]
            done = self.done[Ï„n]
            
            # n steps Ï„+1,..., Ï„+n inclusive of both ends
            self.w += self.Î±*(self.G(Ï„1,Ï„n)+ (1-done)*self.Î³**n *self.V(sn) - self.V(sÏ„))*self.Î”V(sÏ„)


# ======================================= control master class==========================================

class vMDP(MDP(vMRP)):

    def init(self):
        super().init()
        self.W = np.ones((self.env.nA, self.env.nF))*self.q0
        self.Q = self.Q_

    def Q_(self, s=None, a=None):
        #print(s.shape)
        W = self.W if a is None else self.W[a]
        return W.dot(s) if s is not None else np.matmul(W, self.env.S_()).T 

    # we should have used âˆ‡ but python does not like it
    def Î”Q(self,s): 
        return s

# ======================================= control algorithms===================================
class MCC(vMDP):
    def init(self):
        super().init()
        self.store = True
    # ---------------------------- ðŸŒ˜ offline, MC learning: end-of-episode learning-------------    
    def offline(self):  
        # obtain the return for the latest episode
        Gt = 0
        for t in range(self.t, -1, -1):
            s = self.s[t]
            a = self.a[t]
            rn = self.r[t+1]
            
            Gt = self.Î³*Gt + rn
            self.W[a] += self.Î±*(Gt - self.Q(s,a))*self.Î”Q(s)

# -------------------------------------ðŸŒ– Sarsa online learning ----------------------------------
class Sarsa(vMDP):
    def init(self): #Î±=.8
        super().init()
        self.step = self.step_an # for Sarsa we want to decide the next action in time step t

    def online(self, s, rn,sn, done, a,an):
        self.W[a] += self.Î±*(rn + (1-done)*self.Î³*self.Q(sn,an) - self.Q(s,a))*self.Î”Q(s)
 
#--------------------------------------ðŸŒ– Q-learning online --------------------------------------
class Qlearn(vMDP):
    def online(self, s, rn,sn, done, a,_):
        self.W[a] += self.Î±*(rn + (1-done)*self.Î³*self.Q(sn).max() - self.Q(s,a))*self.Î”Q(s)
    
# --------------------- ðŸŒ– XSarsa (value function) online learning ------------------------------------
class XSarsa(vMDP):
    def online(self, s, rn,sn, done, a,_):      
        # obtain the Îµ-greedy policy probabilities, then obtain the expecation via a dot product for efficiency
        Ï€ = self.Ï€(sn)
        v = self.Q(sn).dot(Ï€)
        self.W[a] += self.Î±*(rn + (1-done)*self.Î³*v - self.Q(s,a))*self.Î”Q(s)

# ---------------------- ðŸŒ– Actor-Critic (policy Gradient) online learning ----------------------------
class Actor_Critic(PG(vMDP)):
    def step0(self):
        self.Î³t = 1 # powers of Î³

    def online(self, s, rn,sn, done, a,_): 
        Ï€, Î³, Î³t, Î±, Ï„, t, Î”V, Î”Q = self.Ï€, self.Î³, self.Î³t, self.Î±, self.Ï„, self.t, self.Î”V, self.Î”Q
        
        Î´ = (1- done)*Î³*self.V(sn) + rn - self.V(s)    # TD error is based on the critic estimate
        
        self.w    += Î±*Î´*Î”V(s)                         # critic
        self.W[a] += Î±*Î´*Î”Q(s)*(1 - Ï€(s,a))*Î³t/Ï„       # actor
        self.Î³t *= Î³  

# ------------------------ ðŸŒ– multi-step (value function) online learning -----------------------------      
class Sarsan(vMDP):
    def init(self):
        super().init()
        self.store = True        # although online but we need to access *some* of earlier steps,
        self.step = self.step_an # for Sarsa we want to decide the next action in time step t
      
    def online(self, *args):
        Ï„ = self.t - (self.n-1);  n=self.n
        if Ï„<0: return
        
        # we take the min so that we do not exceed the episode limit (last step+1)
        Ï„1 = Ï„+1
        Ï„n = Ï„+n ; Ï„n=min(Ï„n, self.t+1 - self.skipstep)
        
        sÏ„ = self.s[Ï„];  aÏ„ = self.a[Ï„]
        sn = self.s[Ï„n]; an = self.a[Ï„n]
        done = self.done[Ï„n]
        
        # n steps Ï„+1,..., Ï„+n inclusive of both ends
        self.W[aÏ„] += self.Î±*(self.G(Ï„1,Ï„n) + (1-done)*self.Î³**n *self.Q(sn,an) - self.Q(sÏ„,aÏ„))*self.Î”Q(sÏ„)

# ------------------------ ðŸŒ– multi-step (value function prediction) online learning -----------------------   
class TDÎ»(vMRP):
    def __init__(self, Î»=.5, **kw):
        super().__init__(**kw)
        self.Î» = Î»
    def step0(self):
        self.z = self.w*0
    
    def online(self, s, rn,sn, done, *args): 
        Î±, Î³, Î» = self.Î±, self.Î³, self.Î»
        self.z = Î»*Î³*self.z + self.Î”V(s)
        self.w += Î±*(rn + (1-done)*Î³*self.V(sn) - self.V(s))*self.z
    
# ------------------------ ðŸŒ– multi-step (value function prediction) online learning -----------------------
class trueTDÎ»(vMRP):
    def __init__(self, Î»=.5, **kw):
        super().__init__(**kw)
        self.Î» = Î»

    def step0(self):
        self.z = self.w*0
        self.vo = 0
  
    def online(self, s, rn,sn, done, *args): 
        Î±, Î³, Î» = self.Î±, self.Î³, self.Î»
        
        self.v = self.V(s)
        self.vn= self.V(sn)*(1-done)
        Î´ = rn + Î³*self.vn - self.v
        self.z = Î»*Î³*self.z + (1-Î±*Î»*Î³*self.z.dot(s))*s
        
        self.w += Î±*(Î´ + self.v - self.vo )*self.z - Î±*(self.v - self.vo)*s
        self.vo = self.vn
    
# ------------------------ ðŸŒ– multi-step (value function control) online learning -----------------------
class SarsaÎ»(vMDP):
    def __init__(self, Î»=.5, **kw):
        super().__init__(**kw)
        self.Î» = Î»
        self.step = self.step_an # for Sarsa we want to decide the next action in time step t
    
    def step0(self):
        self.Z = self.W*0

    def online(self, s, rn,sn, done, a,an):
        self.Z[a] = self.Î»*self.Î³*self.Z[a] + self.Î”Q(s)
        self.W[a] += self.Î±*(rn + (1-done)*self.Î³*self.Q(sn,an)- self.Q(s,a))*self.Z[a]

# ------------------------ ðŸŒ– multi-step, value function control, online learning -----------------------
class trueSarsaÎ»(vMDP):
    def __init__(self, Î»=.5, **kw):
        super().__init__(**kw)
        self.Î» = Î»
        self.step = self.step_an # for Sarsa we want to decide the next action in time step t
    def step0(self):
        self.Z = self.W*0
        self.qo = 0
    # --------ðŸŒ– online learning ----------
    def online(self, s, rn,sn, done, a,an):
        Î±, Î³, Î» = self.Î±, self.Î³, self.Î»
        
        self.q = self.Q(s,a)
        self.qn= self.Q(sn,an)*(1-done)
        Î´ = rn + Î³*self.qn - self.q
        self.Z[a] = Î»*Î³*self.Z[a] + (1-Î±*Î»*Î³*self.Z[a].dot(s))*s
        
        self.W[a] += Î±*(Î´ + self.q - self.qo )*self.Z[a] - Î±*(self.q - self.qo)*s
        self.qo = self.qn

# =========================a set of useful prediction comparisons =========================================

def TDtiledwalk(ntilings):
    env=tiledrandwalk_(nS=20, tilesize=4, offset=1, ntilings=ntilings)
    TD(env=env, Î±=.02, episodes=200, **demoV()).interact(label='TD learning, %d tilings'%ntilings)


# =========================a set of useful control comparisons =========================================
def MountainCarRuns(runs=20, algo=Sarsa, env=MountainCar(), label='', Îµ=0):
    for Î± in [.1, .2, .5]:
        sarsaRuns = Runs(algorithm=algo(env=env, Î±=Î±/8, episodes=500, Îµ=Îµ),
                         runs=runs, seed=1, plotT=True).interact(label='Î±=%.2f/8'%Î±)
    plt.ylim((10**2,10**3))
    plt.yscale('log')
    plt.title('Semi Gradient ' + algo.__name__  +' on Mountain Car '+label)


def MountainCarTiledCompare_n(runs=5, ntilings=8,  env=IHTtiledMountainCar): # 10
    xsticks = np.array([0, .5 , 1, 1.5, 2, 2.3])/ntilings
    plt.xticks(ticks=xsticks, labels=xsticks*ntilings)
    plt.yticks([220, 240, 260, 280, 300])
    plt.ylim(210, 300)
    plt.title('Steps per episode averaged over first 50 episodes')

    for n in range(5):
        if n==0: Î±s = np.arange(.4,  1.8,  .1)
        if n==1: Î±s = np.arange(.2,  1.8,  .1)
        if n==2: Î±s = np.arange(.1,  1.8,  .1)
        if n==3: Î±s = np.arange(.1,  1.2,  .07)
        if n==4: Î±s = np.arange(.1,  1.0,  .07)
    
        Compare(algorithm=Sarsan(env=env(ntiles=8, ntilings=ntilings), n=2**n, episodes=50, Îµ=0), runs=runs, 
                                  hyper={'Î±':Î±s/ntilings}, 
                                  plotT=True).compare(label='%d-step Sarsa'%2**n)
    plt.xlabel(r'$\alpha \times 8$ since we used 8 tiles for each tilings')
    plt.show()

figure_10_4_n = MountainCarTiledCompare_n


def SarsaOnMountainCar(ntilings, env=tiledMountainCar):
    sarsa = Sarsa(env=env(ntilings=ntilings), Î±=.5/ntilings, episodes=500, seed=1, Îµ=0, plotT=True).interact(label='ntilings=%d'%ntilings)
    plt.gcf().set_size_inches(20,4)
    plt.ylim(100,1000)
    return sarsa


def MountainCarTilings(runs=20, Î±=.3, algo=Sarsa, env=tiledMountainCar):
    plt.title('Sarsa on mountain car: comparison of different tilings with Î±=%.2f/8'%Î±)
    for ntilings in [2, 4, 8, 16, 32]:
        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=ntilings),Î±=Î±/ntilings,episodes=500, Îµ=0), 
                         runs=runs, seed=1, plotT=True).interact(label='%d tilings'%ntilings)
    plt.ylim((10**2,10**3))
    plt.yscale('log')
    plt.show()
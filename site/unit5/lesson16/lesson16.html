
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Abdulrahman Altahhan, 2024.">
      
      
      
        <link rel="prev" href="../lesson15/lesson15.html">
      
      
        <link rel="next" href="../lesson17/lesson17.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>16. Nonlinear Approximation for Control - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-15-non-linear-action-value-approximation-and-policy-gradient-methods-for-control" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              16. Nonlinear Approximation for Control
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit1/lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson16.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Dependencies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-funciton-approximation-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Non-linear Funciton Approximation Reinforcement Learning
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Dependencies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-funciton-approximation-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Non-linear Funciton Approximation Reinforcement Learning
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.).
Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>
<h1 id="lesson-15-non-linear-action-value-approximation-and-policy-gradient-methods-for-control">Lesson 15: Non-linear Action-Value Approximation and Policy Gradient Methods for Control</h1>
<p><strong>Learning outcomes</strong>
1. build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand
1. understand how to combine reinforcement learning with non-linear function approximators such as a neurla network to create a powerful framework that allows automatic agent learning by observation or self-play.
1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</p>
<p><strong>Reading</strong>:
The accompanying reading of this lesson is <strong>chapter 16</strong> of our text book available online <a href="http://incompleteideas.net/book/RLbook2020.pdf">here</a>. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p>
<p>This lesson deals with non-linear function approximation and RL, particularly with neural networks combined with RL. We have seen earlier how to deal with linear function approximations and the benefits that they bring in terms of a richer state representation. Now we move to an even richer but integrated and automatic feature extraction via a neural network. We can of course do feature extraction separate from training an RL, but integrating these two stages brings the benefit of extracting features that are particularly useful for the RL algorithm. One of the earliest examples of successfully using a neural network is Tesauro Backgammon <a href="https://en.wikipedia.org/wiki/TD-Gammon">TDGammon</a>. Although the theoretical convergence guarantees do not extend from linear network cases to the non-linear function approximation, however, that did stop researchers from integrating both albeit in a few examples prior to the deep learning era, after which a vast number of models that uses both deep learning and reinforcement learning emerges with impressive results.</p>
<p>We will utilise the idea of an experience replay buffer. We have already seen how to benefit from past experience on a large scale in the planning lesson, where we build a model of the environment. Here, we will not build a model, so we are still in the vicinity of model-free RL, but we will see how to execute a batch of updates instead of one update at a time. Training a neural network is an important addition to our arsenal of techniques because it brings RL closer to how we train supervised learning models, which is useful in two folds. The first is to train based on a mix of old and new experiences is useful for incorporating new experiences without forgetting old experiences. The second is to benefit from the built-in parallelisation of neural network training, which is greatly useful for more complex domains such as games and robotics. </p>
<p>Note that the replay buffer dictates the choice of an off-policy algorithm, i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.</p>
<h3 id="dependencies">Dependencies</h3>
<p>Please refer to libraries installation in the Introduction to find a list of libraries that you will need to install. If you are using the Azure VM then these packages will be already there, so you can get started and jum to the next section. If you cannot find a <strong>list</strong> of pip3 install in the IntroductionTOC notebook then re-download the notebook from minerva.</p>
<p>Let us test if it is working</p>
<div class="highlight"><pre><span></span><code><span class="c1"># !conda list -f tensorflow</span>
<span class="c1"># print(&#39;-------------check that the two commands give you the same version--------------------&#39;)</span>
<span class="c1"># print(&#39;-------------otherwise it means you are using a kernel without a GPU------------------&#39;)</span>
<span class="c1"># !pip3 show tensorflow </span>
</code></pre></div>
<p>We can also check if our GPU is in use as follows.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.python.client</span><span class="w"> </span><span class="kn">import</span> <span class="n">device_lib</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">())</span>
<span class="c1"># or</span>
<span class="c1"># !python3 -c &quot;import tensorflow as tf; print(tf.config.list_physical_devices(&#39;GPU&#39;))&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2025-02-23 16:56:04.777721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.


[name: &quot;/device:CPU:0&quot;
device_type: &quot;CPU&quot;
memory_limit: 268435456
locality {
}
incarnation: 12904346926301707512
xla_global_id: -1
]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.python.platform</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_info</span> <span class="k">as</span> <span class="n">tf_build_info</span>
<span class="c1"># print(&quot;cudnn_version&quot;,tf_build_info.build_info[&#39;cudnn_version&#39;])</span>
<span class="c1"># print(&quot;cuda_version&quot;,tf_build_info.build_info[&#39;cuda_version&#39;])</span>
</code></pre></div>
<p>Ok, we are ready, let us get started...!</p>
<p>To run this notebook on a remote Azure lab server <strong>without</strong> using remote desktop check this <a href="https://docs.microsoft.com/en-us/azure/lab-services/class-type-jupyter-notebook#template-virtual-machine">link</a></p>
<h2 id="non-linear-funciton-approximation-reinforcement-learning">Non-linear Funciton Approximation Reinforcement Learning</h2>
<p>First let us import the necessary libraries</p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">rl.rl</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</code></pre></div>
<style>.container {width:90% !important}</style>

<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">rand</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">islice</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.cm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.animation</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">animation</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fashion_mnist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Model</span>
<span class="c1"># from tensorflow.keras.callbacks import ProgbarLogger</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">clear_output</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>
</code></pre></div>
<p>Note that we are importing the basic MRP and MDP classes because this is suitable to create a non-linear MRP and MDP as we do not want to mix up non-linear solutions with linear solutions produced in previous lesson.</p>
<h1 id="rl-with-neural-networks">RL with Neural Networks</h1>
<p>It is time to extend our basic MRP class to handle function approximation using neural networks.
Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the application under consideration.</p>
<h2 id="buffer-implementation">Buffer Implementation</h2>
<p><strong>If you are familiar with the concepts of deque then please skip to the next <a href="#Neural-Network-Based-MRP">section</a>.</strong> </p>
<p>It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deque([1, 2, 3, 4], maxlen=5)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deque([3, 4, 5, 6, 7], maxlen=5)
</code></pre></div>
<p>To access the last item we use the usual indexing.</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>7
8
</code></pre></div>
<h3 id="random-sampling-form-the-buffer">Random Sampling form the Buffer</h3>
<p>Let us now take few random samples of size 2 from the buffer.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">random</span><span class="w"> </span><span class="kn">import</span> <span class="n">sample</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[4, 8]
[7, 8]
[4, 6]
</code></pre></div>
<h3 id="buffer-with-complex-element-tuples">Buffer with Complex Element (Tuples)</h3>
<p>Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images.</p>
<div class="highlight"><pre><span></span><code><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;3&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;4&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;5&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;5&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;4&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deque([(&#39;2&#39;, 1, &#39;3&#39;), (&#39;3&#39;, 2, &#39;4&#39;), (&#39;4&#39;, 2, &#39;5&#39;), (&#39;5&#39;, 1, &#39;4&#39;)], maxlen=4)
</code></pre></div>
<p>Now in order to sample we can directly sample from the buffer </p>
<div class="highlight"><pre><span></span><code><span class="n">batch</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[(&#39;2&#39;, 1, &#39;3&#39;), (&#39;5&#39;, 1, &#39;4&#39;), (&#39;3&#39;, 2, &#39;4&#39;)]
</code></pre></div>
<p>However, the above is not useful, usually we want to place all the actions and states and next states in their own list to feed them as a batch into a neural network. To put all states and actions together each in its own list we can use zip</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(&#39;2&#39;, &#39;5&#39;, &#39;3&#39;)
(1, 1, 2)
(&#39;3&#39;, &#39;4&#39;, &#39;4&#39;)
</code></pre></div>
<p>We can also convert them into a numpy array directly.</p>
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[array([&#39;2&#39;, &#39;5&#39;, &#39;3&#39;], dtype=&#39;&lt;U1&#39;),
 array([1, 1, 2]),
 array([&#39;3&#39;, &#39;4&#39;, &#39;4&#39;], dtype=&#39;&lt;U1&#39;)]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;3&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;4&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;5&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;5&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;4&#39;</span><span class="p">))</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;7&#39;</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="s1">&#39;6&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sample</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="mi">3</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>deque([(&#39;3&#39;, 2, &#39;4&#39;), (&#39;4&#39;, 2, &#39;5&#39;), (&#39;5&#39;, 1, &#39;4&#39;), (&#39;7&#39;, 8, &#39;6&#39;)], maxlen=4)
[array([&#39;5&#39;, &#39;7&#39;, &#39;4&#39;], dtype=&#39;&lt;U1&#39;), array([1, 8, 2]), array([&#39;4&#39;, &#39;6&#39;, &#39;5&#39;], dtype=&#39;&lt;U1&#39;)]
</code></pre></div>
<p>sampling an empty batch</p>
<div class="highlight"><pre><span></span><code><span class="n">nbatch</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sample</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[]
</code></pre></div>
<p>sampling last nbatch elements from the buffer</p>
<div class="highlight"><pre><span></span><code><span class="n">nbatch</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">def</span><span class="w"> </span><span class="nf">slice_</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">islice</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span><span class="o">-</span><span class="n">nbatch</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">buffer</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">slice_</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[(&#39;4&#39;, 2, &#39;5&#39;), (&#39;5&#39;, 1, &#39;4&#39;), (&#39;7&#39;, 8, &#39;6&#39;)]
</code></pre></div>
<h2 id="neural-network-based-mrp">Neural Network Based MRP</h2>
<p>In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_.
We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific <em>total</em> number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y&lt;x.
Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">nnMRP</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">nF</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">nbuffer</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">rndbatch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">save_weights</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>     <span class="c1"># save weights every now and then</span>
                 <span class="n">load_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">print_</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kw</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">γ</span><span class="o">=</span><span class="n">γ</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nF</span> <span class="o">=</span> <span class="n">nF</span>   <span class="c1"># feature extraction is integrated within the neural network model not the env</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">nbuffer</span>  <span class="o">=</span> <span class="n">nbuffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbatch</span>   <span class="o">=</span> <span class="n">nbatch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rndbatch</span> <span class="o">=</span> <span class="n">rndbatch</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_</span><span class="o">=</span> <span class="n">load_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_weights_</span><span class="o">=</span> <span class="n">save_weights</span> <span class="c1"># used to save the target net every now and then</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">update_msg</span> <span class="o">=</span> <span class="s1">&#39;update </span><span class="si">%s</span><span class="s1"> network weights...........! </span><span class="si">%d</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saving_msg</span> <span class="o">=</span> <span class="s1">&#39;saving </span><span class="si">%s</span><span class="s1"> network weights to disk...! </span><span class="si">%d</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loading_msg</span> <span class="o">=</span> <span class="s1">&#39;loading </span><span class="si">%s</span><span class="s1"> network weights from disk...!&#39;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vN</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>                      <span class="c1"># create V deep network</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vN</span><span class="p">,</span><span class="s1">&#39;V.weights.h5&#39;</span><span class="p">)</span> <span class="c1"># from earlier training proces   </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V_</span>

    <span class="c1">#-------------------------------------------Deep model related---------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net_str</span><span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="c1">#(84,84,1))#self.env.frame_size_)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nF</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">net_str</span><span class="o">==</span><span class="s1">&#39;V&#39;</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span> <span class="k">if</span> <span class="n">net_str</span> <span class="o">!=</span> <span class="s1">&#39;Qn&#39;</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">model</span><span class="o">.</span><span class="n">net_str</span> <span class="o">=</span> <span class="n">net_str</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">net_str</span> <span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loading_msg</span><span class="o">%</span><span class="n">net_str</span><span class="p">)</span>
        <span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">net_str</span><span class="p">)</span>
        <span class="n">loaded_weights</span><span class="o">.</span><span class="n">assert_consumed</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">save_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">saving_msg</span><span class="o">%</span><span class="p">(</span><span class="s1">&#39;V &#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vN</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;V.weights.h5&#39;</span><span class="p">)</span>

    <span class="c1">#------------------------------------- value related 🧠-----------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">V_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="c1"># update the V network if Vs is passed</span>
        <span class="k">if</span> <span class="n">Vs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">vN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Vs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span> <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># predict for one state for εgreedy, or predict for a batch of states, copy to avoid auto-grad issues</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">!=</span><span class="mi">4</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> 

    <span class="c1">#-------------------------------------------buffer related----------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nbuffer</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">rn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">sn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">an</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">done</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_weights</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_weights_</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">save_weights_</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="c1"># deque slicing, cannot use buffer[-nbatch:]</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">slice_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">nbatch</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">islice</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span><span class="o">-</span><span class="n">nbatch</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer</span><span class="p">)))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># if nbatch==nbuffer==1 then (this should give the usual qlearning without replay buffer)</span>
        <span class="c1"># sample nbatch tuples (each tuple has 5 items) without replacement or obtain latest nbatch from the buffer</span>
        <span class="c1"># zip the tuples into one tuple of 5 items and convert each item into a np array of size nbatch </span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbatch</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rndbatch</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbatch</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span> <span class="k">for</span> <span class="n">experience</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">samples</span><span class="p">)]</span>

        <span class="c1"># generate a set of indices handy for filtering, to be used in online()</span>
        <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbatch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">inds</span>
</code></pre></div>
<h2 id="neural-network-based-mdp">Neural Network Based MDP</h2>
<p>Now we create the MDP class which implements policy related functionality</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">nnMDP</span><span class="p">(</span><span class="n">MDP</span><span class="p">(</span><span class="n">nnMRP</span><span class="p">)):</span>

    <span class="c1"># update the target network every t_qNn steps</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">create_vN</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">create_vN</span> <span class="o">=</span> <span class="n">create_vN</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_vN</span> <span class="k">else</span> <span class="kc">None</span>                     <span class="c1"># to create also vN, suitable for actor-critic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qN</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;Q&#39;</span><span class="p">)</span>                              <span class="c1"># create main policy network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qNn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;Qn&#39;</span><span class="p">)</span>                             <span class="c1"># create target network to estimate Q(sn)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="p">,</span><span class="s1">&#39;Q.weights.h5&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_</span> <span class="k">else</span> <span class="kc">None</span> <span class="c1"># from earlier training proces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qNn</span><span class="p">,</span><span class="s1">&#39;Q.weights.h5&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_</span> <span class="k">else</span> <span class="kc">None</span> <span class="c1"># from earlier training proces</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">save_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">save_weights</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_vN</span> <span class="k">else</span> <span class="kc">None</span>             <span class="c1"># save vN weights, for actor-critic</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">saving_msg</span><span class="o">%</span><span class="p">(</span><span class="s1">&#39;Q&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;Q.weights.h5&#39;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_msg</span><span class="o">%</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">net_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">))</span>
        <span class="n">net</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>

    <span class="c1">#------------------------------------- policies related 🧠-----------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">Q_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Qs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># update the Q networks if Qs is passed</span>
        <span class="k">if</span> <span class="n">Qs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Qs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span> <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># predict for one state for εgreedy, or predict for a batch of states, </span>
        <span class="c1"># copy to avoid auto-grad issues</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">!=</span><span class="mi">4</span> \
    <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">Qn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># update the Qn networks if Qn is passed</span>
        <span class="k">if</span> <span class="n">update</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qNn</span><span class="p">);</span> <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">qNn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sn</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p>Bellow we double-check that the policies assigned via class inheritance is suitable. MRP should have a stationary policy while MDP has an εgreedy policy.</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">nnMRP</span><span class="p">()</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nnMDP</span><span class="p">()</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;bound method MRP.stationary of &lt;__main__.nnMRP object at 0x14c455b80&gt;&gt;
&lt;bound method MDP.&lt;locals&gt;.MDP.εgreedy of &lt;__main__.nnMDP object at 0x14c455b50&gt;&gt;
</code></pre></div>
<h2 id="deep-q-learning-architecture">Deep Q-Learning Architecture</h2>
<p>Note that we need to set ε here otherwise it will be set by default to .1 in the parent class.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DQN</span><span class="p">(</span><span class="n">nnMDP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">t_Qn</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------------------- 🧠  DQN is being set up 🧠 -----------------------&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">α</span> <span class="o">=</span> <span class="n">α</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_Qn</span> <span class="o">=</span> <span class="n">t_Qn</span>

    <span class="c1">#------------------------------- 🌖 online learning ---------------------------------</span>
    <span class="c1"># update the online network in every step using a batch</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># no updates unless the buffer has enough samples</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbuffer</span><span class="p">:</span> <span class="k">return</span>

        <span class="c1"># sample a tuple batch: each component is a batch of items </span>
        <span class="c1"># (s and a are sets of states and actions)</span>
        <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span> <span class="n">dones</span><span class="p">),</span> <span class="n">inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">()</span> 

        <span class="c1"># obtain the action-values estimation from the two networks and </span>
        <span class="c1"># ensure target is 0 for terminal states</span>
        <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">Qn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Qn</span><span class="p">(</span><span class="n">sn</span><span class="p">);</span> <span class="n">Qn</span><span class="p">[</span><span class="n">dones</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># now dictate what the target should have been as per the Q-learning update rule</span>
        <span class="n">Qs</span><span class="p">[</span><span class="n">inds</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Qn</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">rn</span>

        <span class="c1"># then update both</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Qs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Qn</span><span class="p">(</span><span class="n">sn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">t_Qn</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p>Let us now deal with Grid as a game and state as images. We will try first just to run as usual without sampling but with randomisation form the buffer. To do so, we simply assign the same number for the nbuffer and nbatch which will force the algorithm to pass all of what it has in the buffer albeit randomised in terms of order.</p>
<p>To properly force the algorithm not to randomise the samples, we can pass this flag explicitly.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># deal with the Grid states as images and learn from them to navigate it</span>
<span class="c1"># please be patient as it takes much longer to learn from pixles</span>


<span class="o">%</span><span class="n">time</span> <span class="n">nqlearn</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">iGrid</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;maze&#39;</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> \
                    <span class="n">rndbatch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">t_Qn</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">nbuffer</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span> 
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 3h 32min 14s, sys: 44min 36s, total: 4h 16min 50s
Wall time: 3h 11min 34s
</code></pre></div>
<p><img alt="png" src="output_51_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">nqlearn</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">103</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.DQN at 0x14a0f16d0&gt;
</code></pre></div>
<p><img alt="png" src="output_52_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">nqlearn</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">110</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.DQN at 0x14a0f16d0&gt;
</code></pre></div>
<p><img alt="png" src="output_53_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># deal with the Grid states as images and learn from them to navigate it</span>
<span class="o">%</span><span class="n">time</span> <span class="n">nqlearn</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">iGrid</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;maze&#39;</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> \
                    <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nbuffer</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span> 
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 6h 47min 50s, sys: 58min 54s, total: 7h 46min 45s
Wall time: 6h 44min 8s
</code></pre></div>
<p><img alt="png" src="output_54_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># deal with the Grid states as images and learn from them to navigate it</span>
<span class="o">%</span><span class="n">time</span> <span class="n">nqlearn</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">iGrid</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;maze&#39;</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> \
                    <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nbuffer</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 4h 57min 59s, sys: 49min 30s, total: 5h 47min 30s
Wall time: 4h 39min 39s
</code></pre></div>
<p><img alt="png" src="output_55_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># deal with the Grid states as images and learn from them to navigate it</span>
<span class="o">%</span><span class="n">time</span> <span class="n">nqlearn</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">iGrid</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;maze&#39;</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> \
                    <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nbuffer</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">nbatch</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span> 
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 1h 49min 58s, sys: 15min 27s, total: 2h 5min 26s
Wall time: 1h 33min 50s
</code></pre></div>
<p><img alt="png" src="output_56_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">nqlearn</span><span class="o">.</span><span class="n">policy</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;bound method MDP.&lt;locals&gt;.MDP.εgreedy of &lt;__main__.DQN object at 0x19819fc20&gt;&gt;
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">nqlearn</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(50, 84, 1)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">time</span> <span class="n">nqlearn</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">151</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 23min 48s, sys: 3min 35s, total: 27min 24s
Wall time: 19min 47s





&lt;__main__.DQN at 0x19819fc20&gt;
</code></pre></div>
<p><img alt="png" src="output_59_2.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">nqlearn</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render__</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_60_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># nqlearn.ep -=1</span>
<span class="c1"># nqlearn.plotT = False</span>
<span class="c1"># nqlearn.visual = True</span>
<span class="c1"># nqlearn.underhood=&#39;maxQ&#39; # uncomment to see also the policy</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">159</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.DQN at 0x19819fc20&gt;
</code></pre></div>
<p><img alt="png" src="output_61_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;img/img0.png&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(231, 387, 4)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">nqlearn</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">img</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;matplotlib.image.AxesImage at 0x197c04ec0&gt;
</code></pre></div>
<p><img alt="png" src="output_63_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="mi">163</span><span class="o">*</span><span class="mi">278</span><span class="o">*</span><span class="mi">4</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>181256
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">nqlearn</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">img</span><span class="o">=</span><span class="kc">None</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">ep</span> <span class="o">-=</span><span class="mi">1</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">plotT</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">visual</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># nqlearn.underhood=&#39;maxQ&#39; # uncomment to see also the policy</span>
<span class="n">nqlearn</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.DQN at 0x19819fc20&gt;
</code></pre></div>
<p><img alt="png" src="output_65_1.png" /></p>
<h2 id="double-dqn-learning">Double DQN Learning</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DDQN</span><span class="p">(</span><span class="n">DQN</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------- 🧠 Double DQN is being set up 🧠 ---------------------&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">α</span> <span class="o">=</span> <span class="n">α</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1">#--------------------------- 🌖 online learning -----------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># sample a tuple batch: each component is a batch of items </span>
        <span class="c1">#(ex. s is a set of states, a is a set of actions) </span>
        <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span> <span class="n">dones</span><span class="p">),</span> <span class="n">inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">()</span>

        <span class="c1"># obtain the action-values estimation from the two networks </span>
        <span class="c1"># and make sure the target is 0 for terminal states</span>
        <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">Qn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Qn</span><span class="p">(</span><span class="n">sn</span><span class="p">);</span> <span class="n">Qn</span><span class="p">[</span><span class="n">dones</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># now dictate what the target should have been as per the *Double* Q-learning </span>
        <span class="c1"># update rule, this is where the max estimations are decoupled from the max </span>
        <span class="c1"># action selections</span>
        <span class="n">an_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">sn</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Qs</span><span class="p">[</span><span class="n">inds</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Qn</span><span class="p">[</span><span class="n">inds</span><span class="p">,</span> <span class="n">an_max</span><span class="p">]</span> <span class="o">+</span> <span class="n">rn</span>

        <span class="c1"># update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Qs</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">Qn</span><span class="p">(</span><span class="n">sn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">t_Qn</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># %time ddqlearn = DDQN(env=Gridi(style=&#39;maze&#39;), seed=10, episodes=2, max_t=200, \</span>
<span class="c1">#                       max_t_exp=2000, nbuffer=100, nbatch=32, **demoGame()).interact() </span>
</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. </p>
<p>You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. </p>
<h2 id="discussion-and-activity">Discussion and Activity</h2>
<p>Read the following classic Nips <a href="https://deepmind.com/research/publications/2019/playing-atari-deep-reinforcement-learning">paper</a> and Nature <a href="https://storage.googleapis.com/deepmind-media/DQN/DQNNaturePaper.pdf">paper</a> and discuss it in the discussion forum.</p>
<h2 id="your-turn">Your turn</h2>
<ol>
<li>
<p>try to apply the same concept on other simple environments provided by Gym such as the acrobot.</p>
</li>
<li>
<p>apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum.</p>
</li>
</ol>
<h2 id="challenge">Challenge++</h2>
<ol>
<li>check the implementation of the deep network in the <a href="https://keras.io/examples/rl/actor_critic_cartpole/">tutorial</a> and try to integrate it into the provided infrastructure.</li>
</ol>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
        <script src="../../videos/my-video.mp4"></script>
      
    
  </body>
</html>

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../lesson2/lesson2.html">
      
      
        <link rel="next" href="../lesson4/lesson4.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>3. MDP - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. MDP
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson3.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Decision Process (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transition-and-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Transition and Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-policy-and-its-stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      The Policy and its Stationarity
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-return-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      The Return \(G_t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Return \(G_t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monotonicity-of-g_t-in-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonicity of \(G_t\) in MDPs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Monotonicity of \(G_t\) in MDPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monotonic-decrease-of-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonic Decrease of \(G_t\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonic-increase-of-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonic Increase of \(G_t\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proof-for-monotonically-decreasing-case-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Proof for Monotonically Decreasing case (optional)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonicity-of-g_t-in-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonicity of \(G_t\) in Sparse MDP Rewards
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Monotonicity of \(G_t\) in Sparse MDP Rewards">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#proof-for-sparse-rewards-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Proof for Sparse Rewards (optional)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implications-for-rl-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Implications for RL Agents
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-bellman-equations" class="md-nav__link">
    <span class="md-ellipsis">
      The Bellman Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Bellman Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grid-world-environments" class="md-nav__link">
    <span class="md-ellipsis">
      Grid World Environments
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Grid World Environments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Decision Process (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transition-and-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Transition and Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-policy-and-its-stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      The Policy and its Stationarity
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-return-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      The Return \(G_t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Return \(G_t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monotonicity-of-g_t-in-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonicity of \(G_t\) in MDPs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Monotonicity of \(G_t\) in MDPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monotonic-decrease-of-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonic Decrease of \(G_t\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonic-increase-of-g_t" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonic Increase of \(G_t\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proof-for-monotonically-decreasing-case-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Proof for Monotonically Decreasing case (optional)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonicity-of-g_t-in-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Monotonicity of \(G_t\) in Sparse MDP Rewards
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Monotonicity of \(G_t\) in Sparse MDP Rewards">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#proof-for-sparse-rewards-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Proof for Sparse Rewards (optional)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implications-for-rl-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Implications for RL Agents
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-bellman-equations" class="md-nav__link">
    <span class="md-ellipsis">
      The Bellman Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Bellman Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grid-world-environments" class="md-nav__link">
    <span class="md-ellipsis">
      Grid World Environments
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Grid World Environments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<script>
  window.MathJax = {
    tex: {
      tags: "ams",  // Enables equation numbering
    //   displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>

<h1 id="lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons">Lesson 3- Markov Decision Processes, Dynamics and Bellman Equaitons</h1>
<p><strong>Learning outcomes</strong></p>
<ol>
<li>understand MDP and its elements</li>
<li>understand the return for a time step <span class="arithmatex">\(t\)</span></li>
<li>understand the expected return of a state <span class="arithmatex">\(s\)</span></li>
<li>understand the Bellman optimality equations</li>
<li>become familiar with the different types of grid world problems</li>
</ol>
<!-- 6. become familiar with the way we assign a reward to an environment
1. be able to execute actions in a grid world and observe the result
2. be able to to visualise a policy and its action-value function -->

<p><img alt="agent-env-interaction.png" src="agent-env-interaction.png" /></p>
<h2 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h2>
<p>A Markov Decision Process (MDP) provides a mathematical framework to model decision-making problems where an agent interacts with an environment. It is characterised by a tuple <span class="arithmatex">\( (\mathcal{S}, \mathcal{A} , \mathcal{R}, p, \gamma) \)</span> where:</p>
<ul>
<li><span class="arithmatex">\( \mathcal{S} \)</span> is the set of states.  </li>
<li><span class="arithmatex">\( \mathcal{A} \)</span> is the set of actions.  </li>
<li><span class="arithmatex">\( \mathcal{R} \)</span> is the set of rewards.</li>
<li>
<p>p is the <strong>dynamics</strong> of the MDP
  <span class="arithmatex">\(
    p(s', r | s, a) = \Pr\{ S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a \} 
  \)</span>
  constitutes the probability of transitioning to state <span class="arithmatex">\( s' \)</span> and receiving reward <span class="arithmatex">\( r \)</span>, given that the agent is in state <span class="arithmatex">\( s \)</span> and takes action <span class="arithmatex">\( a \)</span>.  Where:</p>
<ul>
<li><span class="arithmatex">\( S_t \)</span> is the state at time step <span class="arithmatex">\( t \)</span>,</li>
<li><span class="arithmatex">\( A_t \)</span> is the action taken at time step <span class="arithmatex">\( t \)</span>,</li>
<li><span class="arithmatex">\( S_{t+1} \)</span> is the next state after taking action <span class="arithmatex">\( A_t \)</span> from <span class="arithmatex">\( S_t \)</span>.</li>
<li><span class="arithmatex">\( R_{t+1} \)</span> is the next reward after taking action <span class="arithmatex">\( A_t \)</span> from <span class="arithmatex">\( S_t \)</span>.</li>
</ul>
</li>
</ul>
<!-- - \( r(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a] \)  
  is the **expected reward function**, which gives the expected immediate reward when taking action \( a \) in state \( s \).   -->
<ul>
<li>
<p><span class="arithmatex">\( \gamma \in [0,1] \)</span> is the discount factor, which determines how much future rewards are valued relative to immediate rewards.  </p>
<ul>
<li>If <span class="arithmatex">\( \gamma = 0 \)</span>, the agent considers only immediate rewards.  </li>
<li>If <span class="arithmatex">\( \gamma \approx 1 \)</span>, the agent values long-term rewards more.  </li>
</ul>
</li>
</ul>
<p>The MDP framework is central to reinforcement learning, as it allows the agent to plan and optimize its actions over time to maximize the expected return.</p>
<p>The dynamics of the MDP is a probability distribution and satisfies that</p>
<div class="arithmatex">\[
\sum_{s'\in\mathcal{S}} \sum_{r\in\mathcal{R}} p(s', r | s, a) = 1, \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\]</div>
<p>There are two types of dynamics that are actually related to each other:</p>
<ul>
<li>
<p><strong>Stochastic Dynamics</strong>:  <span class="arithmatex">\( p(s', r | s, a) \)</span> defines a probability distribution over possible next states and rewards, this accomodate for a stochastic dynamics. This means the transition probability <span class="arithmatex">\( p(s' | s, a) \)</span> is non-deterministic, so we do not know for sure which state s' the agent will end up with. Similarly, the reward can vary for the same transition.</p>
</li>
<li>
<p><strong>Deterministic Dynamics</strong>: If the dynamics <span class="arithmatex">\( p(s', r | s, a) \)</span> assigns probability 1 to a single next state <span class="arithmatex">\( s' \)</span> and reward <span class="arithmatex">\( r \)</span>, meaning the next state and reward are fully determined by <span class="arithmatex">\( s \)</span> and <span class="arithmatex">\( a \)</span>, the dynamics is deterministic. This implies that <span class="arithmatex">\( p(s' | s, a) \)</span> always results in the same <span class="arithmatex">\( s' \)</span>.  </p>
</li>
</ul>
<p>Note that a deterministic dynamics is a special case of stochastic dynamics. Also note that in a finite MDP, the sets of states, actions, and rewards <span class="arithmatex">\((\mathcal{S, A}\)</span>, and <span class="arithmatex">\(\mathcal{R})\)</span> all have a finite number of elements. </p>
<h2 id="transition-and-reward">Transition and Reward</h2>
<p>The transition describe how the environment behaves when the agent takes an action in a given state. The transition function <span class="arithmatex">\( p(s' | s, a) \)</span> specifies the probability of transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> when the agent takes action <span class="arithmatex">\( a \)</span>.</p>
<p>Formally, the transition function is expressed as:</p>
<div class="arithmatex">\[
p(s' | s, a) = Pr(S_{t+1} = s' | S_t = s, A_t = a)
\]</div>
<ul>
<li><strong>Markov Property</strong>: The environment satisfies the <strong>Markov Property</strong>, means that the next state depends only on the current state and action, not on the history of previous states or actions.</li>
</ul>
<p>Example:
In a grid world, if the agent is at state <span class="arithmatex">\( s = (2, 2) \)</span> and takes action <span class="arithmatex">\( a = \text{move left} \)</span>, the transition probability might be deterministic:</p>
<div class="arithmatex">\[
p(s' | (2, 2), \text{move left}) = 
\begin{cases} 
1 &amp; \text{if } s' = (1, 2) \\
0 &amp; \text{otherwise}
\end{cases}
\]</div>
<p>This means that the agent always moves from <span class="arithmatex">\( (2, 2) \)</span> to <span class="arithmatex">\( (1, 2) \)</span> when taking the action "move left".</p>
<p>The propability <span class="arithmatex">\( p(s' | s, a) \)</span> is called the <strong>transition function</strong>, representing the probability of transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>. Since the dynamics <span class="arithmatex">\( p(s', r | s, a) \)</span> gives the joint probability of the next state and reward, we can obtain the transition probability by summing over all possible rewards (called marginalising the reward):</p>
<p>[
    p(s' | s, a) = \sum_r p(s', r | s, a)
  ]</p>
<p>This expresses the probability of transitioning to state <span class="arithmatex">\( s' \)</span> given <span class="arithmatex">\( s \)</span> and <span class="arithmatex">\( a \)</span>, regardless of the reward received.</p>
<p>The <strong>Markov Property</strong> asserts that the future state depends only on the current state and action, not on any previous states or actions. This is a core assumption in MDPs and ensures that the system has <strong>no memory</strong> of past actions or states.</p>
<p>Formally:</p>
<div class="arithmatex">\[
p(S_{t+1} | S_t, A_t, \dots, S_0, A_0) = p(S_{t+1} | S_t, A_t)
\]</div>
<p>In many MDPs, the transition and reward functions are <strong>stationary</strong>, meaning that they do not change over time. This ensures that the transition probabilities and rewards are the same at every time step.</p>
<p>Formally:</p>
<div class="arithmatex">\[
p(s' | s, a) = p(s' | s, a) \quad \forall t
\]</div>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=f93d3c1e-261f-42bd-93cd-92f67e120d99&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="Markov Decision Processes (MDP)" enablejsapi=1></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=79d3a9ea-5401-4f3c-bcf2-5907255ef8da&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="2. Dynamics.mkv"></iframe>

<h2 id="the-policy-and-its-stationarity">The Policy and its Stationarity</h2>
<p>A <strong>policy</strong> in reinforcement learning is a strategy or function that defines the agent's actions at each state in an environment. Mathematically, a policy is often represented as <span class="arithmatex">\( \pi(a|s) \)</span>, where <span class="arithmatex">\( s \)</span> is a state and <span class="arithmatex">\( a \)</span> is an action. The policy <span class="arithmatex">\( \pi(a|s) \)</span> gives the probability of taking action <span class="arithmatex">\( a \)</span> when in state <span class="arithmatex">\( s \)</span>. </p>
<p>A <strong>stationary policy</strong> is one where the action probabilities depend only on the current state and remain constant over time. Formally, a stationary policy satisfies:</p>
<div class="arithmatex">\[
\pi_t(a|s) = \pi(a|s) \quad \text{for all time steps} \, t
\]</div>
<p>This means the policy does not change as the environment evolves. This is common in many reinforcement learning settings where the dynamics of the problem do not change over time.</p>
<p>In contrast, a <strong>non-stationary policy</strong> is one where the action probabilities can change with time:</p>
<div class="arithmatex">\[
\pi_t(a|s) \neq \pi_{t'}(a|s) \quad \text{for some} \, t \neq t'
\]</div>
<p>This occurs when the policy is adapted or modified based on external factors, such as learning or changes in the environment. A non-stationary policy is useful in situations where the environment or the agent's understanding of it evolves over time.</p>
<h2 id="the-expected-reward">The Expected Reward</h2>
<p>The expected reward define the expected reward that the agent receives when it takes an action in a given state and transitions to a new state. The expected reward function <span class="arithmatex">\( r(s, a, s') \)</span> specifies the reward the agent is expected to recieve when transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>.</p>
<p>Formally, the reward function is expressed as:</p>
<div class="arithmatex">\[
    r(s, a, s') = \mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s']
\]</div>
<p>Example:
If the agent takes action <span class="arithmatex">\( a = \text{move right} \)</span> from state <span class="arithmatex">\( s = (1, 1) \)</span>, the reward function might be: <span class="arithmatex">\(r((1, 1), \text{move right}, (2, 1)) = 10\)</span>. Indicating that moving to the goal state <span class="arithmatex">\( (2, 1) \)</span> yields a reward of 10. Conversely, if the agent moves to a dangerous state: <span class="arithmatex">\(r((1, 1), \text{move left}, (0, 1)) = -5\)</span>. The agent receives a penalty of -5.</p>
<p>The expected reward function <span class="arithmatex">\( r(s, a, s') \)</span> can be derived from the joint transition-reward probability <span class="arithmatex">\( p(s', r | s, a) \)</span> as follows:  </p>
<div class="arithmatex">\[
r(s, a, s') = \sum_r r \cdot p(s', r | s, a)
\]</div>
<p>This formula represents the expected reward received when transitioning to state <span class="arithmatex">\( s' \)</span> from state <span class="arithmatex">\( s \)</span> after taking action <span class="arithmatex">\( a \)</span>, by summing over all possible rewards weighted by their probabilities.</p>
<div class="arithmatex">\[
    r(s, a) = \sum_{s'} p(s' | s, a) r(s, a, s')
\]</div>
<p>which represents the average reward expected when taking action <span class="arithmatex">\( a \)</span> in state <span class="arithmatex">\( s \)</span>, considering all possible next states weighted by their transition probabilities.  </p>
<p>From the above can you work out hwo to calculate  <span class="arithmatex">\( r(s, a) \)</span>  from <span class="arithmatex">\( p(s', r | s, a) \)</span>. </p>
<p>The expected reward function <span class="arithmatex">\( r(s, a) \)</span> can be computed from the joint transition-reward probability <span class="arithmatex">\( p(s', r | s, a) \)</span> as follows:</p>
<div class="arithmatex">\[
r(s, a) = \sum_{s'} \sum_r r \cdot p(s', r | s, a)
\]</div>
<p>This formula represents the expected reward for taking action <span class="arithmatex">\( a \)</span> in state <span class="arithmatex">\( s \)</span>, by summing over all possible next states <span class="arithmatex">\( s' \)</span> and rewards <span class="arithmatex">\( r \)</span>, weighted by their probabilities.</p>
<h2 id="the-return-g_t">The Return <span class="arithmatex">\(G_t\)</span></h2>
<p>The return of time step <span class="arithmatex">\(t\)</span> is defiend as the sum of actual rewards the agent recieves form current time step up until the end of the horizon (end of agent episode or infinitely).</p>
<div class="arithmatex">\[
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T}
\]</div>
<p>Accordingly at time step <span class="arithmatex">\(t+1\)</span> we have:</p>
<div class="arithmatex">\[
    G_{t+1} =  R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ... + \gamma^{T-t-2} R_{T}
\]</div>
<p>Hence by multiplying <span class="arithmatex">\(G_{t+1}\)</span> by <span class="arithmatex">\(\gamma\)</span> and adding R_{t+1} we get</p>
<div class="arithmatex">\[
    G_t = R_{t+1} + \gamma G_{t+1}
\]</div>
<p><strong>The above equation is the most important equation in RL that the Bellman Equations are built on it. In turn, we build all of our incremental updates in RL on Bellman optimality equation</strong></p>
<p>In the video below we talk more about this important concept.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=e5a9acea-f258-4952-8e05-46f5ffb0c576&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="3. Returns 1"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8a1a8b63-58be-45ce-86b1-eedb4bc133c4&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="3. Returns 2"></iframe>

<!-- ### $G_t$ Monotonicity for MDP Rewards
Let us see how the return develops for an MDP with a reward of 1 or -1 for each time step.
To calculate $G_t$ we will go backwards, i.e. we will need to calculate $G_{t+1}$ to be able to calculate $G_t$ due to the incremental form of $G_t$ where we have that $G_t = R_{t+1} + \gamma G_{t+1}$.

- Mathematically, we can prove that $G_t$ is monotonically decreasing iff(if and only if) $\frac{R_{t}}{1 - \gamma} >  G_{t}$ $\forall t$ and $G_T=R_T > 0$. 
    - Furthermore, when $R_t=1$ $\forall t$ and $\gamma=.9$ then $G_t$ converges in the limit to 10, i.e. 10 will be an upper bound for $G_t$. 
    - Similarly, when $R_t=1$ $\forall t$ and $\gamma=.09$ then $G_t$ converges in the limit to 100
    - More generally, when $1-\gamma = 1/\beta$ then $R_t \beta > G_t$ 
- On the other hand, we can prove that $G_t$ is monotonically increasing iff $\frac{R_{t}}{1 - \gamma} <  G_{t}$.
    - Furthermore, when $R_t=-1$ $\forall t$ and $\gamma=.9$ then $G_t$ converges to -10, i.e. -10 is its lower bound. 
    - More generally, when $1-\gamma = 1/\beta$ then $R_t \beta < G_t$ 

Below we prove the former and leave the latter for you as homework.

$G_t = R_{t+1} + \gamma G_{t+1}$

We start by assuming that $G_t$ is strictly monotonically decreasing (we dropped the word strictly in th above for readability)

$G_t > G_{t+1} > 0$ $\forall t$ (which entails that $G_T=R_T > 0$ when the horizon is finite, i.e. ends at $t=T$) we substitute by the incremental form of $G_t$

$G_t > G_{t+1} > 0$ $\forall t \implies R_{t+1} + \gamma G_{t+1} >G_{t+1} \implies$  
$R_{t+1} >  G_{t+1} - \gamma G_{t+1} \implies$
$R_{t+1} >  (1 - \gamma) G_{t+1} \implies$

$\frac{R_{t+1}}{1 - \gamma} > G_{t+1}$ ( $\gamma \ne 1$)

The inequality $\frac{R_{t+1}}{1 - \gamma} >  G_{t+1}$ (which also can be written as $\frac{R_{t}}{1 - \gamma} >  G_{t}$) must be satisfied whenever $G_t$ is monotonically decreasing, i.e. it is a necessary condition. We can show that this inequality is also a sufficient condition to prove that $G_t$ is monotonically decreasing by following the same logic backwards. Similar things can be proven for the non-strictly monotonically decreasing case i.e. when $G_t\ge G_{t+1} \ge 0$ $\forall t$.

Now when $R_{t+1}=1$ and $\gamma=.9$, then by substituting these values in the inequality, we get that
$\frac{1}{1 - .9} >  G_{t+1} \implies$ $10 > G_{t+1}$ 


### $G_t$ Monotonicity for Sparse MDP Rewards

For sparse positive end-of-episode rewards, the above strict inequality is not satisfied since $R_t=0$ $\forall t<T$ and $R_T>0$.
1. In this case, we can show that $G_t \le G_{t+1}$ i.e. $G_t$ it is a monotonically increasing function.
    1. Furthermore, when $\gamma<1$ then $G_t$ is strictly increasing, i.e.  $G_t < G_{t+1}$
1. Furthermore, $G_{t} = \gamma^{T-t-1} R_{T}$.
    1. when $R_T=1$ then $G_{t} = \gamma^{T-t-1}$ 
    1. when $R_T=-1$ then $G_{t} = -\gamma^{T-t-1}$

- To prove the monotonicity we start with our incremental form for the return: 
    $G_t = R_{t+1} + \gamma G_{t+1}$:

    Since we have that $R_{t+1} = 0$ $\forall t<T$ then

    $G_t = \gamma G_{t+1}$ $\forall t<T$, therefore, since $\gamma \le 1$ then $G_t \le G_{t+1}$ $\forall t<T$.

- To prove that  $G_{t} = \gamma^{T-t-1} R_{T}$ we can also utilise the incremental form and perform a deduction, but it is easier to start with the general form of a return, we have:

    $G_t = R_{t+1} + \gamma R_{t+2}  + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T}$

    Since we have that $R_{t+1} = 0$ $\forall t<T$ then

    $G_t = \gamma^{T-t-1} R_{T}$

  -->

<h3 id="monotonicity-of-g_t-in-mdps">Monotonicity of <span class="arithmatex">\(G_t\)</span> in MDPs</h3>
<p>The return <span class="arithmatex">\(G_t\)</span> in an MDP follows the recursive formula: 
<span class="arithmatex">\(
G_t = R_{t+1} + \gamma G_{t+1}
\)</span>
where rewards at each step can be either positive or negative. To calculate <span class="arithmatex">\(G_t\)</span> we will go backwards, i.e. we will need to calculate <span class="arithmatex">\(G_{t+1}\)</span> to be able to calculate <span class="arithmatex">\(G_t\)</span> due to the incremental form of <span class="arithmatex">\(G_t\)</span> where we have that <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>.</p>
<h4 id="monotonic-decrease-of-g_t">Monotonic Decrease of <span class="arithmatex">\(G_t\)</span></h4>
<p><span class="arithmatex">\(G_t\)</span> is monotonically decreasing iff:  <span class="arithmatex">\(
G_t &lt; \frac{R_t}{1 - \gamma} \quad \forall t, \quad \text{and } G_T = R_T &gt; 0
\)</span>.</p>
<p>For example:</p>
<ul>
<li>If <span class="arithmatex">\( R_t = 1 \quad \forall t \quad\)</span> and <span class="arithmatex">\( \gamma = 0.9 \)</span>, then <span class="arithmatex">\( G_t \)</span> is bounded above by <span class="arithmatex">\(\frac{1}{1 - 0.9}=10 \)</span>.  </li>
<li>If <span class="arithmatex">\( R_t = 1 \quad \forall t \quad\)</span> and <span class="arithmatex">\( \gamma = 0.99 \)</span>, then <span class="arithmatex">\( G_t \)</span> is bounded above by   ?.  </li>
<li>More generally, when <span class="arithmatex">\( 1 - \gamma = 1/\beta \)</span>, then <span class="arithmatex">\(G_t&lt; \beta R_t  \)</span>.  </li>
</ul>
<p><em>Exercise</em>: calculate the bound when <span class="arithmatex">\( R_t = 1 \quad \forall t \quad\)</span> and <span class="arithmatex">\( \gamma = 0.99 \)</span>.</p>
<h4 id="monotonic-increase-of-g_t">Monotonic Increase of <span class="arithmatex">\(G_t\)</span></h4>
<p><span class="arithmatex">\(G_t\)</span> is monotonically increasing iff:  <span class="arithmatex">\(
G_t &gt; \frac{R_t}{1 - \gamma}  \quad \forall t, \quad \text{and } G_T = R_T &lt; 0
\)</span>.</p>
<ul>
<li>If <span class="arithmatex">\( R_t = -1 \quad \forall t \quad\)</span> and <span class="arithmatex">\( \gamma = 0.9 \)</span>, then <span class="arithmatex">\( G_t \)</span> is bounded below by -10.  </li>
<li>More generally, when <span class="arithmatex">\( 1 - \gamma = 1/\beta \)</span>, then <span class="arithmatex">\( G_t &gt; \beta R_t  \)</span>.  </li>
</ul>
<h4 id="proof-for-monotonically-decreasing-case-optional">Proof for Monotonically Decreasing case (<em>optional</em>)</h4>
<p>Assume <span class="arithmatex">\( G_t \)</span> is strictly decreasing:  <span class="arithmatex">\(
  0&lt; G_{t+1} &lt; G_t \quad \forall t
\)</span>. </p>
<p>Substituting <span class="arithmatex">\( G_t \)</span> by R_{t+1} + \gamma G_{t+1} as per the recursive return formula: </p>
<p><span class="arithmatex">\(
G_{t+1} &lt; R_{t+1} + \gamma G_{t+1} 
\)</span></p>
<p><span class="arithmatex">\(
G_{t+1} (1 - \gamma) &lt; R_{t+1} 
\)</span></p>
<p><span class="arithmatex">\(
G_{t+1} &lt; \frac{R_{t+1}}{1 - \gamma} 
\)</span>  </p>
<p>Thus, the condition is <em>necessary and sufficient</em> for monotonic decrease.  </p>
<p>For example, if <span class="arithmatex">\( R_{t+1} = 1 \)</span> and <span class="arithmatex">\( \gamma = 0.9 \)</span>: hence
<span class="arithmatex">\(
\frac{1}{1 - 0.9} &gt; G_{t+1} \implies 10 &gt; G_{t+1}
\)</span>. </p>
<p>The proof for the increasing case is similar.</p>
<h3 id="monotonicity-of-g_t-in-sparse-mdp-rewards">Monotonicity of <span class="arithmatex">\(G_t\)</span> in Sparse MDP Rewards</h3>
<p>For sparse end-of-episode rewards, where <span class="arithmatex">\( R_t = 0 \;\; \forall t &lt; T \)</span> and <span class="arithmatex">\( R_T &gt; 0 \)</span>:  </p>
<ul>
<li>
<p><span class="arithmatex">\( G_t \)</span> is monotonically increasing:  <span class="arithmatex">\(
  G_t \leq G_{t+1}, \quad \forall t &lt; T.
  \)</span></p>
</li>
<li>
<p>If <span class="arithmatex">\( \gamma &lt; 1 \)</span>, then <span class="arithmatex">\( G_t \)</span> is strictly increasing: <span class="arithmatex">\(
  G_t &lt; G_{t+1}.
  \)</span></p>
</li>
<li>
<p><span class="arithmatex">\(G_t = \gamma^{T-t-1} R_T.\)</span>  </p>
<ul>
<li>If <span class="arithmatex">\( R_T = 1 \)</span>, then <span class="arithmatex">\( G_t = \gamma^{T-t-1} \)</span>.  </li>
<li>If <span class="arithmatex">\( R_T = -1 \)</span>, then <span class="arithmatex">\( G_t = -\gamma^{T-t-1} \)</span>.  </li>
</ul>
</li>
</ul>
<h4 id="proof-for-sparse-rewards-optional">Proof for Sparse Rewards (<em>optional</em>)</h4>
<p>From the recursive definition:<br />
<span class="arithmatex">\(
G_t = R_{t+1} + \gamma G_{t+1}
\)</span>. Since <span class="arithmatex">\( R_{t+1} = 0 \)</span> for <span class="arithmatex">\( t &lt; T \)</span>:<br />
<span class="arithmatex">\(
G_t = \gamma G_{t+1} \quad \forall t &lt; T
\)</span>. </p>
<p>Since <span class="arithmatex">\( \gamma \leq 1 \)</span>, then <span class="arithmatex">\( G_t \leq G_{t+1} \)</span>.  </p>
<p>Using the full return formula:<br />
<span class="arithmatex">\(
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_T
\)</span>. </p>
<p>Since <span class="arithmatex">\( R_{t+1} = 0 \;\; \forall t &lt; T  \)</span> then <span class="arithmatex">\(G_t = \gamma^{T-t-1} R_T.\)</span>  </p>
<hr />
<h3 id="implications-for-rl-agents">Implications for RL Agents</h3>
<p>This gives us guidance on the type of behaviour that we expect our agent to develop when we follow one of these reward regimes (sparse or non-sparse). </p>
<p>The above suggests that for sparse end-of-episode rewards, decisions near the terminal state(s) have far more important effects on the learning process than earlier decisions. While for non-sparse positive rewards MDPs, earlier states have higher returns and hence more importance than near terminal states. </p>
<p>If we want our agent to place more importance on earlier states, and near-starting state decisions, then we will need to utilise non-sparse (positive or negative) rewards. Positive rewards encourage repeating certain actions that maintain the stream of positive rewards for the agent. An example will be the pole balancing problem. Negative rewards, encourage the agent to speed up towards ending the episode so that it can minimise the number of negative rewards received.</p>
<p>When we want our agent to place more importance for the decisions near the terminal states, then a sparse reward is more convenient. Sparse rewards are also more suitable for offline learning as they simplify the learning and analysis of the agent's behaviour. Non-sparse rewards suit online learning on the other hand, because they give a quick indication of the agent behaviour suitability and hence speed up the early population of the value function.</p>
<p>In summary:</p>
<ul>
<li>Sparse rewards emphasise decisions near terminal states, making them better suited for offline learning.  </li>
<li>Non-sparse rewards emphasise earlier decisions, making them better for online learning and faster value estimation.  </li>
<li>Positive rewards encourage sustaining good behavior (e.g., pole balancing).  </li>
<li>Negative rewards encourage minimising episode duration (e.g., escape or navigation problems).  </li>
</ul>
<h2 id="the-expected-return-function-v">The Expected Return Function V</h2>
<p>Once we move form an actul return that comes froma an actual experience at time step <span class="arithmatex">\(t\)</span> to try to estimate this return, we move to an expectaiton <em>function</em>. This function, traditionally called the value function v, is an important function. But now isntead of tying the value of the return to a particular experience at a step t which would be less useful in generalising the lessons an agent can learn from interacting with the environment, it makes more sense to ty this up to a certain state <span class="arithmatex">\(s\)</span>. This will allow the agent to learn a useful expectation of the return(discounted sum of rewards) for a particualr state when the agent follows a policy <span class="arithmatex">\(\pi\)</span>. I.e. we are now saying that a we will get an expected value of the return for a particular state under a policy <span class="arithmatex">\(\pi\)</span>. So we moved from subscripting by a step <span class="arithmatex">\(t\)</span> into passing a state <span class="arithmatex">\(s\)</span> to the function and subscripting by a policy.</p>
<div class="arithmatex">\[
\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{\pi}(G_t)   \label{eq:v}  %\tag{1}
\end{equation}
\]</div>
<p>Equation <span class="arithmatex">\(\eqref{eq:v}\)</span> gives the definition of v function.</p>
<p>In the following video we tackle this idea in more details.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=7b8178ed-68d1-4335-8ab7-3d81f214f362&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="475" height="200"frameborder="0" scrolling="no" allowfullscreen title="4. Returns Expectation and Sampling.mkv"></iframe>

<p>For algorithms like <strong>Value Iteration</strong>, it is important that the MDP is <strong>irreducible</strong> (all states are reachable from any other state) and <strong>aperiodic</strong> (there are no cycles of fixed lengths that prevent convergence).</p>
<h2 id="the-bellman-equations">The Bellman Equations</h2>
<p>The <strong>Bellman equations</strong> provide recursive relationships between the value of a state (or state-action pair) and the values of neighboring states. These equations are fundamental in solving MDPs and are the basis for many reinforcement learning algorithms.</p>
<!-- ### Bellman Equation for the Value Function -->

<p>The <strong>value function</strong> <span class="arithmatex">\( V_{\pi}(s) \)</span> represents the expected return starting from state <span class="arithmatex">\( s \)</span> and following policy <span class="arithmatex">\( \pi \)</span>. The Bellman equation for <span class="arithmatex">\( V_{\pi}(s) \)</span> is:</p>
<div class="arithmatex">\[
V_{\pi}(s) = \mathbb{E}_{\pi}\left[ r(s, a, s') + \gamma \sum_{s'} p(s' | s, a) V_{\pi}(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( V_{\pi}(s) \)</span> is the value of state <span class="arithmatex">\( s \)</span> under policy <span class="arithmatex">\( \pi \)</span>,
- <span class="arithmatex">\( r(s, a, s') \)</span> is the immediate reward for transitioning from <span class="arithmatex">\( s \)</span> to <span class="arithmatex">\( s' \)</span> after action <span class="arithmatex">\( a \)</span>,
- <span class="arithmatex">\( \gamma \)</span> is the discount factor, and
- <span class="arithmatex">\( p(s' | s, a) \)</span> is the transition probability.</p>
<!-- ### Bellman Equation for the Q-Function -->

<p>The <strong>Q-function</strong> <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> represents the expected return after taking action <span class="arithmatex">\( a \)</span> in state <span class="arithmatex">\( s \)</span> and then following policy <span class="arithmatex">\( \pi \)</span>. The Bellman equation for <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> is:</p>
<div class="arithmatex">\[
Q_{\pi}(s, a) = \mathbb{E}\left[ r(s, a, s') + \gamma \sum_{s'} p(s' | s, a) V_{\pi}(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> is the action-value function,
- The terms <span class="arithmatex">\( r(s, a, s') \)</span>, <span class="arithmatex">\( \gamma \)</span>, and <span class="arithmatex">\( p(s' | s, a) \)</span> are the same as in the value function equation.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=6d6d9455-7174-447a-8bcb-eceaa51a4af5&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="5. Bellman v.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=3a18cbb0-6960-42c1-bcf4-8e0893c09c89&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="6. Bellman q simple.mkv"></iframe>

<h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3>
<p>The <strong>Bellman optimality equations</strong> describe the relationship between the optimal value function <span class="arithmatex">\( V^*(s) \)</span> or the optimal Q-function <span class="arithmatex">\( Q^*(s, a) \)</span> and the transition and reward dynamics. These equations are used to compute the optimal policy that maximizes the expected return.</p>
<div class="arithmatex">\[
    V^*(s) = \max_a \mathbb{E}\left[ r(s, a, s') + \gamma \sum_{s'} p(s' | s, a) V^*(s') \right]
\]</div>
<div class="arithmatex">\[
    Q^*(s, a) = \mathbb{E}\left[ r(s, a, s') + \gamma \sum_{s'} p(s' | s, a) \max_{a'} Q^*(s', a') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( V^*(s) \)</span> is the optimal value function,
- <span class="arithmatex">\( Q^*(s, a) \)</span> is the optimal Q-function,
- The <strong>max</strong> operator ensures that the agent chooses the action <span class="arithmatex">\( a \)</span> that maximizes the expected return.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=b73eab99-7af2-4b9e-8909-19492615d273&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 1.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8d89893b-6e99-4380-a31e-93e2974cd04a&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 2.mkv"></iframe>

<p>You can adjust the video settings in SharePoint (speed up to 1.2 and reduce the noise if necessary)</p>
<p><em>Exercise 1</em>: If you realise there is a missing symbol in the [video: Bellman Equation for v] last equations, do you know what it is and where it has originally come from?</p>
<p><em>Exercise 2</em>: Can you derive Bellman Optimality Equation for <span class="arithmatex">\(q(s,a)\)</span> from first principles?</p>
<p>Bellman Optimality for q from first principles can be found in this <em>optional video</em>.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=fef86f50-e352-4af5-a85b-7f134be29085&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. MDP Bellman Equation for q from first prinsiple.mkv"></iframe>

<h2 id="grid-world-environments">Grid World Environments</h2>
<p>Ok, so now we are ready to tackle the practicals, please go ahead and download the worksheet and run and experiement with the provided code to build some grid world environments and visualise them and make a simple robot agent takes some steps/actions within these environments!.</p>
<p>You will need to download a python library (Grid.py) that we bespokley developed to help you run RL algorithms on toy problems and be abel to easily visualise them as needed, the code is optimised to run efficiently and you will be able to use these environmnets to test different RL algorithms extensively. Please place the library in the same directory of the worksheet. In general it would be a good idea to place all worksheets and libraries provided in one directory. This will make importing and runing code easier and more streamlined.</p>
<h3 id="summary">Summary</h3>
<p>The Markov Decision Process (MDP) framework models decision-making problems where an agent interacts with an environment. It includes dynamics <span class="arithmatex">\( p(s', r | s, a) \)</span>, which define the probability of transitioning to state <span class="arithmatex">\( s' \)</span> and receiving reward <span class="arithmatex">\( r \)</span> given that the agent is in state <span class="arithmatex">\( s \)</span> and takes action <span class="arithmatex">\( a \)</span>. The Bellman equations provide recursive relationships for computing the value of states or actions, while the Bellman optimality equations help find the optimal policy. Key properties of MDPs include the Markov Property, stationarity, and the stochastic nature of the dynamics. Understanding these dynamics and equations is fundamental to reinforcement learning algorithms designed to find optimal decision-making strategies.  </p>
<p><strong>Further Reading</strong>:
For further info please refer to chapter 3 of the Sutton and Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">book</a>. </p>
<h2 id="your-turn">Your turn</h2>
<p>Go ahead and play around with some grid world environment by executing and experiementing with the code in <a href="../../workseets/worksheet3.ipynb">worksheet3</a>.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
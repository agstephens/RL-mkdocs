
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../../index.html">
      
      
        <link rel="next" href="../lesson2/lesson2.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>1. Tabular Methods - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-1-introduction-to-tabular-methods-in-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1. Tabular Methods
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson1.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      Markov property
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#elements-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Elements of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Elements of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-policy" class="md-nav__link">
    <span class="md-ellipsis">
      The Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-transition-probability" class="md-nav__link">
    <span class="md-ellipsis">
      The Transition Probability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-dynamics-of-an-enviornment" class="md-nav__link">
    <span class="md-ellipsis">
      The Dynamics of an Enviornment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-and-the-task" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward and the Task
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Reward and the Task">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Types of Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-return" class="md-nav__link">
    <span class="md-ellipsis">
      The Return
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-task" class="md-nav__link">
    <span class="md-ellipsis">
      The Task
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-value-function-and-action-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      The Value Function and Action-Value Function
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tabular-representation-and-example" class="md-nav__link">
    <span class="md-ellipsis">
      Tabular Representation and Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unit-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Unit Overview
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      Markov property
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#elements-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Elements of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Elements of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-policy" class="md-nav__link">
    <span class="md-ellipsis">
      The Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-transition-probability" class="md-nav__link">
    <span class="md-ellipsis">
      The Transition Probability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-dynamics-of-an-enviornment" class="md-nav__link">
    <span class="md-ellipsis">
      The Dynamics of an Enviornment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-and-the-task" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward and the Task
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Reward and the Task">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Types of Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-return" class="md-nav__link">
    <span class="md-ellipsis">
      The Return
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-task" class="md-nav__link">
    <span class="md-ellipsis">
      The Task
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-value-function-and-action-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      The Value Function and Action-Value Function
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tabular-representation-and-example" class="md-nav__link">
    <span class="md-ellipsis">
      Tabular Representation and Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unit-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Unit Overview
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="lesson-1-introduction-to-tabular-methods-in-reinforcement-learning">Lesson 1: Introduction to Tabular Methods in Reinforcement Learning</h1>
<p><strong>Unit 1 Learning outcomes</strong></p>
<p>By the end of this unit, you will be able to:  </p>
<ol>
<li><strong>Explain</strong> the armed bandit problem and how isolating the action space simplifies decision-making.  </li>
<li><strong>Describe</strong> the value function and the action-value function, highlighting their essential roles in reinforcement learning (RL).  </li>
<li><strong>Differentiate</strong> between associative and non-associative problems in RL.  </li>
<li><strong>Analyze</strong> the theoretical foundations of RL, including Markov Decision Processes (MDPs) and the Bellman equation.  </li>
<li><strong>Compare</strong> prediction and control in RL settings, outlining their respective challenges and solutions.  </li>
</ol>
<hr />
<p>In this unit, we start by covering a simplified RL settings in the form of common problems, called Armed-bandit problem. We then move into understanding the main mathematical framework underpining  reinforcement learning, namely Markov Decision Processes(MDPs). As always we take a balaced approach by discussing such concepts from a theoretical and practical perspectives. </p>
<h2 id="markov-property">Markov property</h2>
<p>RL has gained a lot of attention in recent years due to its unmatched ability to tackle difficult control problems with a minimal assumption about the settings and the environment that an agent works in. Controlling an agent (such as a simulated robot) is not trivial and can often require a specific setup and strong assumptions about its environment that make the corresponding solutions sometimes either difficult to attain or impractical in real scenarios. In RL, we try to minimise these assumptions and require that only the environment adheres to the Markov property. In simple terms, the Markov property assumes that inferring what to do (taking action) in a specific state can be fully specified by looking at this state and does not depend on other past states.</p>
<h2 id="elements-of-rl">Elements of RL</h2>
<p>In RL, we have mainly four elements that we deal with: states, actions and rewards and the policy. The state space is the space the agent operates in, whether physical or virtual. The state can represent something specific in the environment, an agent configuration or both. The actions are the set of decisions available for the agent to take that affect the state the agent is in and/or cause the environment to respond to it in a certain way, via a reward signal. An RL agent's main aim is to attain, usually via learning, a cohesive policy that allows it to achieve a specific goal of maximising its reward in the long and short terms. </p>
<h3 id="the-policy">The Policy</h3>
<p>This policy <span class="arithmatex">\(π\)</span> can take a simple form <span class="arithmatex">\(\pi(s)=a\)</span> or symbolically <span class="arithmatex">\(s → a\)</span>, which means if the agent is in state <span class="arithmatex">\(s\)</span>, then take action <span class="arithmatex">\(a\)</span>. This type of policy is deterministic because the agent will definitely take the action <span class="arithmatex">\(a\)</span> if it is in state <span class="arithmatex">\(s\)</span>. Below we show an example of a deterministic policy.</p>
<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
</tr>
</tbody>
</table>
<p>Another type of policy that we deal with is stochastic policy. A stochastic policy takes the form of <span class="arithmatex">\(\pi(a|s)\)</span>, which represents the probability of taking action <span class="arithmatex">\(a\)</span> given that the agent is in state <span class="arithmatex">\(s\)</span>. For such a policy, the agent draws from the set of available actions according to the conditional probability, which we call its policy. The higher the probability of an action, the more likely it will be chosen, when the probability is 1 it means the action will be taken definitely, when it is 0 it means it will not be taken. below we show an example of stochastic policy.</p>
<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
<th>Action's Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>.8</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>.2</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>.6</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>.4</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>0.</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>1.</td>
</tr>
</tbody>
</table>
<p>A deterministic policy is a special case of a stochastic policy with all of its actions' probabilities being 0 except one action.</p>
<h3 id="the-reward">The Reward</h3>
<p>The reward function can take the simple form of <span class="arithmatex">\(r(s,a)\)</span> or symbolically <span class="arithmatex">\((s,a) →r\)</span>, which is interpreted as follows: if the agent is in state <span class="arithmatex">\(s\)</span> and applied action <span class="arithmatex">\(a\)</span> it obtains a reward <span class="arithmatex">\(r\)</span>. This reward can be actual or expected. The general setting that we deal with is the probabilistic one with the form of <span class="arithmatex">\(p(r|s,a)\)</span>, which provides us with the probability of the agent obtaining reward <span class="arithmatex">\(r\)</span> given it was in state <span class="arithmatex">\(s\)</span> and applied action <span class="arithmatex">\(a\)</span>. </p>
<h3 id="the-transition-probability">The Transition Probability</h3>
<p>Another conditional probability that we deal with takes the form of <span class="arithmatex">\(p(s’|s,a)\)</span>, which is the probability of transitioning to state <span class="arithmatex">\(s’\)</span> given the agent was in state <span class="arithmatex">\(s\)</span> and applied action <span class="arithmatex">\(a\)</span>. This is called the transition probability. </p>
<h2 id="the-dynamics-of-an-enviornment">The Dynamics of an Enviornment</h2>
<p>Both the transition and reward probabilities can be inferred from a more general probability that specifies the dynamics of the environment. This is a joint conditional probability that takes the form of <span class="arithmatex">\(p(s’,r|s,a)\)</span>. This probability is interpreted as the joint conditional probability of transitioning to state <span class="arithmatex">\(s’\)</span> and obtaining reward <span class="arithmatex">\(r\)</span> | <em>given</em> that the agent was in state <span class="arithmatex">\(s\)</span> and applied action <span class="arithmatex">\(a\)</span>. We will deal mostly with the dynamics in the second lesson of this unit. Bear in mind that obtaining the dynamics is difficult or intractable in most cases except for the simplest environment. Nevertheless, the dynamics are very useful theoretically for understanding the basic ideas of RL.</p>
<h2 id="the-reward-and-the-task">The Reward and the Task</h2>
<p>The reward function is strongly linked to the task that the agent is trying to achieve, this is the minimal information provided for the agent to indicate to it whether it is on the right track or not. We need to be careful not to devise a complicated reward function that directly supervise the agent. This is not only usually unnecessary, but it is also harmful for the agent perfromance, since when doing so, we may be directly solving the problem for the agent which defies the purpos of the RL framework. Instead, we would want the agent to solve the problem by utilising a simple reward signal and interacting with its environment to gain experience and sharpen and improve its decision-making policy. </p>
<p>Improving the agent's decision-making involves two things: evaluating its current policy and changing it in a way that will improve its performance, basically collecting as much reward as possible in the long and short terms. This is where the rewards, particualrly the sum of rewards an agent can collect while achieving a task, plays a major role. </p>
<p>In RL, we link the policy to maximising the discounted sum of the rewards an agent can obtain while moving towards achieving a task. The reward can be negative, and in this case, the agent will be trying to minimise the sum of negative rewards that it is collecting before terminating. </p>
<p>The <strong>termination</strong> happens when the agent achieves the required task, has taken a pre-specified number of steps or consumed pre-set computational resources. </p>
<h3 id="types-of-rewards">Types of Rewards</h3>
<p>An example of a good simple reward is giving a robot a reward of 1 when it reaches a goal location and 0 otherwise. This is the most generic and most sparse reward we can set for an agent. It basically just tells the agent whether it succeeded or not. This kind of reward demonstrates the essential capabilities and advantage RL can provide in constrat to supervised learning. </p>
<p>Another example is giving a robot a negative reward (penalty) of -1 for each step it takes before reaching the goal location, where the agent can be given a reward of 0 (no penalty), or positive reward. It is effectively informing the agent in each step whether it has achieved the task or not. This kind of reward is useful for situations that invloves achieving a task in a minimum number of steps. Therefore, it is a good fit for shortest path problems and navigation tasks.</p>
<p>Both of these rewards have their advantages, and we will explore and experiment with them in our exercises. The advantage of the first type is its simplicity, generality and inforced sparsity, but it can take longer to explore the environment and longer to populate its estimations. The advantage of the second type is also its relative generality,albeit less generic than the first, and that it provides intermediate general information that the agent can immediately utilise to improve its policy before achieving the task or reaching the goal location. The second type is specifically useful for online learning and when we want to alleviate the agent from having to change its starting position to cover all possible starts. The first type is useful for studying the capabilities of a learning algorithm with minimum information.</p>
<h3 id="the-return">The Return</h3>
<p>The sum of rewards from a specific state to the end of the task is called the <em>return</em>. Because we do not know how long the agent may take to achieve the task and to fairly maximise the rewards and allow for variability, we discount the rewards so that more recent rewards have more effect than later rewards. Nevertheless, our aim is to maximise the rewards in the long run. To be more explicit, we call the sum of discounted rewards from the current state to the end of the task the <em>return</em> that the agent will obtain in the future- the whole idea is to predict these rewards and be able to collect as much as possible.</p>
<h3 id="the-task">The Task</h3>
<p>The task we give the agent can be a continuous, infinite interaction with the environment. It can also take the form of a task with a specific goal or termination state, and when it is reached, the task is naturally aborted and repeated or reattempted. The former is called a continuous task, while the latter is called an episodic task. Episodic tasks have a start and end, while continuous tasks have a start but never end. The horizon (the number of steps) of episodic tasks is finite, while the horizon for continuous tasks is infinite. It turns out that discounting is necessary for theoretical guarantees for continuous tasks, which must be strictly &lt; 1, while for episodic tasks, it can be set to 1. We will deal mostly with episodic tasks.</p>
<h3 id="the-value-function-and-action-value-function">The Value Function and Action-Value Function</h3>
<p>The function that specifies each state's return when the agent <em>follows a specific policy</em> is called the <strong>value function</strong> and is denoted as <span class="arithmatex">\(v(s)\)</span>. On the other hand, we call the function that specifies the return of the current state given that the agent takes a specific action <span class="arithmatex">\(a\)</span> and then just follows a specific policy, the action-value function and is denoted as <span class="arithmatex">\(q(s,a)\)</span>. These two functions provide a great utility for us in guiding our search for an optimal policy. The action-value function can be directly utilised to provide a policy that greedily chooses the action with the maximum value; when we do so, we call the algorithms that follow this pattern a value-function algorithm. Alternatively, we can maximise the policy directly without having to maximise the expected return first. These types of algorithms that do so are called policy-gradient algorithms and depend on approximation.</p>
<p>We will largely deal with two types of algorithms: tabular algorithms, which use a tabular representation of the value function <span class="arithmatex">\(v\)</span> and the action-value function <span class="arithmatex">\(q\)</span>. These will be covered in the first three units. In the subsequent units, we cover the second type of algorithms that deal with function approximation, where representing the state and actions in a table is intractable or impractical. In these algorithms, we generalise the tabular algorithms that we cover in the first three units to be able to use the models that we covered in machine learning, such as linear regression models or neural networks</p>
<h2 id="tabular-representation-and-example">Tabular Representation and Example</h2>
<p>In the first three units, you will learn about the main ideas of reinforcement learning that use lookup tables. These tables identify a certain action that will be taken in a certain state and are called a policy. Our main concern is to design algorithms that <em>learn</em> a suitable policy.</p>
<p><strong>A Policy Lookup Table For Commuting to Work</strong></p>
<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td>have energy and have time</td>
<td>walk</td>
</tr>
<tr>
<td>have energy and no   time</td>
<td>cycle</td>
</tr>
<tr>
<td>no  energy  and have time</td>
<td>take a bus</td>
</tr>
<tr>
<td>no  energy  and no   time</td>
<td>take a taxi</td>
</tr>
</tbody>
</table>
<h2 id="unit-overview">Unit Overview</h2>
<p>More formally, in the first three units, we will study important reinforcement learning algorithms that use <em>tabular policy representation</em>. These algorithms learn a lookup table that identifies a suitable action that can be taken for a certain state. Our main concern is to design practical and efficient algorithms that can <em>learn</em> an optimal policy either via direct interaction with an environment, via an environment model that captures the dynamics of the environment, or both. An optimal policy is a policy that maximises the sum of discounted rewards obtained by following this policy.</p>
<p>The main underlying framework we assume is a Markov Decision Process (MDP). In a nutshell, as we said earlier this framework assumes that the probability of moving to the next state is only dependent on the current state and not on past states.</p>
<p>We start by covering non-associated problems, such as K-armed Bandit. These problems have no states. This will help us focus on the action space as it will be isolated from the state's effect. We then study how to solve MDP problems using Dynamic Programming (DP). DP assumes that we have a model of the environment that the agent is acting on. By model, we mean the <em>dynamics</em> or probabilities of landing in a state and obtaining a specific reward given an action and a previous state. This kind of conditional probability provides a comprehensive framework to reach an optimal policy, but it is hard to obtain in the real world. We then reside in sample methods, particularly Monte Carlo. This method allows us to gather samples of an agent running, making environmental decisions, and collecting rewards. We use averages to obtain an estimate of the discounted returns of an episode. We conclude our units by developing suitable core classes that allow us to study and demonstrate the advantages and disadvantages of these and other methods.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
        <script src="../../videos/my-video.mp4"></script>
      
    
  </body>
</html>
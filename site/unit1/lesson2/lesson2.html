
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../lesson1/lesson1.html">
      
      
        <link rel="next" href="../lesson3/lesson3.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>2. K-Arm Bandit - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-2-understanding-q-via-k-armed-bandit" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2. K-Arm Bandit
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson2.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Outcomes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivating-example" class="md-nav__link">
    <span class="md-ellipsis">
      Motivating Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Motivating Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-a" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario A
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-b" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario B
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-c" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario C
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#averaging-the-rewards-for-greedy-and-greedy-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Averaging the Rewards for Greedy and ε-Greedy Policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-armed-bandet-testbed-estimating-q-via-samples-average" class="md-nav__link">
    <span class="md-ellipsis">
      10-armed Bandet Testbed: Estimating Q via Samples Average
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10-armed Bandet Testbed: Estimating Q via Samples Average">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-the-experiencesampling" class="md-nav__link">
    <span class="md-ellipsis">
      Generate the experience(sampling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-the-bandit-q-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      Learning the bandit Q action-values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-runs-aka-trials" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple runs (aka trials)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-selection-policy-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Action Selection (Policy) Comparison:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-stationary-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Non-stationary Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Non-stationary Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compare-different-learning-rates" class="md-nav__link">
    <span class="md-ellipsis">
      Compare different learning rates
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policies-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Policies: Exploration vs. Exploitation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimistic-initial-values-as-an-exploration-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      Optimistic Initial Values as an Exploration Strategy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your Turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Outcomes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivating-example" class="md-nav__link">
    <span class="md-ellipsis">
      Motivating Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Motivating Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-a" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario A
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-b" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario B
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-c" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario C
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#averaging-the-rewards-for-greedy-and-greedy-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Averaging the Rewards for Greedy and ε-Greedy Policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-armed-bandet-testbed-estimating-q-via-samples-average" class="md-nav__link">
    <span class="md-ellipsis">
      10-armed Bandet Testbed: Estimating Q via Samples Average
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10-armed Bandet Testbed: Estimating Q via Samples Average">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-the-experiencesampling" class="md-nav__link">
    <span class="md-ellipsis">
      Generate the experience(sampling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-the-bandit-q-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      Learning the bandit Q action-values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-runs-aka-trials" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple runs (aka trials)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-selection-policy-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Action Selection (Policy) Comparison:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-stationary-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Non-stationary Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Non-stationary Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compare-different-learning-rates" class="md-nav__link">
    <span class="md-ellipsis">
      Compare different learning rates
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policies-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Policies: Exploration vs. Exploitation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimistic-initial-values-as-an-exploration-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      Optimistic Initial Values as an Exploration Strategy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your Turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="lesson-2-understanding-q-via-k-armed-bandit">Lesson 2- Understanding Q via K-armed Bandit</h1>
<p><img alt="image-3.png" src="image-3.png" /></p>
<p><img alt="image-2.png" src="image-2.png" /></p>
<p>In this lesson, you will learn about the k-armed bandit problem and its applications in reinforcement learning (RL). This problem is useful for understanding the basics of RL, particularly how an algorithm can <em>learn</em> an action-value function, commonly denoted as Q in RL.</p>
<h2 id="learning-outcomes">Learning Outcomes</h2>
<p>By the end of this lesson, you will:</p>
<ol>
<li>Understand the role the action-value function plays in RL and its relationship with a policy.</li>
<li>Appreciate the difference between stationary and non-stationary problems.</li>
<li>Understand how to devise a sample-averaging solution to approximate an action-value function.</li>
<li>Appreciate the different types of policies and the role a policy plays in RL algorithms.</li>
</ol>
<p>In this lesson, we develop the foundational concepts of actions and policies, which are the key elements that distinguish RL from other machine learning sub-disciplines. We study a simple yet effective problem: the k-armed bandit. This problem can be applied to scenarios such as a medical specialist deciding which treatment to administer to a patient from a set of medications, some of which they are trying for the first time (exploring).</p>
<p>Our toy problem is similar to the classic bandit problem, but with the assumption that there is a set of <strong>k</strong> actions the agent can choose from. The goal is to develop an effective policy that allows the agent to maximize its returns (wins). The bandit is assumed to have a Gaussian distribution centered around a mean reward, which differs for each action (arm). Each time an arm is pulled (or an action is taken, as we say in RL terminology), the bandit will return a reward (positive or negative) by drawing from its Gaussian reward distribution. The agent's task is to identify which action has the highest mean reward and consistently choose it to maximize its wins. Note that the distributions are fixed and do not change, though we will relax this assumption later.</p>
<p>Now, let’s get started!</p>
<h2 id="motivating-example">Motivating Example</h2>
<p>Let us assume that we have an armed bandit with two levers.</p>
<h3 id="scenario-a">Scenario A</h3>
<ul>
<li>We have a deterministic reward function that returns a reward of -5 for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1).</li>
<li>We have a deterministic reward function that returns a reward of 0 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2).</li>
</ul>
<p><strong>Question</strong>: What is the optimal policy for this bandit?</p>
<h3 id="scenario-b">Scenario B</h3>
<ul>
<li>We have a nondeterministic reward function that returns a reward of either -5 or 15, each with an equal probability of 0.5 for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1).</li>
<li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with an equal probability of 0.5 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2).</li>
</ul>
<p><strong>Questions</strong>:</p>
<ol>
<li>What is the net overall reward for actions <span class="arithmatex">\(a_1\)</span> and <span class="arithmatex">\(a_2\)</span>?</li>
<li>What is the optimal policy for this bandit?</li>
<li>How many optimal policies do we have for this bandit?</li>
</ol>
<h3 id="scenario-c">Scenario C</h3>
<ul>
<li>We have a nondeterministic reward function that returns a reward of either -5 or 15, with probabilities of 0.4 and 0.6 respectively for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1).</li>
<li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with a probability of 0.5 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2).</li>
</ul>
<p><strong>Questions</strong>:</p>
<ol>
<li>What is the net overall reward for actions <span class="arithmatex">\(a_1\)</span> and <span class="arithmatex">\(a_2\)</span>?</li>
<li>What is the optimal policy for this bandit?</li>
<li>How many optimal policies do we have for this bandit?</li>
<li>Can you find a way to represent this?</li>
</ol>
<hr />
<p>The figure below gives an insight about some of the above questions. Each line represents a bandit Q1 and Q2.</p>
<p><img alt="png" src="output_18_0.png" /></p>
<p>As can be seen, the two bandit functions intersect with each other at a probability of 0.5, meaning they are equivalent at this probability. For other probabilities, Bandit 1 is superior for <span class="arithmatex">\( pr &gt; 0.5 \)</span> (and hence the optimal policy will be to select this bandit always), while Bandit 2 is superior for <span class="arithmatex">\( pr &lt; 0.5 \)</span> (and hence the optimal policy will be to select this bandit always). However, bear in mind that we do not know the underlying probability beforehand, and we would need to try out both bandits to estimate their corresponding value functions in order to come up with a suitable policy.</p>
<p>Now, let's move on to covering the different concepts of the multi-armed bandit in more detail.</p>
<p>We proceed by developing simple functions for:
1. Averaging rewards for statinary policy.
2. Moving Average of rewards for non-stationary policy.</p>
<!-- 1. Returning an action from a stationary policy.
2. Returning a simple fixed reward from a stationary distribution. -->

<h2 id="averaging-the-rewards-for-greedy-and-greedy-policies">Averaging the Rewards for Greedy and ε-Greedy Policies</h2>
<p>A greedy policy is a simple policy that always selects the action with the highest action-value. On the other hand, an ε-greedy policy is similar to a greedy policy but allows the agent to take random exploratory actions from time to time. The percentage of exploratory actions is designated by ε (epsilon). Typically, we set ε to 0.1 (10%) or 0.05 (5%). A third type of greedy policy is the dynamic ε-greedy policy, which anneals or decays the exploration factor (ε) over time. In practice, the ε-greedy policy generally works well and often better than more sophisticated policies that aim to strike a balance between exploration and exploitation (where taking the greedy action is called exploitation, and taking other actions is called exploration). Regardless of the approach, we need to allow for some exploratory actions; otherwise, it would not be possible for the agent to improve its policy.</p>
<p>Below, we show an implementation of a bandit function that uses an ε-greedy policy. <code>nA</code> denotes the number of actions (the number of arms to be pulled). Since we are only dealing with actions, the Q-function has the form of Q(a). The armed bandit is a non-associative problem, meaning we do not deal with states. Later, we will address associative problems, where Q(s, a) has two inputs: the state and the action.</p>
<p>We now create a function that takes a bandit (in the form of a set of rewards, each corresponding to an action) and generates a value Q that quantifies the value/benefit obtained by taking each action. This is a simple improvement over the earlier code, where we calculated the expected reward of an action. This time, we choose actions <em>randomly instead of uniformly</em>, so we need to keep track of each action. At the end, we divide the sum of the obtained rewards by the count of each action to compute the average, which serves as a good estimator of the expected reward.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">r</span>  <span class="o">=</span> <span class="n">bandit</span>      <span class="c1"># the bandit is assumed to be a set of rewards for each action</span>
    <span class="n">nA</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>      <span class="c1"># number of actions</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span> <span class="c1"># action-values</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>  <span class="c1"># action-counts</span>
    <span class="n">avgR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># ε-greedy action selection (when ε=0 this turns into a greedy selection)</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
        <span class="n">C</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">avgR</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">avgR</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>


    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">avgR</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;average reward ε=</span><span class="si">%.2f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">ε</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Q</span><span class="o">/</span><span class="n">C</span>
</code></pre></div>
<p>Let us see how the Q_bandit_fixed will learn to choose the best action that yields the maximum returns.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="o">=</span><span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.7</span><span class="p">])</span>
</code></pre></div>
    array([0.09772727, 0.19874214, 0.699125  ])</p>
<p><img alt="png" src="output_37_1.png" /></p>
<p>As we can see it has improved to more than 0.6.</p>
<p>Let us see how the completely greedy policy would do on average:</p>
<p><div class="highlight"><pre><span></span><code><span class="n">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="o">=</span><span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.7</span><span class="p">],</span> <span class="n">ε</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
    array([0.0999001, 0.       , 0.       ])</p>
<p><img alt="png" src="output_40_1.png" /></p>
<p>As we can see it could not improve beyond 0.1</p>
<p>The main restriction in Q_bandit_fixed( ) function is that we assume that the reward function is fixed (hence the name Q_bandit_fixed). I.e., each action will receive a specific reward that does not change. This made the above solution a bit excessive since we could have just summed the rewards and then took their max. Nevertheless, this is useful as a scaffolding for our next step.</p>
<p>In the next section we develop a more general armed-bandit function that allows for the reward to vary according to some <em>unknown</em> distribution. The Q_banditAvg function will learn the distribution and find the best action that will allow it to obtain a maximal reward on average, similar to what we have done here.</p>
<h2 id="10-armed-bandet-testbed-estimating-q-via-samples-average">10-armed Bandet Testbed: Estimating Q via Samples Average</h2>
<p>Remember that Q represents the average/expected reward of an action from the start up until time step <span class="arithmatex">\(t\)</span> exclusive. Later we will develop this idea to encompass what we call the expected return of an action.
q* (qˣ in the code) represents the actual action-values for the armeds which are a set of reward that has been offset by a normal distribution randn(). This guarantees that on average the rewards of an action a is q*[a] but it will make it not easy for an observer to know exactly what the expected reward is.</p>
<h3 id="generate-the-experiencesampling">Generate the experience(sampling)</h3>
<p>generate an experience (rewards)</p>
<p>We use a function that get us a multivariate normal distribution of size k. Below see how we will generate a sample bandit with all of its possible data and plot it.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">generate_a_bandit_data</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">nA</span> <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">nA</span><span class="p">),</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">generate_a_bandit_data</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_48_0.png" /></p>
<h3 id="learning-the-bandit-q-action-values">Learning the bandit Q action-values</h3>
<p>Now we turn our attention to learning the Q function for an unknown reward distribution. Each action has its own Gaussian distribution around a mean but we could use other distributions. The set of means are themselves drawn from a normal distribution of mean 0 and variance of 1.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># learn the Q value for bandit and use it to select the action that will win the most reward</span>
<span class="k">def</span><span class="w"> </span><span class="nf">Q_banditAvg</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>    

    <span class="c1"># |A| and max(q*)</span>
    <span class="n">nA</span>   <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>            <span class="c1"># number of actions, usually 10</span>
    <span class="n">amax</span> <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>            <span class="c1"># the optimal action for this bandit</span>

    <span class="c1"># stats.</span>
    <span class="n">r</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>                  <span class="c1"># reward at time step t</span>
    <span class="n">a</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>       <span class="c1"># chosen action at time step t, needs to be int as it will be used as index</span>
    <span class="n">oA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>                  <span class="c1"># whether an optimal action is selected at time step t</span>

    <span class="c1"># estimates</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>                  <span class="c1"># action-values all initialised to 0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>                   <span class="c1"># actions selection count</span>


    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># action selection is what prevents us from vectorising the solution which must reside in a for loop</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>       <span class="c1"># explore</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>    <span class="c1"># exploit</span>

        <span class="c1"># update the stats.</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>
        <span class="n">oA</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span>        <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">==</span><span class="n">amax</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> 

    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">oA</span>
</code></pre></div>
<p>Let us now run this function and plot one 10-armed bandits</p>
<div class="highlight"><pre><span></span><code><span class="n">R</span><span class="p">,</span> <span class="n">oA</span> <span class="o">=</span> <span class="n">Q_banditAvg</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">R</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Average rewrads&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">oA</span><span class="p">,</span><span class="s1">&#39;.&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;%Optimal action&#39;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_53_1.png" /></p>
<p>Note how the % of optimal actions for one trial (run) takes either 1 or 0. This figure to the left seems not be conveying useful information. However when we average this percentage over several runs we will see a clear learning pattern. This is quite common theme in RL. We often would want to average a set of runs/experiments due to the stochasticity of the process that we deal with.</p>
<h3 id="multiple-runs-aka-trials">Multiple runs (aka trials)</h3>
<p>We need to average multiple runs to obtain a reliable unbiased results that reflect the expected performance of the learning algorithm. We do that via running the same function or algorithm multiple times, which is what the Q_bandits_runs function does. We do not show the code, we only show the results of executing it. The code can be found in the associated worksheet.</p>
<p>Note that we obtain different set of 10-bandit distributions and conduct an experimental run on them. Because all of them are normally standard distribution their sums of rewards (values) converges to the same quantity around 1.5.</p>
<h3 id="action-selection-policy-comparison">Action Selection (Policy) Comparison:</h3>
<p>Now we are ready to compare between policies with different exploration rates ε. Note that ε kw(keyword argument) has been passed on to the Q_bandit() function from the Q_bandits_runs() function.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.0&#39;</span> <span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_69_1.png" /></p>
<p>As we can see the ε=.1 exploration rate seems to give us a sweet spot. Try ε=.2 and see the effect.
This empirically indicates that indeed we need to allow the agent to explore in order to come up with a viable optimal or close to optimal policy.</p>
<h2 id="incremental-implementation">Incremental Implementation</h2>
<p>If we look at the sum</p>
<div class="arithmatex">\[
% \begin{aligned}
Q_{t+1}  = \frac{1}{t}\sum_{i=1}^{t}R_i = \frac{1}{t}\left(\sum_{i=1}^{t-1}R_i + R_t\right) 
        = \frac{1}{t}\left((t-1)\frac{\sum_{i=1}^{t-1}R_i}{t-1} + R_t\right) 
        = \frac{1}{t}\left(\left(t-1\right)Q_t + R_t\right) 
\]</div>
<div class="arithmatex">\[
Q_{t+1} = Q_t + \frac{1}{t}\left(R_t - Q_t\right) 
\]</div>
<!-- % \end{aligned} -->
<!-- $$ -->
<p>We can see that we can write the estimate in an incremental form that allows us to update our estimate <span class="arithmatex">\(Q_t\)</span> instead of recalculate the sum in each time step. This is very handy when it comes to efficiently implement an algorithm to give us the sum. Further, it  turns out that it also has other advantages. To realise this, note that the <span class="arithmatex">\(\frac{1}{t}\)</span> diminishes when <span class="arithmatex">\(t\)</span> grows, which is natural for averages. But if we want the latest rewards to have a bigger impact (weights) then we can simply replace this fraction by a constance<span class="arithmatex">\(\alpha\)</span> to obtain the following <strong>incremental update</strong></p>
<div class="arithmatex">\[
    Q_{t+1} = Q_t + \alpha\left(R_t - Q_t\right)
\]</div>
<p>Note that incremental updates plays a very important role in RL and we will be constantly seeking them due to their efficiency in online application.</p>
<p>Below we show the results of running a code that was designed to capture the ideas of incremental implementation via a function called Q_banditN. <em>We do not show the code of Q_banditN here</em></p>
<div class="highlight"><pre><span></span><code><span class="c1">#Q_bandits_runs(ε=.2)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_77_1.png" /></p>
<h2 id="non-stationary-problems">Non-stationary Problems</h2>
<p>The limitation of the above implementation is that it requires actions counts and when the underlying reward distribution changes (non-stationary reward distribution) it does not respond well to take these changes into account. A better approach when we are faced with such problems is to use a fixed size step &lt;1 instead of dividing by the actions count. This way, because the step size is small the estimate gets updated when the underlying reward distribution changes. Of course this means that the estimates will keep changing even when the underlying distribution is not changing, however in practice this is not a problem when the step size is small enough. This effectively gives more weights to recent updates which gives a good changes-responsiveness property for this and similar methods that use a fixed size learning step <span class="arithmatex">\(\alpha\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># returns one of the max Q actions; there is an element of stochasticity in this policy</span>
<span class="k">def</span><span class="w"> </span><span class="nf">greedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">):</span>   
    <span class="k">return</span> <span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="o">==</span><span class="n">Q</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>


<span class="c1"># returns the first max Q action most of the time (1-ε)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">εgreedy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">ε</span> <span class="k">else</span> <span class="n">randint</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">εgreedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">greedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">ε</span> <span class="k">else</span> <span class="n">randint</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_banditα</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span>  <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedy</span><span class="p">):</span>
    <span class="n">nA</span><span class="p">,</span> <span class="n">amax</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bandit_init</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">q0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># using a specific policy</span>
        <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">)</span>               

        <span class="c1"># get the reward from bandit</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]])</span>  


    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="o">==</span><span class="n">amax</span>
</code></pre></div>
<p>Note that the majority of RL problem are actually non-stationary. This is because, as we shall see later, when we gradually move towards an optimal policy by changing the Q action-values, the underlying reward distribution changes in response to taking actions that are optimal according to the current estimation. This is also the case here but in a subtle way.</p>
<h3 id="compare-different-learning-rates">Compare different learning rates</h3>
<p>Let us compare different <em>learning rates</em> α to see how our Q_bandits() function reacts to them. </p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.5&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_93_1.png" /></p>
<h3 id="policies-exploration-vs-exploitation">Policies: Exploration vs. Exploitation</h3>
<p>Getting the right balance between exploration and exploitation is a constant dilemma in RL.
One simple strategy as we saw earlier is to explore constantly occasionally ε% of the time! which we called ε-greedy. Another strategy is to insure that when we have multiple actions that are greedy we chose ebtween them equally and not bias one over the other. This is what we do in the greedyStoch policy below.</p>
<p>Let us now compare different exploration rates for this learning function. As before we show the results only not the code.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_83_1.png" /></p>
<h3 id="optimistic-initial-values-as-an-exploration-strategy">Optimistic Initial Values as an Exploration Strategy</h3>
<p>It turns out that we can infuse exploration in the RL solution by optimistically initialising the Q values.
This encourages the agent to explore due to its disappointment when its initial Q values are not matching the reward values that are coming from the ground (interacting with the environment). This intrinsic exploration motive to explore more actions at the start, vanishes with time when the Q values become more realistic. </p>
<p>This is a good and effective strategy for exploration. But of course it has its limitations, for example it does not necessarily work for if there a constant or renewed need for exploration. This could happen either when the task or the environment are changing. Below, we show the effect of optimistically initiating the Q values on the 10-armed bandit testbed. We can clearly see that without exploration i.e. when ε=0 and Q=5 initial values outperformed the exploratory policy ε=.1 with Q=0 initial values.</p>
<p>Ok, we will apply the same principle to stochastically return one of the max Q actions which is coded in greedyStoch() policy. This type of policy will prove useful later when we deal with control.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1  Realistic Q=0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedyStoch</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=0   Optimistic Q=5&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedyStoch</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_98_1.png" /></p>
<p>As we can see above the optimistic initialization has actually beaten the constant exploration rate and it constitutes a very useful trick for us to encourage the agent to explore while still acting greedily. Of course we can combine both strategies and we will leave this for you as a task. We will use this trick in our coverage of RL in later lessons.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson you have learned about the importance of the action value function Q and stationary and non-stationary reward distribution and how we can devise a general algorithms to address them and we concluded by showing an incremental learning algorithm to tackle the k-armed bandit problem. You have seen different exploration strategy and we extensively compared between exploration rates and learning rates for our different algorithms.</p>
<h2 id="further-reading">Further Reading</h2>
<p>For further information refer to chapters 1 and 2 of the <a href="(http://incompleteideas.net/book/RLbook2020.pdf)">rl book</a>. </p>
<!-- Please note that we approach the ideas in this lesson from a practical perspective, which complements the theoretical coverage in the textbook. -->

<h2 id="your-turn">Your Turn</h2>
<p><a href="../../workseets/worksheet2.ipynb">Worksheet2</a> implement the above concepts and more. Please experiment with the code and run it to get familiar with the essential concepts presented in the lessons.</p>
<!-- 1. Define a softmax policy as per its definition in eq. 2.11, then compare the Q_bandits_runs on ε-greedy and softmax policies.
1. Combine the optimistic initialisation and exploration rates and see how the bandit_Q function react to them.
 -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
        <script src="../../videos/my-video.mp4"></script>
      
    
  </body>
</html>
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-reinforcement-learning-and-robotics","title":"Welcome to Reinforcement Learning and Robotics!","text":"<p>Welcome to the Reinforcement Learning and Robotics module, where you learn the fundamentals of reinforcement learning(RL) with various simulaitons including robotics. The module focuses on RL as a general robust framework that allows us to deal with autonomous agent learning. </p> <p>In each unit, you will learn essential RL ideas and algorithms and see how these can be implemented in simplified environments. Each lesson involves some reading material, along with some videos explaining these lessons. This will then be followed by running and experimenting with the Jupyter notebooks we provide, which implement the main ideas you have read and show you the RL algorithms you studied in action. It is essential that you read the material and engage with these notebooks in the way you see fit. This includes studying and running the provided code to gain insight into the covered algorithms, observing the algorithm convergence behaviour, experimenting with the hyperparameter and their effect on the agent's behaviour, and taking a turn to implement some concepts that were left out for you. </p> <p>There are also some lessons that familiarise you with robotics as an application domain, which covers some simple concepts in robotics without delving deep into classical robotics, which is outside the scope of this module. Our units conclude with a simple, practical tutorial on utilising a simulated robot. You will use the provided code to control a simulated mobile robot called TurtleBot. The lessons are meant to gradually build your ability to deal with atonomous agents in a simplified environment. The final project will focus on comparing different RL solutions to solve a simulated robot navigation problem. We will provide you access to an Azure Ubuntu virtual machine already set up with ROS2 to run the sheets. You do not need to set up your own VM.</p> <p>Reinforcement Learning (RL) is dedicated to acquiring an optimal policy to enhance agent performance. Traditionally, RL involves the agent seeking an optimal policy to maximise cumulative discounted rewards garnered by navigating various environmental states. While the agent is commonly perceived as a physical entity interacting with its surroundings, it can also encompass abstract systems, with states representing system configurations or settings. Notably, recent RL advancements have ventured into novel territories, such as optimising language models like LLMs for perplexity or other metrics.</p> <p>In this module, our focus primarily revolves around simulated agents, aligning with the nature of our programme. However, the underlying principles remain universal and applicable across diverse scenarios. The concept of guiding learning through rewards is deeply ingrained in biological organisms, from complex human brains to single-cell organisms like amoebas, all driven by the innate urge to maximise survival and proliferation.</p> <p>While RL offers tremendous efficacy when configured appropriately, it is susceptible to spectacular failure when its conditions are unmet. Its inherent stochasticity adds another layer of complexity, contributing to its volatile nature. Nonetheless, this volatility serves as a catalyst for researchers to delve deeper into understanding the governing rules of RL processes.</p> <p>Our module adopts a pragmatic approach, aiming to provide you with a solid theoretical grounding in RL without overly delving into intricate mathematical details. Simultaneously, we will attempt to equip you with practical skills and techniques to harness RL's benefits effectively. This balanced approach ensures that you grasp RL's essence while gaining valuable real-world application tools.</p>    Wecome Video"},{"location":"index.html#rl-in-context-advantages-and-challenges","title":"RL in context: advantages and challenges","text":"<p>In our RL coverage, you will notice that we do not do classical robotics and we believe it is not the way to go except possibly for industrial robots. So, we do not need motion planning or accurate trajectory calculations/kinematics to allow the robot to execute a task, we simply let it interact with its environment and learn by itself how to solve the task and this is what reinforcement learning is about. In our coverage we also do not supervise or directly teach the robot, this type of interference is called imitation. </p> <p>This is all great but what about challenges? Obviously, we still have several challenges to this approach. One is the number of experiments required to learn the task which can be numerous, which exposes the physical agents (real robots) to the risk of wear and tear due to repetition and can be very lengthy and tedious for the human that is supervising the task. This can be partially overcome by starting in simulation and then moving to a real robot with a technique called sim-to-real where we can employ GANs(generative adversarial neural networks).  The other challenge is the time required for training regardless whether it is in simulation or in real scenarios. </p> <p>The origin of these problems is actually the exploitation/exploration needed by RL algorithms which is in the heart of what we will be exploring in all of our RL coverage. Reducing the amount of training required is important and remains an active area for research. One approach is via experience replay and the other is via eligibility traces as we shall see later.</p>"},{"location":"index.html#textbook","title":"Textbook","text":"<p>The primary textbook for this unit and the following ones is Introduction to Reinforcement Learning by Sutton and Barto, available online here. While reading the corresponding chapters is not required, they serve as valuable supplementary material to enhance your understanding and explore the subject in greater depth.</p> <p>list of symbols</p> <ul> <li>\\(v0\\): denotes an initial value</li> <li>\\(\u03b8\\): denotes a threshold</li> <li>\\(nS\\): denotes state space dimension </li> <li>\\(nA\\): denotes actions space dimension </li> <li>\\(nR\\): denotes rewards space dimension</li> <li>\\(nU\\): denotes the number of updates</li> <li> <p>goal: a terminal state</p> </li> <li> <p>\\(r\\): current step reward</p> </li> <li>\\(s\\): current step state</li> <li>\\(a\\): current step action</li> <li>\\(rn\\): next step reward</li> <li>\\(sn\\): next step state</li> <li> <p>\\(an\\): next step action</p> </li> <li> <p>\\(\u03b1\\): learning rate</p> </li> <li>\\(\u03b5\\): exploration rate</li> <li>\\(d\u03b1\\): decay factor for \u03b1</li> <li>\\(d\u03b5\\): decay factor for \u03b5</li> <li> <p>\\(G(t+1,t+n)\\): the return between time step t+1 and t+n</p> </li> <li> <p>Rs: is the sum of rewards of an episode</p> </li> <li>Ts: is the steps of a set of an episode</li> <li>Es: is the errors (RMSE) of an episode</li> </ul>"},{"location":"index.html#module-plan","title":"Module Plan","text":"<p>We cover the tabular solution methods in the first three units, while approximate solution methods will be covered in subsequent units.</p> <p>Tabular and approximate solution methods fall under two types of RL methods that we will attempt to deal with </p> <ol> <li>Prediction methods: AKA Policy Evaluation Methods that attempt to find the best estimate for the value-function \\(V\\) or action-value function \\(Q\\) for a policy \\(\\pi\\).</li> <li>Control methods: AKA Policy Improvement Methods that attempt to find the best policy \\(\\pi_*\\), often by starting from an initial policy and then moving into a better and improved policy.</li> </ol> <p>Control methods, or policy improvement methods, in turn, falls under two categories:</p> <ol> <li>Methods that improve the policy via improving the action-value function \\(Q\\)</li> <li>Methods that improve the policy directly \\(\\pi\\)</li> </ol> <p>We start by assuming that the policy is fixed. This will help us develop algorithms predicting the state\u2018s value function (expected return). Then, we will move to the policy improvement methods, i.e., methods that help us compare our policy with other policies and move to a better policy when necessary. We then move to seamlessly integrating both for control case (policy and value iteration methods). Finally, we cover policy gradient methods that improve the policy directly.</p> <p>Note that the guarantee from the policy-improvement theorem no longer applies when we move from the table representation of the value function for small state space to the parametric function approximation representation for large state space. This will encourage us to move to direct policy-improvement methods instead of improving the policy via improving the value function.</p>"},{"location":"index.html#table-of-contents","title":"Table of Contents","text":"<p>Unit 1</p> <ol> <li>Tabular Methods </li> <li>K-Arm Bandit </li> <li>MDP </li> <li>ROS </li> </ol> <p>Unit 2</p> <ol> <li>Dynamic Programming </li> <li>Monte Carlo </li> <li>Mobile Robots </li> </ol> <p>Unit 3</p> <ol> <li>Temporal Difference </li> <li>n-Step Methods </li> <li>Planning in RL (optional) </li> <li>Localisation and SLAM </li> </ol> <p>Unit 4</p> <ol> <li>Function Approximation Methods </li> <li>Linear Approximation for Prediction </li> <li>Linear Approximation for Control </li> </ol> <p>Unit 5</p> <ol> <li>Linear Approximation with Eligibility Traces (Prediction and Control) </li> <li>Nonlinear Approximation for Control </li> <li>Application on Robot Navigation </li> </ol> <p>Unit 6</p> <ol> <li>Application on Games (optional) </li> </ol>"},{"location":"index.html#code-structure-and-notebooks-dependecies","title":"Code Structure and Notebooks Dependecies","text":"<p>We have provided you with two RL libraries designed for this module. One has bespoke environments and one that has base RL classes that makes working with algorithms very easy and as close as it can be to just provide an update rule.</p> <p>Important note: Please place all worksheets in one folder, and inside this folder you must have the downloaded libraries folders (env and rl) to allow the imports to work appropriately.</p>"},{"location":"index.html#installing-other-libraries-that-will-be-needed-later","title":"Installing other libraries that will be needed later","text":"<pre><code>!pip install --upgrade pip\n!pip install opencv-python\n!pip install scikit-learn\n!pip install matplotlib\n!pip install tqdm \n</code></pre> <pre><code>!pip install jupyterlab\n!pip install jupyterthemes\n\n!jt -t solarizedl -T -N  # -T, -N keeps the toolbar and header\n</code></pre>"},{"location":"index.html#available-themes","title":"available themes:","text":"<ul> <li>oceans16 </li> <li>grade3 </li> <li>chesterish </li> <li>solarizedl </li> <li>solarizedd </li> <li>gruvboxl</li> <li>!jt -r # resets back to the default theme</li> </ul>"},{"location":"index.html#better-readability-and-audibility","title":"Better Readability and Audibility","text":"<p>For better readability and experience, please use Jupyter Lab or Vcode(if you are using Azure VM) to navigate between the different notebooks easily. If you want to use Jupyter Notebooks and not Jupyter Lab, we recommend increasing the cells' width for a better experience. We provided a function that increase your notebook width which is envoked automatically when you import an environment (grid particularly). You may want to utilise also the table of contents button in Jupetr Lab. For better audibility of the provided videos please click on the 'noise supression' button, you may want to speed up as per your need.</p>"},{"location":"unit1/lesson1/lesson1.html","title":"Lesson 1: Introduction to Tabular Methods in Reinforcement Learning","text":"<p>Unit 1 Learning outcomes</p> <p>By the end of this unit, you will be able to:  </p> <ol> <li>Explain the armed bandit problem and how isolating the action space simplifies decision-making.  </li> <li>Describe the value function and the action-value function, highlighting their essential roles in reinforcement learning (RL).  </li> <li>Differentiate between associative and non-associative problems in RL.  </li> <li>Analyze the theoretical foundations of RL, including Markov Decision Processes (MDPs) and the Bellman equation.  </li> <li>Compare prediction and control in RL settings, outlining their respective challenges and solutions.  </li> </ol> <p>In this unit, we start by covering a simplified RL settings in the form of common problems, called Armed-bandit problem. We then move into understanding the main mathematical framework underpining  reinforcement learning, namely Markov Decision Processes(MDPs). As always we take a balaced approach by discussing such concepts from a theoretical and practical perspectives. </p>"},{"location":"unit1/lesson1/lesson1.html#markov-property","title":"Markov property","text":"<p>RL has gained a lot of attention in recent years due to its unmatched ability to tackle difficult control problems with a minimal assumption about the settings and the environment that an agent works in. Controlling an agent (such as a simulated robot) is not trivial and can often require a specific setup and strong assumptions about its environment that make the corresponding solutions sometimes either difficult to attain or impractical in real scenarios. In RL, we try to minimise these assumptions and require that only the environment adheres to the Markov property. In simple terms, the Markov property assumes that inferring what to do (taking action) in a specific state can be fully specified by looking at this state and does not depend on other past states.</p>"},{"location":"unit1/lesson1/lesson1.html#elements-of-rl","title":"Elements of RL","text":"<p>In RL, we have mainly four elements that we deal with: states, actions and rewards and the policy. The state space is the space the agent operates in, whether physical or virtual. The state can represent something specific in the environment, an agent configuration or both. The actions are the set of decisions available for the agent to take that affect the state the agent is in and/or cause the environment to respond to it in a certain way, via a reward signal. An RL agent's main aim is to attain, usually via learning, a cohesive policy that allows it to achieve a specific goal of maximising its reward in the long and short terms. </p>"},{"location":"unit1/lesson1/lesson1.html#the-policy","title":"The Policy","text":"<p>This policy \\(\u03c0\\) can take a simple form \\(\\pi(s)=a\\) or symbolically \\(s \u2192 a\\), which means if the agent is in state \\(s\\), then take action \\(a\\). This type of policy is deterministic because the agent will definitely take the action \\(a\\) if it is in state \\(s\\). Below we show an example of a deterministic policy.</p> State Action \\(S_1\\) \\(A_1\\) \\(S_2\\) \\(A_2\\) \\(S_3\\) \\(A_2\\) <p>Another type of policy that we deal with is stochastic policy. A stochastic policy takes the form of \\(\\pi(a|s)\\), which represents the probability of taking action \\(a\\) given that the agent is in state \\(s\\). For such a policy, the agent draws from the set of available actions according to the conditional probability, which we call its policy. The higher the probability of an action, the more likely it will be chosen, when the probability is 1 it means the action will be taken definitely, when it is 0 it means it will not be taken. below we show an example of stochastic policy.</p> State Action Action's Probability \\(S_1\\) \\(A_1\\) .8 \\(S_1\\) \\(A_2\\) .2 \\(S_2\\) \\(A_1\\) .6 \\(S_2\\) \\(A_2\\) .4 \\(S_3\\) \\(A_1\\) 0. \\(S_3\\) \\(A_2\\) 1. <p>A deterministic policy is a special case of a stochastic policy with all of its actions' probabilities being 0 except one action.</p>"},{"location":"unit1/lesson1/lesson1.html#the-reward","title":"The Reward","text":"<p>The reward function can take the simple form of \\(r(s,a)\\) or symbolically \\((s,a) \u2192r\\), which is interpreted as follows: if the agent is in state \\(s\\) and applied action \\(a\\) it obtains a reward \\(r\\). This reward can be actual or expected. The general setting that we deal with is the probabilistic one with the form of \\(p(r|s,a)\\), which provides us with the probability of the agent obtaining reward \\(r\\) given it was in state \\(s\\) and applied action \\(a\\). </p>"},{"location":"unit1/lesson1/lesson1.html#the-transition-probability","title":"The Transition Probability","text":"<p>Another conditional probability that we deal with takes the form of \\(p(s\u2019|s,a)\\), which is the probability of transitioning to state \\(s\u2019\\) given the agent was in state \\(s\\) and applied action \\(a\\). This is called the transition probability. </p>"},{"location":"unit1/lesson1/lesson1.html#the-dynamics-of-an-enviornment","title":"The Dynamics of an Enviornment","text":"<p>Both the transition and reward probabilities can be inferred from a more general probability that specifies the dynamics of the environment. This is a joint conditional probability that takes the form of \\(p(s\u2019,r|s,a)\\). This probability is interpreted as the joint conditional probability of transitioning to state \\(s\u2019\\) and obtaining reward \\(r\\) | given that the agent was in state \\(s\\) and applied action \\(a\\). We will deal mostly with the dynamics in the second lesson of this unit. Bear in mind that obtaining the dynamics is difficult or intractable in most cases except for the simplest environment. Nevertheless, the dynamics are very useful theoretically for understanding the basic ideas of RL.</p>"},{"location":"unit1/lesson1/lesson1.html#the-reward-and-the-task","title":"The Reward and the Task","text":"<p>The reward function is strongly linked to the task that the agent is trying to achieve, this is the minimal information provided for the agent to indicate to it whether it is on the right track or not. We need to be careful not to devise a complicated reward function that directly supervise the agent. This is not only usually unnecessary, but it is also harmful for the agent perfromance, since when doing so, we may be directly solving the problem for the agent which defies the purpos of the RL framework. Instead, we would want the agent to solve the problem by utilising a simple reward signal and interacting with its environment to gain experience and sharpen and improve its decision-making policy. </p> <p>Improving the agent's decision-making involves two things: evaluating its current policy and changing it in a way that will improve its performance, basically collecting as much reward as possible in the long and short terms. This is where the rewards, particualrly the sum of rewards an agent can collect while achieving a task, plays a major role. </p> <p>In RL, we link the policy to maximising the discounted sum of the rewards an agent can obtain while moving towards achieving a task. The reward can be negative, and in this case, the agent will be trying to minimise the sum of negative rewards that it is collecting before terminating. </p> <p>The termination happens when the agent achieves the required task, has taken a pre-specified number of steps or consumed pre-set computational resources. </p>"},{"location":"unit1/lesson1/lesson1.html#types-of-rewards","title":"Types of Rewards","text":"<p>An example of a good simple reward is giving a robot a reward of 1 when it reaches a goal location and 0 otherwise. This is the most generic and most sparse reward we can set for an agent. It basically just tells the agent whether it succeeded or not. This kind of reward demonstrates the essential capabilities and advantage RL can provide in constrat to supervised learning. </p> <p>Another example is giving a robot a negative reward (penalty) of -1 for each step it takes before reaching the goal location, where the agent can be given a reward of 0 (no penalty), or positive reward. It is effectively informing the agent in each step whether it has achieved the task or not. This kind of reward is useful for situations that invloves achieving a task in a minimum number of steps. Therefore, it is a good fit for shortest path problems and navigation tasks.</p> <p>Both of these rewards have their advantages, and we will explore and experiment with them in our exercises. The advantage of the first type is its simplicity, generality and inforced sparsity, but it can take longer to explore the environment and longer to populate its estimations. The advantage of the second type is also its relative generality,albeit less generic than the first, and that it provides intermediate general information that the agent can immediately utilise to improve its policy before achieving the task or reaching the goal location. The second type is specifically useful for online learning and when we want to alleviate the agent from having to change its starting position to cover all possible starts. The first type is useful for studying the capabilities of a learning algorithm with minimum information.</p>"},{"location":"unit1/lesson1/lesson1.html#the-return","title":"The Return","text":"<p>The sum of rewards from a specific state to the end of the task is called the return. Because we do not know how long the agent may take to achieve the task and to fairly maximise the rewards and allow for variability, we discount the rewards so that more recent rewards have more effect than later rewards. Nevertheless, our aim is to maximise the rewards in the long run. To be more explicit, we call the sum of discounted rewards from the current state to the end of the task the return that the agent will obtain in the future- the whole idea is to predict these rewards and be able to collect as much as possible.</p>"},{"location":"unit1/lesson1/lesson1.html#the-task","title":"The Task","text":"<p>The task we give the agent can be a continuous, infinite interaction with the environment. It can also take the form of a task with a specific goal or termination state, and when it is reached, the task is naturally aborted and repeated or reattempted. The former is called a continuous task, while the latter is called an episodic task. Episodic tasks have a start and end, while continuous tasks have a start but never end. The horizon (the number of steps) of episodic tasks is finite, while the horizon for continuous tasks is infinite. It turns out that discounting is necessary for theoretical guarantees for continuous tasks, which must be strictly &lt; 1, while for episodic tasks, it can be set to 1. We will deal mostly with episodic tasks.</p>"},{"location":"unit1/lesson1/lesson1.html#the-value-function-and-action-value-function","title":"The Value Function and Action-Value Function","text":"<p>The function that specifies each state's return when the agent follows a specific policy is called the value function and is denoted as \\(v(s)\\). On the other hand, we call the function that specifies the return of the current state given that the agent takes a specific action \\(a\\) and then just follows a specific policy, the action-value function and is denoted as \\(q(s,a)\\). These two functions provide a great utility for us in guiding our search for an optimal policy. The action-value function can be directly utilised to provide a policy that greedily chooses the action with the maximum value; when we do so, we call the algorithms that follow this pattern a value-function algorithm. Alternatively, we can maximise the policy directly without having to maximise the expected return first. These types of algorithms that do so are called policy-gradient algorithms and depend on approximation.</p> <p>We will largely deal with two types of algorithms: tabular algorithms, which use a tabular representation of the value function \\(v\\) and the action-value function \\(q\\). These will be covered in the first three units. In the subsequent units, we cover the second type of algorithms that deal with function approximation, where representing the state and actions in a table is intractable or impractical. In these algorithms, we generalise the tabular algorithms that we cover in the first three units to be able to use the models that we covered in machine learning, such as linear regression models or neural networks</p>"},{"location":"unit1/lesson1/lesson1.html#tabular-representation-and-example","title":"Tabular Representation and Example","text":"<p>In the first three units, you will learn about the main ideas of reinforcement learning that use lookup tables. These tables identify a certain action that will be taken in a certain state and are called a policy. Our main concern is to design algorithms that learn a suitable policy.</p> <p>A Policy Lookup Table For Commuting to Work</p> State Action have energy and have time walk have energy and no   time cycle no  energy  and have time take a bus no  energy  and no   time take a taxi"},{"location":"unit1/lesson1/lesson1.html#unit-overview","title":"Unit Overview","text":"<p>More formally, in the first three units, we will study important reinforcement learning algorithms that use tabular policy representation. These algorithms learn a lookup table that identifies a suitable action that can be taken for a certain state. Our main concern is to design practical and efficient algorithms that can learn an optimal policy either via direct interaction with an environment, via an environment model that captures the dynamics of the environment, or both. An optimal policy is a policy that maximises the sum of discounted rewards obtained by following this policy.</p> <p>The main underlying framework we assume is a Markov Decision Process (MDP). In a nutshell, as we said earlier this framework assumes that the probability of moving to the next state is only dependent on the current state and not on past states.</p> <p>We start by covering non-associated problems, such as K-armed Bandit. These problems have no states. This will help us focus on the action space as it will be isolated from the state's effect. We then study how to solve MDP problems using Dynamic Programming (DP). DP assumes that we have a model of the environment that the agent is acting on. By model, we mean the dynamics or probabilities of landing in a state and obtaining a specific reward given an action and a previous state. This kind of conditional probability provides a comprehensive framework to reach an optimal policy, but it is hard to obtain in the real world. We then reside in sample methods, particularly Monte Carlo. This method allows us to gather samples of an agent running, making environmental decisions, and collecting rewards. We use averages to obtain an estimate of the discounted returns of an episode. We conclude our units by developing suitable core classes that allow us to study and demonstrate the advantages and disadvantages of these and other methods.</p>"},{"location":"unit1/lesson2/lesson2.html","title":"Lesson 2- Understanding Q via K-armed Bandit","text":"<p>In this lesson, you will learn about the k-armed bandit problem and its applications in reinforcement learning (RL). This problem is useful for understanding the basics of RL, particularly how an algorithm can learn an action-value function, commonly denoted as Q in RL.</p>"},{"location":"unit1/lesson2/lesson2.html#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you will:</p> <ol> <li>Understand the role the action-value function plays in RL and its relationship with a policy.</li> <li>Appreciate the difference between stationary and non-stationary problems.</li> <li>Understand how to devise a sample-averaging solution to approximate an action-value function.</li> <li>Appreciate the different types of policies and the role a policy plays in RL algorithms.</li> </ol> <p>In this lesson, we develop the foundational concepts of actions and policies, which are the key elements that distinguish RL from other machine learning sub-disciplines. We study a simple yet effective problem: the k-armed bandit. This problem can be applied to scenarios such as a medical specialist deciding which treatment to administer to a patient from a set of medications, some of which they are trying for the first time (exploring).</p> <p>Our toy problem is similar to the classic bandit problem, but with the assumption that there is a set of k actions the agent can choose from. The goal is to develop an effective policy that allows the agent to maximize its returns (wins). The bandit is assumed to have a Gaussian distribution centered around a mean reward, which differs for each action (arm). Each time an arm is pulled (or an action is taken, as we say in RL terminology), the bandit will return a reward (positive or negative) by drawing from its Gaussian reward distribution. The agent's task is to identify which action has the highest mean reward and consistently choose it to maximize its wins. Note that the distributions are fixed and do not change, though we will relax this assumption later.</p> <p>Now, let\u2019s get started!</p>"},{"location":"unit1/lesson2/lesson2.html#motivating-example","title":"Motivating Example","text":"<p>Let us assume that we have an armed bandit with two levers.</p>"},{"location":"unit1/lesson2/lesson2.html#scenario-a","title":"Scenario A","text":"<ul> <li>We have a deterministic reward function that returns a reward of -5 for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a deterministic reward function that returns a reward of 0 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Question: What is the optimal policy for this bandit?</p>"},{"location":"unit1/lesson2/lesson2.html#scenario-b","title":"Scenario B","text":"<ul> <li>We have a nondeterministic reward function that returns a reward of either -5 or 15, each with an equal probability of 0.5 for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with an equal probability of 0.5 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Questions:</p> <ol> <li>What is the net overall reward for actions \\(a_1\\) and \\(a_2\\)?</li> <li>What is the optimal policy for this bandit?</li> <li>How many optimal policies do we have for this bandit?</li> </ol>"},{"location":"unit1/lesson2/lesson2.html#scenario-c","title":"Scenario C","text":"<ul> <li>We have a nondeterministic reward function that returns a reward of either -5 or 15, with probabilities of 0.4 and 0.6 respectively for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with a probability of 0.5 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Questions:</p> <ol> <li>What is the net overall reward for actions \\(a_1\\) and \\(a_2\\)?</li> <li>What is the optimal policy for this bandit?</li> <li>How many optimal policies do we have for this bandit?</li> <li>Can you find a way to represent this?</li> </ol> <p>The figure below gives an insight about some of the above questions. Each line represents a bandit Q1 and Q2.</p> <p></p> <p>As can be seen, the two bandit functions intersect with each other at a probability of 0.5, meaning they are equivalent at this probability. For other probabilities, Bandit 1 is superior for \\( pr &gt; 0.5 \\) (and hence the optimal policy will be to select this bandit always), while Bandit 2 is superior for \\( pr &lt; 0.5 \\) (and hence the optimal policy will be to select this bandit always). However, bear in mind that we do not know the underlying probability beforehand, and we would need to try out both bandits to estimate their corresponding value functions in order to come up with a suitable policy.</p> <p>Now, let's move on to covering the different concepts of the multi-armed bandit in more detail.</p> <p>We proceed by developing simple functions for: 1. Averaging rewards for statinary policy. 2. Moving Average of rewards for non-stationary policy.</p>"},{"location":"unit1/lesson2/lesson2.html#averaging-the-rewards-for-greedy-and-greedy-policies","title":"Averaging the Rewards for Greedy and \u03b5-Greedy Policies","text":"<p>A greedy policy is a simple policy that always selects the action with the highest action-value. On the other hand, an \u03b5-greedy policy is similar to a greedy policy but allows the agent to take random exploratory actions from time to time. The percentage of exploratory actions is designated by \u03b5 (epsilon). Typically, we set \u03b5 to 0.1 (10%) or 0.05 (5%). A third type of greedy policy is the dynamic \u03b5-greedy policy, which anneals or decays the exploration factor (\u03b5) over time. In practice, the \u03b5-greedy policy generally works well and often better than more sophisticated policies that aim to strike a balance between exploration and exploitation (where taking the greedy action is called exploitation, and taking other actions is called exploration). Regardless of the approach, we need to allow for some exploratory actions; otherwise, it would not be possible for the agent to improve its policy.</p> <p>Below, we show an implementation of a bandit function that uses an \u03b5-greedy policy. <code>nA</code> denotes the number of actions (the number of arms to be pulled). Since we are only dealing with actions, the Q-function has the form of Q(a). The armed bandit is a non-associative problem, meaning we do not deal with states. Later, we will address associative problems, where Q(s, a) has two inputs: the state and the action.</p> <p>We now create a function that takes a bandit (in the form of a set of rewards, each corresponding to an action) and generates a value Q that quantifies the value/benefit obtained by taking each action. This is a simple improvement over the earlier code, where we calculated the expected reward of an action. This time, we choose actions randomly instead of uniformly, so we need to keep track of each action. At the end, we divide the sum of the obtained rewards by the count of each action to compute the average, which serves as a good estimator of the expected reward.</p> <pre><code>def Q_bandit_fixed(bandit, \u03b5=.1, T=1000):\n    r  = bandit      # the bandit is assumed to be a set of rewards for each action\n    nA = len(r)      # number of actions\n    Q = np.zeros(nA) # action-values\n    C = np.ones(nA)  # action-counts\n    avgR = np.zeros(T+1)\n\n    for t in range(T):\n        # \u03b5-greedy action selection (when \u03b5=0 this turns into a greedy selection)\n        if rand()&lt;= \u03b5: a = randint(nA)\n        else:          a = (Q/C).argmax()\n\n        Q[a] += r[a]\n        C[a] += 1\n        avgR[t+1] = (t*avgR[t] + r[a])/(t+1)\n\n\n    plt.plot(avgR, label='average reward \u03b5=%.2f'%\u03b5)\n    plt.legend()\n    return Q/C\n</code></pre> <p>Let us see how the Q_bandit_fixed will learn to choose the best action that yields the maximum returns.</p> <p><pre><code>Q_bandit_fixed(bandit=[.1, .2, .7])\n</code></pre>     array([0.09772727, 0.19874214, 0.699125  ])</p> <p></p> <p>As we can see it has improved to more than 0.6.</p> <p>Let us see how the completely greedy policy would do on average:</p> <p><pre><code>Q_bandit_fixed(bandit=[.1, .2, .7], \u03b5=0)\n</code></pre>     array([0.0999001, 0.       , 0.       ])</p> <p></p> <p>As we can see it could not improve beyond 0.1</p> <p>The main restriction in Q_bandit_fixed( ) function is that we assume that the reward function is fixed (hence the name Q_bandit_fixed). I.e., each action will receive a specific reward that does not change. This made the above solution a bit excessive since we could have just summed the rewards and then took their max. Nevertheless, this is useful as a scaffolding for our next step.</p> <p>In the next section we develop a more general armed-bandit function that allows for the reward to vary according to some unknown distribution. The Q_banditAvg function will learn the distribution and find the best action that will allow it to obtain a maximal reward on average, similar to what we have done here.</p>"},{"location":"unit1/lesson2/lesson2.html#10-armed-bandet-testbed-estimating-q-via-samples-average","title":"10-armed Bandet Testbed: Estimating Q via Samples Average","text":"<p>Remember that Q represents the average/expected reward of an action from the start up until time step \\(t\\) exclusive. Later we will develop this idea to encompass what we call the expected return of an action. q* (q\u02e3 in the code) represents the actual action-values for the armeds which are a set of reward that has been offset by a normal distribution randn(). This guarantees that on average the rewards of an action a is q*[a] but it will make it not easy for an observer to know exactly what the expected reward is.</p>"},{"location":"unit1/lesson2/lesson2.html#generate-the-experiencesampling","title":"Generate the experience(sampling)","text":"<p>generate an experience (rewards)</p> <p>We use a function that get us a multivariate normal distribution of size k. Below see how we will generate a sample bandit with all of its possible data and plot it.</p> <pre><code>def generate_a_bandit_data(q\u02e3, T):\n    nA = q\u02e3.shape[0]\n    r = multivariate_normal(q\u02e3, np.eye(nA), T)\n    plt.violinplot(r)\n    plt.show()\n\ngenerate_a_bandit_data(normal(0, 1, 10), T=100)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#learning-the-bandit-q-action-values","title":"Learning the bandit Q action-values","text":"<p>Now we turn our attention to learning the Q function for an unknown reward distribution. Each action has its own Gaussian distribution around a mean but we could use other distributions. The set of means are themselves drawn from a normal distribution of mean 0 and variance of 1.</p> <pre><code># learn the Q value for bandit and use it to select the action that will win the most reward\ndef Q_banditAvg(q\u02e3, \u03b5=.1, T=1000):    \n\n    # |A| and max(q*)\n    nA   = q\u02e3.shape[0]            # number of actions, usually 10\n    amax = q\u02e3.argmax()            # the optimal action for this bandit\n\n    # stats.\n    r  = np.zeros(T)                  # reward at time step t\n    a  = np.zeros(T, dtype=int)       # chosen action at time step t, needs to be int as it will be used as index\n    oA = np.zeros(T)                  # whether an optimal action is selected at time step t\n\n    # estimates\n    Q = np.zeros(nA)                  # action-values all initialised to 0\n    N = np.ones(nA)                   # actions selection count\n\n\n    for t in range(T):\n        # action selection is what prevents us from vectorising the solution which must reside in a for loop\n        if rand()&lt;= \u03b5: a[t] = randint(nA)       # explore\n        else:          a[t] = (Q/N).argmax()    # exploit\n\n        # update the stats.\n        r[t]  = bandit(a[t], q\u02e3)\n        oA[t] =        a[t]==amax\n\n        # update Q (action-values estimate)\n        N[a[t]] += 1\n        Q[a[t]] += r[t] \n\n    return r, oA\n</code></pre> <p>Let us now run this function and plot one 10-armed bandits</p> <pre><code>R, oA = Q_banditAvg(normal(0, 1, 10) , \u03b5=.1, T=1000)\n\nplt.gcf().set_size_inches(16, 3.5)\nplt.subplot(121).plot(R); plt.xlabel('Steps'); plt.ylabel('Average rewrads')\nplt.subplot(122).plot(oA,'.'); plt.xlabel('Steps'); plt.ylabel('%Optimal action')\n</code></pre> <p></p> <p>Note how the % of optimal actions for one trial (run) takes either 1 or 0. This figure to the left seems not be conveying useful information. However when we average this percentage over several runs we will see a clear learning pattern. This is quite common theme in RL. We often would want to average a set of runs/experiments due to the stochasticity of the process that we deal with.</p>"},{"location":"unit1/lesson2/lesson2.html#multiple-runs-aka-trials","title":"Multiple runs (aka trials)","text":"<p>We need to average multiple runs to obtain a reliable unbiased results that reflect the expected performance of the learning algorithm. We do that via running the same function or algorithm multiple times, which is what the Q_bandits_runs function does. We do not show the code, we only show the results of executing it. The code can be found in the associated worksheet.</p> <p>Note that we obtain different set of 10-bandit distributions and conduct an experimental run on them. Because all of them are normally standard distribution their sums of rewards (values) converges to the same quantity around 1.5.</p>"},{"location":"unit1/lesson2/lesson2.html#action-selection-policy-comparison","title":"Action Selection (Policy) Comparison:","text":"<p>Now we are ready to compare between policies with different exploration rates \u03b5. Note that \u03b5 kw(keyword argument) has been passed on to the Q_bandit() function from the Q_bandits_runs() function.</p> <pre><code>Q_bandits_runs(\u03b5=.1,  label='\u03b5=.1',  Q_bandit=Q_banditAvg)\nQ_bandits_runs(\u03b5=.01, label='\u03b5=.01', Q_bandit=Q_banditAvg)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5=.0' , Q_bandit=Q_banditAvg)\n</code></pre> <p></p> <p>As we can see the \u03b5=.1 exploration rate seems to give us a sweet spot. Try \u03b5=.2 and see the effect. This empirically indicates that indeed we need to allow the agent to explore in order to come up with a viable optimal or close to optimal policy.</p>"},{"location":"unit1/lesson2/lesson2.html#incremental-implementation","title":"Incremental Implementation","text":"<p>If we look at the sum</p> \\[ % \\begin{aligned} Q_{t+1}  = \\frac{1}{t}\\sum_{i=1}^{t}R_i = \\frac{1}{t}\\left(\\sum_{i=1}^{t-1}R_i + R_t\\right)          = \\frac{1}{t}\\left((t-1)\\frac{\\sum_{i=1}^{t-1}R_i}{t-1} + R_t\\right)          = \\frac{1}{t}\\left(\\left(t-1\\right)Q_t + R_t\\right)  \\] \\[ Q_{t+1} = Q_t + \\frac{1}{t}\\left(R_t - Q_t\\right)  \\] <p>We can see that we can write the estimate in an incremental form that allows us to update our estimate \\(Q_t\\) instead of recalculate the sum in each time step. This is very handy when it comes to efficiently implement an algorithm to give us the sum. Further, it  turns out that it also has other advantages. To realise this, note that the \\(\\frac{1}{t}\\) diminishes when \\(t\\) grows, which is natural for averages. But if we want the latest rewards to have a bigger impact (weights) then we can simply replace this fraction by a constance\\(\\alpha\\) to obtain the following incremental update</p> \\[     Q_{t+1} = Q_t + \\alpha\\left(R_t - Q_t\\right) \\] <p>Note that incremental updates plays a very important role in RL and we will be constantly seeking them due to their efficiency in online application.</p> <p>Below we show the results of running a code that was designed to capture the ideas of incremental implementation via a function called Q_banditN. We do not show the code of Q_banditN here</p> <pre><code>#Q_bandits_runs(\u03b5=.2)\nQ_bandits_runs(\u03b5=.1,  label='\u03b5 =.1',  Q_bandit=Q_banditN)\nQ_bandits_runs(\u03b5=.01, label='\u03b5 =.01', Q_bandit=Q_banditN)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5 =.0',  Q_bandit=Q_banditN)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#non-stationary-problems","title":"Non-stationary Problems","text":"<p>The limitation of the above implementation is that it requires actions counts and when the underlying reward distribution changes (non-stationary reward distribution) it does not respond well to take these changes into account. A better approach when we are faced with such problems is to use a fixed size step &lt;1 instead of dividing by the actions count. This way, because the step size is small the estimate gets updated when the underlying reward distribution changes. Of course this means that the estimates will keep changing even when the underlying distribution is not changing, however in practice this is not a problem when the step size is small enough. This effectively gives more weights to recent updates which gives a good changes-responsiveness property for this and similar methods that use a fixed size learning step \\(\\alpha\\).</p> <pre><code># returns one of the max Q actions; there is an element of stochasticity in this policy\ndef greedyStoch(Q):   \n    return choice(np.argwhere(Q==Q.max()).ravel())\n\n\n# returns the first max Q action most of the time (1-\u03b5)\ndef \u03b5greedy(Q, \u03b5):\n    return Q.argmax() if rand() &gt; \u03b5 else randint(Q.shape[0])\n\n\ndef \u03b5greedyStoch(Q, \u03b5):\n    return greedyStoch(Q) if rand() &gt; \u03b5 else randint(Q.shape[0])\n</code></pre> <pre><code>def Q_bandit\u03b1(q\u02e3,  \u03b1=.1, \u03b5=.1, T=1000, q0=0, policy=\u03b5greedy):\n    nA, amax, r, a, Q, _ = bandit_init(q\u02e3, T, q0)\n\n    for t in range(T):\n        # using a specific policy\n        a[t] = policy(Q, \u03b5)               \n\n        # get the reward from bandit\n        r[t]  = bandit(a[t], q\u02e3)\n\n        # update Q (action-values estimate)\n        Q[a[t]] += \u03b1*(r[t] - Q[a[t]])  \n\n\n    return r, a==amax\n</code></pre> <p>Note that the majority of RL problem are actually non-stationary. This is because, as we shall see later, when we gradually move towards an optimal policy by changing the Q action-values, the underlying reward distribution changes in response to taking actions that are optimal according to the current estimation. This is also the case here but in a subtle way.</p>"},{"location":"unit1/lesson2/lesson2.html#compare-different-learning-rates","title":"Compare different learning rates","text":"<p>Let us compare different learning rates \u03b1 to see how our Q_bandits() function reacts to them. </p> <pre><code>Q_bandits_runs(\u03b1=.1,  label='\u03b1=.1',  Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.01, label='\u03b1=.01', Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.5,  label='\u03b1=.5',  Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.0,  label='\u03b1=.0',  Q_bandit=Q_bandit\u03b1)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#policies-exploration-vs-exploitation","title":"Policies: Exploration vs. Exploitation","text":"<p>Getting the right balance between exploration and exploitation is a constant dilemma in RL. One simple strategy as we saw earlier is to explore constantly occasionally \u03b5% of the time! which we called \u03b5-greedy. Another strategy is to insure that when we have multiple actions that are greedy we chose ebtween them equally and not bias one over the other. This is what we do in the greedyStoch policy below.</p> <p>Let us now compare different exploration rates for this learning function. As before we show the results only not the code.</p> <p><pre><code>Q_bandits_runs(\u03b5=.1,  label='\u03b5=.1',  Q_bandit=Q_bandit\u03b1, T=5000, runs=2000)\nQ_bandits_runs(\u03b5=.01, label='\u03b5=.01', Q_bandit=Q_bandit\u03b1, T=5000, runs=2000)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5=.0',  Q_bandit=Q_bandit\u03b1, T=5000, runs=500)\n</code></pre> </p>"},{"location":"unit1/lesson2/lesson2.html#optimistic-initial-values-as-an-exploration-strategy","title":"Optimistic Initial Values as an Exploration Strategy","text":"<p>It turns out that we can infuse exploration in the RL solution by optimistically initialising the Q values. This encourages the agent to explore due to its disappointment when its initial Q values are not matching the reward values that are coming from the ground (interacting with the environment). This intrinsic exploration motive to explore more actions at the start, vanishes with time when the Q values become more realistic. </p> <p>This is a good and effective strategy for exploration. But of course it has its limitations, for example it does not necessarily work for if there a constant or renewed need for exploration. This could happen either when the task or the environment are changing. Below, we show the effect of optimistically initiating the Q values on the 10-armed bandit testbed. We can clearly see that without exploration i.e. when \u03b5=0 and Q=5 initial values outperformed the exploratory policy \u03b5=.1 with Q=0 initial values.</p> <p>Ok, we will apply the same principle to stochastically return one of the max Q actions which is coded in greedyStoch() policy. This type of policy will prove useful later when we deal with control.</p> <pre><code>Q_bandits_runs(\u03b5=.1, q0=0, label='\u03b5=.1  Realistic Q=0',  Q_bandit=Q_bandit\u03b1, policy=\u03b5greedyStoch)\nQ_bandits_runs(\u03b5=0 , q0=5, label='\u03b5=0   Optimistic Q=5', Q_bandit=Q_bandit\u03b1, policy=\u03b5greedyStoch)\n</code></pre> <p></p> <p>As we can see above the optimistic initialization has actually beaten the constant exploration rate and it constitutes a very useful trick for us to encourage the agent to explore while still acting greedily. Of course we can combine both strategies and we will leave this for you as a task. We will use this trick in our coverage of RL in later lessons.</p>"},{"location":"unit1/lesson2/lesson2.html#conclusion","title":"Conclusion","text":"<p>In this lesson you have learned about the importance of the action value function Q and stationary and non-stationary reward distribution and how we can devise a general algorithms to address them and we concluded by showing an incremental learning algorithm to tackle the k-armed bandit problem. You have seen different exploration strategy and we extensively compared between exploration rates and learning rates for our different algorithms.</p>"},{"location":"unit1/lesson2/lesson2.html#further-reading","title":"Further Reading","text":"<p>For further information refer to chapters 1 and 2 of the rl book. </p>"},{"location":"unit1/lesson2/lesson2.html#your-turn","title":"Your Turn","text":"<p>Worksheet2 implement the above concepts and more. Please experiment with the code and run it to get familiar with the essential concepts presented in the lessons.</p>"},{"location":"unit1/lesson3/lesson3.html","title":"3. MDP","text":""},{"location":"unit1/lesson3/lesson3.html#lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons","title":"Lesson 3- Markov Decision Processes, Dynamics and Bellman Equaitons","text":"<p>Learning outcomes</p> <ol> <li>understand MDP and its elements</li> <li>understand the return for a time step \\(t\\)</li> <li>understand the expected return of a state \\(s\\)</li> <li>understand the Bellman optimality equations</li> <li>become familiar with the different types of grid world problems</li> </ol> <p></p>"},{"location":"unit1/lesson3/lesson3.html#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>A Markov Decision Process (MDP) provides a mathematical framework to model decision-making problems where an agent interacts with an environment. It is characterised by a tuple \\( (\\mathcal{S}, \\mathcal{A} , \\mathcal{R}, p, \\gamma) \\) where:</p> <ul> <li>\\( \\mathcal{S} \\) is the set of states.  </li> <li>\\( \\mathcal{A} \\) is the set of actions.  </li> <li>\\( \\mathcal{R} \\) is the set of rewards.</li> <li> <p>p is the dynamics of the MDP   \\(     p(s', r | s, a) = \\Pr\\{ S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a \\}    \\)   constitutes the probability of transitioning to state \\( s' \\) and receiving reward \\( r \\), given that the agent is in state \\( s \\) and takes action \\( a \\).  Where:</p> <ul> <li>\\( S_t \\) is the state at time step \\( t \\),</li> <li>\\( A_t \\) is the action taken at time step \\( t \\),</li> <li>\\( S_{t+1} \\) is the next state after taking action \\( A_t \\) from \\( S_t \\).</li> <li>\\( R_{t+1} \\) is the next reward after taking action \\( A_t \\) from \\( S_t \\).</li> </ul> </li> </ul> <ul> <li> <p>\\( \\gamma \\in [0,1] \\) is the discount factor, which determines how much future rewards are valued relative to immediate rewards.  </p> <ul> <li>If \\( \\gamma = 0 \\), the agent considers only immediate rewards.  </li> <li>If \\( \\gamma \\approx 1 \\), the agent values long-term rewards more.  </li> </ul> </li> </ul> <p>The MDP framework is central to reinforcement learning, as it allows the agent to plan and optimize its actions over time to maximize the expected return.</p> <p>The dynamics of the MDP is a probability distribution and satisfies that</p> \\[ \\sum_{s'\\in\\mathcal{S}} \\sum_{r\\in\\mathcal{R}} p(s', r | s, a) = 1, \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\] <p>There are two types of dynamics that are actually related to each other:</p> <ul> <li> <p>Stochastic Dynamics:  \\( p(s', r | s, a) \\) defines a probability distribution over possible next states and rewards, this accomodate for a stochastic dynamics. This means the transition probability \\( p(s' | s, a) \\) is non-deterministic, so we do not know for sure which state s' the agent will end up with. Similarly, the reward can vary for the same transition.</p> </li> <li> <p>Deterministic Dynamics: If the dynamics \\( p(s', r | s, a) \\) assigns probability 1 to a single next state \\( s' \\) and reward \\( r \\), meaning the next state and reward are fully determined by \\( s \\) and \\( a \\), the dynamics is deterministic. This implies that \\( p(s' | s, a) \\) always results in the same \\( s' \\).  </p> </li> </ul> <p>Note that a deterministic dynamics is a special case of stochastic dynamics. Also note that in a finite MDP, the sets of states, actions, and rewards \\((\\mathcal{S, A}\\), and \\(\\mathcal{R})\\) all have a finite number of elements. </p>"},{"location":"unit1/lesson3/lesson3.html#transition-and-reward","title":"Transition and Reward","text":"<p>The transition describe how the environment behaves when the agent takes an action in a given state. The transition function \\( p(s' | s, a) \\) specifies the probability of transitioning from state \\( s \\) to state \\( s' \\) when the agent takes action \\( a \\).</p> <p>Formally, the transition function is expressed as:</p> \\[ p(s' | s, a) = Pr(S_{t+1} = s' | S_t = s, A_t = a) \\] <ul> <li>Markov Property: The environment satisfies the Markov Property, means that the next state depends only on the current state and action, not on the history of previous states or actions.</li> </ul> <p>Example: In a grid world, if the agent is at state \\( s = (2, 2) \\) and takes action \\( a = \\text{move left} \\), the transition probability might be deterministic:</p> \\[ p(s' | (2, 2), \\text{move left}) =  \\begin{cases}  1 &amp; \\text{if } s' = (1, 2) \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>This means that the agent always moves from \\( (2, 2) \\) to \\( (1, 2) \\) when taking the action \"move left\".</p> <p>The propability \\( p(s' | s, a) \\) is called the transition function, representing the probability of transitioning from state \\( s \\) to state \\( s' \\) after taking action \\( a \\). Since the dynamics \\( p(s', r | s, a) \\) gives the joint probability of the next state and reward, we can obtain the transition probability by summing over all possible rewards (called marginalising the reward):</p> <p>[     p(s' | s, a) = \\sum_r p(s', r | s, a)   ]</p> <p>This expresses the probability of transitioning to state \\( s' \\) given \\( s \\) and \\( a \\), regardless of the reward received.</p> <p>The Markov Property asserts that the future state depends only on the current state and action, not on any previous states or actions. This is a core assumption in MDPs and ensures that the system has no memory of past actions or states.</p> <p>Formally:</p> \\[ p(S_{t+1} | S_t, A_t, \\dots, S_0, A_0) = p(S_{t+1} | S_t, A_t) \\] <p>In many MDPs, the transition and reward functions are stationary, meaning that they do not change over time. This ensures that the transition probabilities and rewards are the same at every time step.</p> <p>Formally:</p> \\[ p(s' | s, a) = p(s' | s, a) \\quad \\forall t \\]"},{"location":"unit1/lesson3/lesson3.html#the-policy-and-its-stationarity","title":"The Policy and its Stationarity","text":"<p>A policy in reinforcement learning is a strategy or function that defines the agent's actions at each state in an environment. Mathematically, a policy is often represented as \\( \\pi(a|s) \\), where \\( s \\) is a state and \\( a \\) is an action. The policy \\( \\pi(a|s) \\) gives the probability of taking action \\( a \\) when in state \\( s \\). </p> <p>A stationary policy is one where the action probabilities depend only on the current state and remain constant over time. Formally, a stationary policy satisfies:</p> \\[ \\pi_t(a|s) = \\pi(a|s) \\quad \\text{for all time steps} \\, t \\] <p>This means the policy does not change as the environment evolves. This is common in many reinforcement learning settings where the dynamics of the problem do not change over time.</p> <p>In contrast, a non-stationary policy is one where the action probabilities can change with time:</p> \\[ \\pi_t(a|s) \\neq \\pi_{t'}(a|s) \\quad \\text{for some} \\, t \\neq t' \\] <p>This occurs when the policy is adapted or modified based on external factors, such as learning or changes in the environment. A non-stationary policy is useful in situations where the environment or the agent's understanding of it evolves over time.</p>"},{"location":"unit1/lesson3/lesson3.html#the-expected-reward","title":"The Expected Reward","text":"<p>The expected reward define the expected reward that the agent receives when it takes an action in a given state and transitions to a new state. The expected reward function \\( r(s, a, s') \\) specifies the reward the agent is expected to recieve when transitioning from state \\( s \\) to state \\( s' \\) after taking action \\( a \\).</p> <p>Formally, the reward function is expressed as:</p> \\[     r(s, a, s') = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s'] \\] <p>Example: If the agent takes action \\( a = \\text{move right} \\) from state \\( s = (1, 1) \\), the reward function might be: \\(r((1, 1), \\text{move right}, (2, 1)) = 10\\). Indicating that moving to the goal state \\( (2, 1) \\) yields a reward of 10. Conversely, if the agent moves to a dangerous state: \\(r((1, 1), \\text{move left}, (0, 1)) = -5\\). The agent receives a penalty of -5.</p> <p>The expected reward function \\( r(s, a, s') \\) can be derived from the joint transition-reward probability \\( p(s', r | s, a) \\) as follows:  </p> \\[ r(s, a, s') = \\sum_r r \\cdot p(s', r | s, a) \\] <p>This formula represents the expected reward received when transitioning to state \\( s' \\) from state \\( s \\) after taking action \\( a \\), by summing over all possible rewards weighted by their probabilities.</p> \\[     r(s, a) = \\sum_{s'} p(s' | s, a) r(s, a, s') \\] <p>which represents the average reward expected when taking action \\( a \\) in state \\( s \\), considering all possible next states weighted by their transition probabilities.  </p> <p>From the above can you work out hwo to calculate  \\( r(s, a) \\)  from \\( p(s', r | s, a) \\). </p> <p>The expected reward function \\( r(s, a) \\) can be computed from the joint transition-reward probability \\( p(s', r | s, a) \\) as follows:</p> \\[ r(s, a) = \\sum_{s'} \\sum_r r \\cdot p(s', r | s, a) \\] <p>This formula represents the expected reward for taking action \\( a \\) in state \\( s \\), by summing over all possible next states \\( s' \\) and rewards \\( r \\), weighted by their probabilities.</p>"},{"location":"unit1/lesson3/lesson3.html#the-return-g_t","title":"The Return \\(G_t\\)","text":"<p>The return of time step \\(t\\) is defiend as the sum of actual rewards the agent recieves form current time step up until the end of the horizon (end of agent episode or infinitely).</p> \\[     G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... + \\gamma^{T-t-1} R_{T} \\] <p>Accordingly at time step \\(t+1\\) we have:</p> \\[     G_{t+1} =  R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ... + \\gamma^{T-t-2} R_{T} \\] <p>Hence by multiplying \\(G_{t+1}\\) by \\(\\gamma\\) and adding R_{t+1} we get</p> \\[     G_t = R_{t+1} + \\gamma G_{t+1} \\] <p>The above equation is the most important equation in RL that the Bellman Equations are built on it. In turn, we build all of our incremental updates in RL on Bellman optimality equation</p> <p>In the video below we talk more about this important concept.</p>    Returns 1     Return 2"},{"location":"unit1/lesson3/lesson3.html#monotonicity-of-g_t-in-mdps","title":"Monotonicity of \\(G_t\\) in MDPs","text":"<p>The return \\(G_t\\) in an MDP follows the recursive formula:  \\( G_t = R_{t+1} + \\gamma G_{t+1} \\) where rewards at each step can be either positive or negative. To calculate \\(G_t\\) we will go backwards, i.e. we will need to calculate \\(G_{t+1}\\) to be able to calculate \\(G_t\\) due to the incremental form of \\(G_t\\) where we have that \\(G_t = R_{t+1} + \\gamma G_{t+1}\\).</p>"},{"location":"unit1/lesson3/lesson3.html#monotonic-decrease-of-g_t","title":"Monotonic Decrease of \\(G_t\\)","text":"<p>\\(G_t\\) is monotonically decreasing iff:  \\( G_t &lt; \\frac{R_t}{1 - \\gamma} \\quad \\forall t, \\quad \\text{and } G_T = R_T &gt; 0 \\).</p> <p>For example:</p> <ul> <li>If \\( R_t = 1 \\quad \\forall t \\quad\\) and \\( \\gamma = 0.9 \\), then \\( G_t \\) is bounded above by \\(\\frac{1}{1 - 0.9}=10 \\).  </li> <li>If \\( R_t = 1 \\quad \\forall t \\quad\\) and \\( \\gamma = 0.99 \\), then \\( G_t \\) is bounded above by   ?.  </li> <li>More generally, when \\( 1 - \\gamma = 1/\\beta \\), then \\(G_t&lt; \\beta R_t  \\).  </li> </ul> <p>Exercise: calculate the bound when \\( R_t = 1 \\quad \\forall t \\quad\\) and \\( \\gamma = 0.99 \\).</p>"},{"location":"unit1/lesson3/lesson3.html#monotonic-increase-of-g_t","title":"Monotonic Increase of \\(G_t\\)","text":"<p>\\(G_t\\) is monotonically increasing iff:  \\( G_t &gt; \\frac{R_t}{1 - \\gamma}  \\quad \\forall t, \\quad \\text{and } G_T = R_T &lt; 0 \\).</p> <ul> <li>If \\( R_t = -1 \\quad \\forall t \\quad\\) and \\( \\gamma = 0.9 \\), then \\( G_t \\) is bounded below by -10.  </li> <li>More generally, when \\( 1 - \\gamma = 1/\\beta \\), then \\( G_t &gt; \\beta R_t  \\).  </li> </ul>"},{"location":"unit1/lesson3/lesson3.html#proof-for-monotonically-decreasing-case-optional","title":"Proof for Monotonically Decreasing case (optional)","text":"<p>Assume \\( G_t \\) is strictly decreasing:  \\(   0&lt; G_{t+1} &lt; G_t \\quad \\forall t \\). </p> <p>Substituting \\( G_t \\) by R_{t+1} + \\gamma G_{t+1} as per the recursive return formula: </p> <p>\\( G_{t+1} &lt; R_{t+1} + \\gamma G_{t+1}  \\)</p> <p>\\( G_{t+1} (1 - \\gamma) &lt; R_{t+1}  \\)</p> <p>\\( G_{t+1} &lt; \\frac{R_{t+1}}{1 - \\gamma}  \\) </p> <p>Thus, the condition is necessary and sufficient for monotonic decrease.  </p> <p>For example, if \\( R_{t+1} = 1 \\) and \\( \\gamma = 0.9 \\): hence \\( \\frac{1}{1 - 0.9} &gt; G_{t+1} \\implies 10 &gt; G_{t+1} \\). </p> <p>The proof for the increasing case is similar.</p>"},{"location":"unit1/lesson3/lesson3.html#monotonicity-of-g_t-in-sparse-mdp-rewards","title":"Monotonicity of \\(G_t\\) in Sparse MDP Rewards","text":"<p>For sparse end-of-episode rewards, where \\( R_t = 0 \\;\\; \\forall t &lt; T \\) and \\( R_T &gt; 0 \\):  </p> <ul> <li> <p>\\( G_t \\) is monotonically increasing:  \\(   G_t \\leq G_{t+1}, \\quad \\forall t &lt; T.   \\)</p> </li> <li> <p>If \\( \\gamma &lt; 1 \\), then \\( G_t \\) is strictly increasing: \\(   G_t &lt; G_{t+1}.   \\)</p> </li> <li> <p>\\(G_t = \\gamma^{T-t-1} R_T.\\) </p> <ul> <li>If \\( R_T = 1 \\), then \\( G_t = \\gamma^{T-t-1} \\).  </li> <li>If \\( R_T = -1 \\), then \\( G_t = -\\gamma^{T-t-1} \\).  </li> </ul> </li> </ul>"},{"location":"unit1/lesson3/lesson3.html#proof-for-sparse-rewards-optional","title":"Proof for Sparse Rewards (optional)","text":"<p>From the recursive definition: \\( G_t = R_{t+1} + \\gamma G_{t+1} \\). Since \\( R_{t+1} = 0 \\) for \\( t &lt; T \\): \\( G_t = \\gamma G_{t+1} \\quad \\forall t &lt; T \\). </p> <p>Since \\( \\gamma \\leq 1 \\), then \\( G_t \\leq G_{t+1} \\).  </p> <p>Using the full return formula: \\( G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T-t-1} R_T \\). </p> <p>Since \\( R_{t+1} = 0 \\;\\; \\forall t &lt; T  \\) then \\(G_t = \\gamma^{T-t-1} R_T.\\) </p>"},{"location":"unit1/lesson3/lesson3.html#implications-for-rl-agents","title":"Implications for RL Agents","text":"<p>This gives us guidance on the type of behaviour that we expect our agent to develop when we follow one of these reward regimes (sparse or non-sparse). </p> <p>The above suggests that for sparse end-of-episode rewards, decisions near the terminal state(s) have far more important effects on the learning process than earlier decisions. While for non-sparse positive rewards MDPs, earlier states have higher returns and hence more importance than near terminal states. </p> <p>If we want our agent to place more importance on earlier states, and near-starting state decisions, then we will need to utilise non-sparse (positive or negative) rewards. Positive rewards encourage repeating certain actions that maintain the stream of positive rewards for the agent. An example will be the pole balancing problem. Negative rewards, encourage the agent to speed up towards ending the episode so that it can minimise the number of negative rewards received.</p> <p>When we want our agent to place more importance for the decisions near the terminal states, then a sparse reward is more convenient. Sparse rewards are also more suitable for offline learning as they simplify the learning and analysis of the agent's behaviour. Non-sparse rewards suit online learning on the other hand, because they give a quick indication of the agent behaviour suitability and hence speed up the early population of the value function.</p> <p>In summary:</p> <ul> <li>Sparse rewards emphasise decisions near terminal states, making them better suited for offline learning.  </li> <li>Non-sparse rewards emphasise earlier decisions, making them better for online learning and faster value estimation.  </li> <li>Positive rewards encourage sustaining good behavior (e.g., pole balancing).  </li> <li>Negative rewards encourage minimising episode duration (e.g., escape or navigation problems).  </li> </ul>"},{"location":"unit1/lesson3/lesson3.html#the-expected-return-function-v","title":"The Expected Return Function V","text":"<p>Once we move form an actul return that comes froma an actual experience at time step \\(t\\) to try to estimate this return, we move to an expectaiton function. This function, traditionally called the value function v, is an important function. But now isntead of tying the value of the return to a particular experience at a step t which would be less useful in generalising the lessons an agent can learn from interacting with the environment, it makes more sense to ty this up to a certain state \\(s\\). This will allow the agent to learn a useful expectation of the return(discounted sum of rewards) for a particualr state when the agent follows a policy \\(\\pi\\). I.e. we are now saying that a we will get an expected value of the return for a particular state under a policy \\(\\pi\\). So we moved from subscripting by a step \\(t\\) into passing a state \\(s\\) to the function and subscripting by a policy.</p> \\[ \\begin{equation}     v_{\\pi}(s) = \\mathbb{E}_{\\pi}(G_t)   \\label{eq:v}  %\\tag{1} \\end{equation} \\] <p>Equation \\(\\eqref{eq:v}\\) gives the definition of v function.</p> <p>In the following video we tackle this idea in more details.</p>    Returns Expectation and Sampling  <p>For algorithms like Value Iteration, it is important that the MDP is irreducible (all states are reachable from any other state) and aperiodic (there are no cycles of fixed lengths that prevent convergence).</p>"},{"location":"unit1/lesson3/lesson3.html#the-bellman-equations","title":"The Bellman Equations","text":"<p>The Bellman equations provide recursive relationships between the value of a state (or state-action pair) and the values of neighboring states. These equations are fundamental in solving MDPs and are the basis for many reinforcement learning algorithms.</p> <p>The value function \\( V_{\\pi}(s) \\) represents the expected return starting from state \\( s \\) and following policy \\( \\pi \\). The Bellman equation for \\( V_{\\pi}(s) \\) is:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma V^\\pi(S_{t+1}) \\mid S_t = s \\right] \\] <p>By unfolding the expectation operator with the help of the dynamics and the policy \\(\\pi\\) we have:</p> \\[ V^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V^\\pi(s') \\right] \\] <p>This equation expresses the expected value of a state under a policy \\(\\pi\\) by summing over all actions \\(a\\), next states \\(s'\\) and rewards \\(r\\), weighted by their respective probabilities.</p> <p>Where: - \\( V_{\\pi}(s) \\) is the value of state \\( s \\) under policy \\( \\pi \\), - \\( r(s, a, s') \\) is the immediate reward for transitioning from \\( s \\) to \\( s' \\) after action \\( a \\), - \\( \\gamma \\) is the discount factor, and - \\( p(s' | s, a) \\) is the transition probability.</p> <p>The Q-function \\( Q_{\\pi}(s, a) \\) represents the expected return after taking action \\( a \\) in state \\( s \\) and then following policy \\( \\pi \\). The Bellman equation for \\( Q_{\\pi}(s, a) \\) is:</p> \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a \\right] \\] <p>By incorporating the expectation over the policy \\(\\pi\\)for future actions, we get:</p> \\[ Q^\\pi(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' \\mid s') Q^\\pi(s', a') \\right] \\] <p>Where: - \\( Q_{\\pi}(s, a) \\) is the action-value function, - The terms \\( r(s, a, s') \\), \\( \\gamma \\), and \\( p(s' | s, a) \\) are the same as in the value function equation.</p>    Bellman v     Bellman q simple"},{"location":"unit1/lesson3/lesson3.html#bellman-optimality-equations","title":"Bellman Optimality Equations","text":"<p>The Bellman optimality equations describe the relationship between the optimal value function \\( V^*(s) \\) or the optimal Q-function \\( Q^*(s, a) \\) and the transition and reward dynamics. These equations are used to compute the optimal policy that maximizes the expected return.</p> \\[ V_*(s) = \\max_{a} \\mathbb{E} \\left[ R_{t+1} + \\gamma V_*(S_{t+1}) \\mid S_t = s, A_t = a \\right] \\] \\[ Q_*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q_*(S_{t+1}, a') \\mid S_t = s, A_t = a \\right] \\] <p>By substituting for the expectation operator, we have:</p> \\[ V_*(s) = \\max_{a} \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma V_*(s') \\right] \\] \\[ Q_*(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma \\max_{a'} Q_*(s', a') \\right] \\] <ul> <li>The max operator ensures that the agent chooses the action \\( a \\) that maximizes the expected return.</li> </ul>    Bellman Optimality 1     Bellman Optimality 2  <p>You can adjust the video settings in SharePoint (speed up to 1.2 and reduce the noise if necessary)</p> <p>Exercise 1: If you realise there is a missing symbol in the [video: Bellman Equation for v] last equations, do you know what it is and where it has originally come from?</p> <p>Exercise 2: Can you derive Bellman Optimality Equation for \\(q(s,a)\\) from first principles?</p> <p>Bellman Optimality for q from first principles can be found in this optional video.</p>    MDP Bellman Equation for q from first principle"},{"location":"unit1/lesson3/lesson3.html#grid-world-environments","title":"Grid World Environments","text":"<p>Ok, so now we are ready to tackle the practicals, please go ahead and download the worksheet and run and experiement with the provided code to build some grid world environments and visualise them and make a simple robot agent takes some steps/actions within these environments!.</p> <p>You will need to download a python library (grid.py) that we bespokley developed to help you run RL algorithms on toy problems and be abel to easily visualise them as needed, the code is optimised to run efficiently and you will be able to use these environmnets to test different RL algorithms extensively. Please place the library in the same directory of the worksheet. In general it would be a good idea to place all worksheets and libraries provided in one directory. This will make importing and runing code easier and more streamlined.</p>"},{"location":"unit1/lesson3/lesson3.html#summary","title":"Summary","text":"<p>The Markov Decision Process (MDP) framework models decision-making problems where an agent interacts with an environment. It includes dynamics \\( p(s', r | s, a) \\), which define the probability of transitioning to state \\( s' \\) and receiving reward \\( r \\) given that the agent is in state \\( s \\) and takes action \\( a \\). The Bellman equations provide recursive relationships for computing the value of states or actions, while the Bellman optimality equations help find the optimal policy. Key properties of MDPs include the Markov Property, stationarity, and the stochastic nature of the dynamics. Understanding these dynamics and equations is fundamental to reinforcement learning algorithms designed to find optimal decision-making strategies.  </p> <p>Further Reading: For further info please refer to chapter 3 of the Sutton and Barto book. </p>"},{"location":"unit1/lesson3/lesson3.html#your-turn","title":"Your turn","text":"<p>Go ahead and play around with some grid world environment by executing and experiementing with the code in worksheet3.</p>"},{"location":"unit1/lesson4/lesson4.html","title":"ROS 2 Basics and Fundamentals","text":"<p>Please see the following video to get started with teh VM. </p>    Wecome Video  <p>You should recieve an email inviting you to have access to an Azure VM. The VM has ROS 2 Foxy Fitzroy already installed. ROS2 commands need to be run from the terminal not from a conda-activated terminal (due to compatibility), and they use the default system Python 3.8. The VM has the libraries required for ROS2 along with TurtleBot3 installed with the worlds required for assessment.</p> <p>We have tested the notebooks on Python 3.8, so they should work smoothly for higher versions. Note that ROS2 code must be run with the default VM Python3.8 kernel. For the best experience, use VScode</p> <p>The machine has decent cores and memory (according to Azure 4 cores | 8GB RAM | 128GB Standard SSD). The VM has Ubuntu 20 and Xfce (Xubuntu) interface due to its lightweight (to give you the best experience remotely- to come as close as a local machine feeling) and it is tailored to give the same feeling as the usual Ubuntu Genome. You can run hardinfo in the terminal to check the VM specs. I hope you will enjoy it. </p> <p>To access the VM, please use the usual remote desktop app available on your system. You will receive an email with access to your VM. The username is rl, and the password is rl@ros2. </p> <p>You will have sudo access. Please apply caution when dealing with the system and avoid installing packages so as not to break it, which can be time-consuming. You will have around a 40-hour time limit, so please be mindful not to leave the system running unless necessary so as not to run out of time. Usually, you would want time for running the exercises and save plenty of time (\u00bd) for your project training (this is where the VM will be most useful).</p> <p>If the VM becomes corrupted for some reason, then you can reimage it by going to Azure Lab page and selecting the three dots, then reimage. That will cause all the data you have on the machine to be lost. You are advised to back up your data, you may want to use OneDrive or other backup methods.</p>"},{"location":"unit1/lesson4/lesson4.html#introduction-to-ros-2","title":"Introduction to ROS 2","text":"<p>Robot Operating System (ROS) 2 is an open-source framework for building robot applications. It provides a collection of tools, libraries, and conventions to help developers build robot software. ROS 2 is the next generation of ROS, with improvements in architecture, communication, and performance.</p>"},{"location":"unit1/lesson4/lesson4.html#key-concepts-in-ros-2","title":"Key Concepts in ROS 2","text":"<ol> <li> <p>Node:    A node is a fundamental unit in ROS 2. It is a process that performs computation. Nodes can communicate with each other through topics, services, and actions.</p> </li> <li> <p>Package:    A package is a collection of files that support a specific functionality. A package may include nodes, libraries, configuration files, and more.</p> </li> <li> <p>Topic:    Topics are a means of communication between nodes. Nodes can send data to topics (publishing) or receive data from topics (subscribing). They are used for asynchronous communication in ROS 2.</p> </li> <li> <p>Publisher and Subscriber:</p> </li> <li>Publisher: A node that sends data to a topic.</li> <li> <p>Subscriber: A node that receives data from a topic.</p> </li> <li> <p>Service:    Services are a synchronous communication mechanism in ROS 2 where a request is made by one node and a response is provided by another.</p> </li> <li> <p>Action:    Actions are similar to services but allow for more complex interactions, such as monitoring progress, canceling tasks, or receiving feedback.</p> </li> </ol>"},{"location":"unit1/lesson4/lesson4.html#setting-up-ros-2","title":"Setting Up ROS 2","text":"<p>You do not have to set up anything since you are given access to the ready setup VM. Before starting, ensure that you have ROS 2 installed on your system. </p> <p>Usually you do not need to source the ROS 2 workspace by running the following command:</p> <p><pre><code>source /opt/ros/foxy/setup.bash\n</code></pre> because we stored this in a file that will be run everytie you open the terminal. </p> <p></p> <p>picture credit</p>"},{"location":"unit1/lesson4/lesson4.html#your-turn","title":"Your turn","text":"<p>Now go ahead and try doing worksheet4 to experiement more with some of the ros concepts. ROS Worksheet 4: Robotics Operating System</p> <ul> <li>Explore other features of the Turtlesim package, such as controlling the turtle\u2019s position.</li> <li>Learn about ROS 2 services and actions for more interactive communication between nodes.</li> <li>Start developing your own custom ROS 2 nodes and packages for real-world robotics applications.</li> </ul>"},{"location":"unit1/lesson4/lesson4.html#summary","title":"Summary","text":"<p>In this lesson, you have learned the following ROS 2 concepts:</p> <ul> <li>Nodes: We created a TurtleControl node that publishes messages.</li> <li>Packages: We created a package to encapsulate our node.</li> <li>Topics: We used the /turtle1/cmd_vel topic to control the turtle.</li> </ul> <p>By following this tutorial, you have gained a basic understanding of how to interact with ROS 2 using packages, nodes, and topics. </p>"},{"location":"unit1/lesson4/worksheet4.html","title":"Running Turtlesim in ROS 2 Foxy: A Step-by-Step Tutorial","text":"<p>learning outcomes:</p> <p>By the end of this worksheet, you will be better able to:</p> <ul> <li>explain the purpose and function of the Robot Operating System (ROS)</li> <li>describe how \"messages\", \"topics\", and \"message types\" work in ROS</li> <li>demonstrate skills to use ROS messages to make two ROS processes communicate</li> <li>Turtlesim is a simple simulation tool in ROS 2 that lets you interact with a turtle in a 2D environment. This tutorial will guide you through running the Turtlesim node and interacting with it using the ROS 2 terminal commands.</li> </ul>"},{"location":"unit1/lesson4/worksheet4.html#step-1-setup-your-ros-2-workspace","title":"Step 1: Setup Your ROS 2 Workspace","text":"<p>Before running Turtlesim, ensure your ROS 2 Foxy environment is set up.</p> <p>In a terminal, source your ROS 2 Foxy installation:</p> <p><pre><code>source /opt/ros/foxy/setup.bash\n</code></pre> If you have a ROS 2 workspace, source it as well:</p> <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre>"},{"location":"unit1/lesson4/worksheet4.html#step-2-launch-turtlesim","title":"Step 2: Launch Turtlesim","text":"<p>Now you are ready to launch the turtlesim_node.</p> <p>To run Turtlesim, use the following command in a terminal:</p> <pre><code>ros2 run turtlesim turtlesim_node\n</code></pre> <p>This will open a window displaying a turtle in a 2D world.</p>"},{"location":"unit1/lesson4/worksheet4.html#step-3-interact-with-the-turtle","title":"Step 3: Interact with the Turtle","text":"<p>Now that Turtlesim is running, you can interact with the turtle using ROS 2 commands and topics.</p> <p>You can view the active topics in your ROS 2 environment using the following command:</p> <pre><code>ros2 topic list\n</code></pre> <p>This will list all topics being used. You should see something like:</p> <pre><code>/clock\n/turtle1/cmd_vel\n/turtle1/pose\n</code></pre> <ul> <li>/turtle1/cmd_vel: The topic for controlling the turtle\u2019s velocity.</li> <li>/turtle1/pose: The topic for getting the turtle\u2019s position and orientation.</li> </ul>"},{"location":"unit1/lesson4/worksheet4.html#move-the-turtle-using-velocity-commands","title":"Move the Turtle Using Velocity Commands","text":"<p>You can use the cmd_vel topic to control the turtle\u2019s movement. First, let\u2019s send a velocity command to move the turtle forward. To send a forward velocity to the turtle, use the following command:</p> <pre><code>ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.0}, angular: {z: 0.0}}\"\n</code></pre> <ul> <li>linear.x = 2.0: Moves the turtle forward with a speed of 2.0.</li> <li>angular.z = 0.0: No rotation (straightforward).</li> </ul> <p>You will see the turtle start moving forward in the Turtlesim window.</p>"},{"location":"unit1/lesson4/worksheet4.html#stop-the-turtle","title":"Stop the Turtle","text":"<p>To stop the turtle, publish a message with zero velocity:</p> <pre><code>ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.0}, angular: {z: 0.0}}\"\n</code></pre> <p>The turtle will stop moving.</p>"},{"location":"unit1/lesson4/worksheet4.html#rotate-the-turtle","title":"Rotate the Turtle","text":"<p>To rotate the turtle, you can publish a message that applies angular velocity:</p> <pre><code>ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.0}, angular: {z: 1.0}}\"\n</code></pre> <ul> <li>linear.x = 0.0: No forward movement.</li> <li>angular.z = 1.0: Rotates the turtle clockwise.</li> </ul> <p>You will see the turtle start rotating in the Turtlesim window.</p> <p>3.3. View Turtle\u2019s Pose To view the turtle\u2019s current position and orientation, you can use the pose topic:</p> <pre><code>ros2 topic echo /turtle1/pose\n</code></pre> <p>This will display real-time information about the turtle, such as its position (x, y) and orientation (theta), as well as its linear and angular velocities.</p> <p>For example:</p> <pre><code>x: 5.544\ny: 5.544\ntheta: 0.0\nlinear_velocity: 2.0\nangular_velocity: 0.0\n</code></pre> <p>This information updates every time the turtle moves.</p>"},{"location":"unit1/lesson4/worksheet4.html#step-4-shutdown-the-turtlesim-node","title":"Step 4: Shutdown the Turtlesim Node","text":"<p>Once you are done, you can stop the Turtlesim node by pressing Ctrl+C in the terminal where the ros2 run turtlesim turtlesim_node command is running.</p>"},{"location":"unit1/lesson4/worksheet4.html#summary","title":"Summary","text":"<p>In this tutorial, you learned how to:</p> <ul> <li>Launch the Turtlesim node in ROS 2 Foxy.</li> <li>Interact with the turtle by publishing velocity commands through the terminal.</li> <li>View the turtle\u2019s pose by subscribing to the /turtle1/pose topic.</li> <li>Stop and control the turtle's movement using the ros2 topic pub command.</li> </ul> <p>This is a simple demonstration of how to control a simulated robot in ROS2 using the terminal. We will provide you with a way to control a simulated robot using python code in the next tutorial. </p>"},{"location":"unit2/lesson5/lesson5.html","title":"5. Dynamic Programming","text":""},{"location":"unit2/lesson5/lesson5.html#lesson-5-dynamic-programming-model-based-approach","title":"Lesson 5 - Dynamic Programming: Model-Based Approach","text":"<p>Unit 2: Learning Outcomes </p> <p>By the end of this unit, you will be able to:</p> <ol> <li>Compute the value function for a given policy in tabular settings.  </li> <li>Implement control methods that infer an agent\u2019s policy from an action-value function.  </li> <li>Explain the concept of Generalized Policy Iteration (GPI) and how it underpins many RL methods.  </li> <li>Compare full-backup action-value-based control methods with direct policy estimation control methods.  </li> <li>Evaluate how Monte Carlo (MC) methods provide unbiased but high-variance estimates through interaction with the environment.  </li> <li>Analyze how REINFORCE achieves unbiased but high-variance policy gradient estimation through interaction with the environment.  </li> </ol> <p>In the first lesson, you looked at a basic RL problem, the k-arm bandit, which involves only actions and no states (non-associative problem). In general, in RL, we are faced with different situations, and we need to take different actions in each situation in order to achieve a certain goal. This general type of environment with states and actions imposes a different flavor to the solution we can design. From now on, we will tackle associative problems. For associative problems, there are two approaches:</p> <ol> <li>Model-based approach  </li> <li>Model-free approach  </li> </ol> <p>In this lesson, we will take the first approach. We will learn how to use a model of the environment to solve an RL problem. The model is given in the form of the dynamics of the environment. These usually come in the form of 4 dimensions of conditional probability involving an answer to the following question: what is the probability of obtaining a certain reward \\(r\\) in a certain state \\(s'\\) given that the agent was previously in a state \\(s\\) and applied action \\(a\\)?</p> <p>We will assume that there is already a model for the environment and try to take advantage of this model to come up with the best policy. Nevertheless, we will see simple ways to build such models and come back to this question later when we tackle planning algorithms in RL.</p> <p>Plan</p> <p>As usual, in general, there are two types of RL problems that we will attempt to design methods to deal with:</p> <ol> <li> <p>Prediction problem For these problems, we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p> </li> <li> <p>Control problems For these problems, we will design Value Iteration methods which utilize the idea of Generalized Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy, then moving to a better and improved policy by choosing a greedy action often.</p> </li> </ol>"},{"location":"unit2/lesson5/lesson5.html#inducing-the-dynamics-by-interacting-with-the-environment","title":"Inducing the dynamics by interacting with the environment","text":"<p>We cover obtaining the dynamics from an actual environment in the practicals. We will use mainly the random walk environment and the grid world environment to generate their dynamics. These are deterministic, simple environments. Nevertheless, they are very useful to demonstrate the ideas of RL. Note that when we move to the real world, the dynamics become much more complex, and building or obtaining the dynamics becomes impractical in most cases. </p> <p>Therefore, towards that end, instead of dealing directly with the environment's dynamics, we will see later how we can substitute this requirement by having to interact with the environment to gain experience, which will help us infer a good estimate of the expected value function (discounted sum of rewards), which in turn will help us to infer a close to optimal policy for the task at hand. </p> <p>The exercise of dealing with probabilities and then using them in designing a Dynamic Programming solution is valuable since most of the other solutions utilize the basic ideas (policy iteration, value iteration algorithms, and policy improvement theorem) that we cover here and will mainly show us that we can devise a form of Bellman equation that is suitable for interaction, where we use sampling, model-free algorithms instead of using probabilities (dynamics), model-based algorithms. </p> <p>Dynamic programming suffers from what Bellman described as the curse of dimensionality, which indicates that the computational resources required to solve a problem grow exponentially with the dimensionality of the problem. So, in our case, the dimensionality is the number of states (as well as actions and rewards). For example, if the dynamic programming solution computational complexity is \\(2^{|S|}\\) and the number of states \\(|S| = 10\\), then it costs \\(2^{10} = 1024\\), but when the number of states \\(|S|\\) grows to 100, the cost becomes \\(2^{100} = 1267650600228229401496703205376\\).</p>"},{"location":"unit2/lesson5/lesson5.html#sources-of-stochasticity-dynamics-and-policy","title":"Sources of Stochasticity - Dynamics and Policy","text":"<p>One important point to make is that stochasticity comes from different elements of the MDP and from the policy itself.</p> <ol> <li>There might be stochasticity in the dynamics at the state transition level, where applying action \\(a\\) in a state \\(s\\) may cause the agent to transition to different states, each with a different probability.</li> <li>There might be stochasticity in the dynamics at the reward level, where applying action \\(a\\) in a state \\(s\\) may result in different rewards, each with a certain probability.</li> <li>There might be stochasticity in the policy itself, where the policy applies different actions in a state \\(s\\) with different probabilities.</li> <li>There might be stochasticity or randomness in observing the current state due to the complexity of the state space. For example, when a robot moves around in the environment, after a while, we cannot reliably designate its position from its motor encoders even when we know the start position due to dead-reckoning. This is called partial observability, and there is a framework called BOMDP (Partially Observable MDP) to tackle this problem. However, we will not study this branch. The field is divided about the necessity of BOMDP, with a line of thought that considers that we can overcome this difficulty by encoding our states differently but staying in the MDP framework.</li> </ol> <p>These sources of stochasticity dictate using suitable techniques to obtain the dynamics and to evaluate or improve stochastic and deterministic policies. You will see an implementation of this in the associated worksheet.</p>"},{"location":"unit2/lesson5/lesson5.html#dynamic-programming-methods","title":"Dynamic Programming Methods","text":"<p>Okay, so we are ready now to move to Dynamic Programming algorithms to solve the RL problem of finding the best estimate of a value function and/or finding an optimal policy. Dynamic Programming (DP) refers to a collection of algorithms used for solving Markov Decision Processes (MDPs). DP methods rely on the principle of optimality and require a known model of the environment (transition probabilities and rewards). The main algorithms used in DP are Policy Evaluation, Policy Iteration, and Value Iteration.</p>      Dynamic Programming 1         Dynamic Programming 2"},{"location":"unit2/lesson5/lesson5.html#policy-evaluation","title":"Policy Evaluation","text":"<p>The first step to improving any policy is to evaluate how good or bad the policy is for the given task. This fundamental question can be addressed by tying up the task with a reward function that basically rewards the agent for achieving the task or a subtask that leads to the final goal. The agent's aim then becomes to collect as many rewards as possible (or to incur as few losses as possible), which should help the agent achieve the given task. One example is when a robot is moving in an environment, and we want it to reach a specific location; then we can reward/punish the robot for each step that is taking it closer to the goal or away from it. But this awareness of the goal location is usually difficult to attain in real environments. Hence it is replaced by rewarding the agent when it reaches the goal or punishing the agent for each step taken without reaching the goal location.</p> <p>We can devise an evaluation strategy based on the discounted sum of rewards the agent is expected to collect while executing the task. The strategy depends on the dynamics of the environment. You may want to read section 4.1 and come back here to continue reading the code for the policy evaluation algorithm to get an insight into how it works.</p>      Policy Evaluation    <p>In summary, policy evaluation computes the state-value function \\( V^{\\pi} \\) for a given policy \\( \\pi \\). It determines how good it is to follow a specific policy in an MDP. Below we show the pseudocode for this algorithm. Bellman equation is the basis of the update used in the policy evaluation algorithm.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-evaluation-algorithm","title":"Policy Evaluation Algorithm:","text":"<p>\\( \\begin{array}{ll} \\textbf{Algorithm: }  \\text{Policy Evaluation} \\\\ \\textbf{Input: }  \\text{MDP } (S, A, R, p, \\gamma), \\text{ Policy } \\pi, \\text{ Threshold } \\theta \\\\ \\textbf{Initialize: }  V(s) \\leftarrow 0, \\forall s \\in S \\\\ \\textbf{Repeat:}  \\\\ \\quad \\Delta \\leftarrow 0 \\\\ \\quad \\textbf{For all } s \\in S:  \\\\ \\quad \\quad v \\leftarrow V(s) \\\\ \\quad \\quad V(s) \\leftarrow \\sum_{a \\in A} \\pi(a | s) \\sum_{s', r} p(s',r | s, a) [r + \\gamma V(s')] \\\\ \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|) \\\\ \\textbf{Until:}  \\Delta &lt; \\theta \\\\ \\textbf{Return:}  V^{\\pi}(s), \\forall s \\in S \\end{array} \\)</p> <p>The existence and uniqueness of \\(v_\\pi\\) are guaranteed as long as either  \\(\\gamma&lt; 1\\) or eventual termination is guaranteed from all states under the policy \\(\\pi\\). Note that \\(\\gamma\\) must be \\(&lt; 1\\) to guarantee convergence of the Bellman equation because, in general, we do not know whether the policy guarantees reaching a terminal (goal) state; if we do, then \\(\\gamma=1\\) is okay.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-improvement-theorem","title":"Policy Improvement Theorem","text":"<p>The Policy Improvement Theorem states that if for all states:</p> \\[ Q_{\\pi}(s, a^*) &gt; V_{\\pi}(s) \\] <p>where \\( a^* = \\arg\\max_a Q_{\\pi}(s, a) \\), then choosing \\( a^* \\) instead of the action dictated by \\( \\pi \\) results in a strictly better policy. This theorem justifies Policy Iteration.</p> <p>Based on this important theorem, we can then devise an algorithm that improves our policy. A natural way to do this is to first evaluate the current policy, then improve it, then evaluate the improved policy, and then improve it, and so on until the policy stabilizes, which means we reached an optimal policy that cannot be improved anymore. This idea of iteratively improving the policy constitutes the base for two algorithms that we cover: Policy Iteration and Value Iteration.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-iteration","title":"Policy Iteration","text":"<p>Now that we know how to evaluate a policy, it is time to improve it. Policy iteration is a fundamental algorithm. It explicitly and iteratively tries first to reach a highly accurate estimate of the value function of the current policy, then it tries to improve the policy by maximizing the probability of greedy actions as per the current value function. Evaluating the current policy fully and then improving it via policy iteration can be inefficient, but it shows the fundamental ideas behind reinforcement learning. </p> <p>Policy Iteration is an iterative process to find the optimal policy \\( \\pi^* \\). It consists of two alternating steps:</p> <ol> <li>Policy Evaluation: Compute \\( V^{\\pi} \\) using the policy evaluation algorithm.</li> <li>Policy Improvement: Improve the policy by acting greedily with respect to \\( V^{\\pi} \\):    [    \\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V(s')]    ]</li> <li>Repeat until \\( \\pi \\) converges to \\( \\pi^* \\).</li> </ol> <p>So, as you can see, the policy improvement step is based on the Bellman Optimality equation. Below we show the pseudocode for this algorithm.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-iteration-algorithm","title":"Policy Iteration Algorithm:","text":"<p>\\( \\begin{array}{ll} \\textbf{Algorithm: }  \\text{Policy Iteration} \\\\ \\textbf{Initialize: } \\pi \\text{ arbitrarily} \\\\ \\textbf{Repeat:}  \\\\ \\quad \\text{Policy Evaluation (using the above algorithm)} \\\\ \\quad \\text{Policy Improvement:} \\\\ \\quad \\quad \\textbf{For all } s \\in S: \\\\ \\quad \\quad \\quad \\pi'(s) \\leftarrow \\arg\\max_a \\sum_{s', r} p(s',r | s, a) [r + \\gamma V(s')] \\\\ \\quad \\quad \\quad \\textbf{If } \\pi'(s) \\neq \\pi(s) \\text{ then policy is not stable} \\\\ \\textbf{Until:} \\pi \\text{ is stable} \\\\ \\textbf{Return:} \\pi^*, V^{\\pi^*} \\end{array} \\)</p>"},{"location":"unit2/lesson5/lesson5.html#policy-iteration-on-a-maze-grid-world","title":"Policy Iteration on a Maze Grid World","text":"<p>Let us try it on a slightly more complex environment, such as the maze.</p> <pre><code>\u03c0 = Policy_iteration(env=maze(), show=True)[0]\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 2:\npolicy improvement stopped @ iteration 16:\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#value-iteration","title":"Value Iteration","text":"<p>Our final step to fully develop the ideas of dynamic programming is to shorten the time it takes for a policy to be evaluated and improved. One simple idea we will follow here is to slightly improve the evaluation and immediately improve the policy. We do these two steps iteratively until our policy has stopped improving. This is a very effective strategy because we do not wait until the policy is fully evaluated to improve it; we weave and interleave the two loops together in one loop.</p> <p>Value Iteration is a special case of Policy Iteration where policy evaluation is truncated to a single update per state. Instead of evaluating a policy to convergence, we update the value function directly: [ V(s) \\leftarrow \\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V(s')] ] This is a nifty idea since we are saving on most of the policy evaluation and suffice with just one step of evaluation per iteration that also incorporates a policy improvement step. The Bellman Optimality equation is the basis of the update used in the value iteration algorithm. Below we show this algorithm.</p>"},{"location":"unit2/lesson5/lesson5.html#value-iteration-algorithm","title":"Value Iteration Algorithm:","text":"<p>\\( \\begin{array}{ll} \\textbf{Algorithm: }  \\text{Value Iteration} \\\\ \\textbf{Input: } \\text{MDP } (S, A, R, p, \\gamma), \\text{ Threshold } \\theta \\\\ \\textbf{Initialize: } V(s) \\leftarrow 0, \\forall s \\in S \\\\ \\textbf{Repeat:}  \\\\ \\quad \\Delta \\leftarrow 0 \\\\ \\quad \\textbf{For all } s \\in S:  \\\\ \\quad \\quad v \\leftarrow V(s) \\\\ \\quad \\quad V(s) \\leftarrow \\max_a \\sum_{s', r} p(s',r | s, a) [r + \\gamma V(s')] \\\\ \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|) \\\\ \\textbf{Until:}  \\Delta &lt; \\theta \\\\ \\textbf{Return:}  V^{*}(s), \\forall s \\in S \\end{array} \\)</p>"},{"location":"unit2/lesson5/lesson5.html#key-differences-between-policy-iteration-and-value-iteration","title":"Key Differences Between Policy Iteration and Value Iteration","text":"Feature Policy Iteration Value Iteration Policy Evaluation Full evaluation Single update Convergence Speed Slower, but fewer iterations Faster updates, but more iterations Computational Cost Higher per iteration Lower per iteration"},{"location":"unit2/lesson5/lesson5.html#value-iteration-on-a-windy-grid-world","title":"Value Iteration on a Windy Grid World","text":"<p>Below we show the results of applying the value iteration method on a windy grid world. This is almost identical to the previous simple grid world without any obstacles, the only difference is that there is a wind blowing upwards, which shifts the agent 2 or 1 cell depending on its location. </p> <pre><code>Q = value_iteration(env=windy(), show=True)\n</code></pre> <p></p> <p>Let us now apply the policy-iteration on the maze env.</p> <p><pre><code>policy = value_iteration(env=maze(), show=True)\n</code></pre> </p> <pre><code>loop stopped @ iteration: 14 , \u0394 =  0\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we covered the main dynamic programming algorithms. We saw how evaluating a policy was extremely useful as it is the key component in allowing us to improve the policy. We then developed a policy iteration algorithm that improves the policy in two main steps: 1. A step that evaluates the policy fully to reach an accurate estimation of the action values of the current policy. 2. A step that improves the policy by adopting a greedy action. The usage of an action-value function \\( Q(s,a) \\) was key in allowing us to choose between actions since the state-value function \\( V(s) \\) does not differentiate between the values of actions.</p> <p>We finally saw how the value iteration algorithm has a similar structure to the policy iteration algorithm with one important difference: it can arrive at an optimal policy by just taking a step towards the optimal policy by slightly refining its estimation of the action-value function without fully evaluating it. Hence, it improves its policy more concisely and with much less overhead than the full policy iteration method.</p> <p>In summary:</p> <ul> <li>Policy Evaluation computes \\( V^{\\pi} \\) for a fixed policy.</li> <li>Policy Improvement Theorem ensures that improving a policy results in a better policy.</li> <li>Policy Iteration alternates between evaluation and improvement until convergence.</li> <li>Value Iteration updates values directly, skipping full policy evaluation.</li> </ul> <p>These techniques form the foundation of solving MDPs using Dynamic Programming. In the next lesson, we will take a different approach and move on to cover sampling methods that do not use the dynamics of the environment explicitly and instead try to improve their policy by interacting with the environment.</p> <p>Further Reading: For further reading, you can refer to chapter 4 from the Sutton and Barto book.</p>"},{"location":"unit2/lesson5/lesson5.html#your-turn","title":"Your turn","text":"<p>Now it is time to experiment further and interact with the code in worksheet5.</p>"},{"location":"unit2/lesson6/lesson6.html","title":"Lesson 6 - Tabular Methods: Monte Carlo","text":"<p>Learning outcomes</p> <p>By the end of this lesson, you will be able to: </p> <ol> <li>Understand the difference between learning the expected return and computing it via dynamic programming.</li> <li>Understand the strengths and weaknesses of Monte Carlo (MC) methods.</li> <li>Appreciate that MC methods need to wait until the end of the task to obtain their estimate of the expected return.</li> <li>Compare MC methods with dynamic programming methods.</li> <li>Understand the implication of satisfying and not satisfying the explore-start requirement for MC control and how to mitigate it via the reward function.</li> <li>Understand how to move from prediction to control by extending the \\( V \\) function to a \\( Q \\) function and make use of the idea of generalized policy iteration (GPI).</li> <li>Understand how policy gradient methods work and appreciate how they differ from value function methods.</li> </ol>"},{"location":"unit2/lesson6/lesson6.html#overview","title":"Overview","text":"<p>In this lesson, we develop the ideas of Monte Carlo methods. Monte Carlo methods are powerful and widely used in settings other than reinforcement learning (RL). You may have encountered them in a previous module where they were mainly used for sampling. We will also use them here to sample observations and average their expected returns. Because they average the returns, Monte Carlo methods have to wait until all trajectories are available to estimate the return. Later, we will find out that Temporal Difference methods do not wait until the end of the episode to update their estimate and outperform MC methods.</p> <p>Note that we have now moved to learning instead of computing the value function and its associated policy. This is because we expect our agent to learn from interacting with the environment instead of using the dynamics of the environment, which is usually hard to compute except for a simple lab-confined environment.</p> <p>Remember that we are dealing with expected return, and we are either finding an exact solution for this expected return as when we solve the set of Bellman equations or finding an approximate solution for the expected return as in DP or MC. Also, remember that the expected return for a state is the future cumulative discounted rewards, given that the agent follows a specific policy.</p> <p>One pivotal observation that summarizes the justification for using MC methods over DP methods is that it is often the case that we are able to interact with the environment instead of obtaining its dynamics due to its complexity and intractability. In other words, interacting with the environment is often more direct and easier than obtaining the model of the environment. Therefore, we say that MC methods are model-free.</p>"},{"location":"unit2/lesson6/lesson6.html#plan","title":"Plan","text":"<p>As usual, in general, there are two types of RL problems that we will attempt to design methods to deal with:</p> <ol> <li> <p>Prediction problems    For these problems, we will design Policy Evaluation Methods that attempt to find the best estimate for the value function given a policy.</p> </li> <li> <p>Control problems    For these problems, we will design Value Iteration methods that utilize Generalized Policy Iteration. These methods attempt to find the best policy by estimating an action-value function for the current policy and then moving to a better and improved one by often choosing a greedy action. They minimize an error function to improve their value function estimate, which is used to deduce a policy.    We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximizing its value function.</p> </li> </ol> <p>We start by assuming that the policy is fixed. This will help us develop an algorithm that predicts the state space's value function (expected return). Then we will move to the policy improvement methods, i.e., these methods that help us compare and improve our policy with respect to other policies and move to a better policy when necessary. Finally, we move to the control case (policy iteration methods).</p>"},{"location":"unit2/lesson6/lesson6.html#bellman-equationsreminder","title":"Bellman Equations(reminder)","text":""},{"location":"unit2/lesson6/lesson6.html#lesson-6-tabular-methods-monte-carlo_1","title":"Lesson 6 - Tabular Methods: Monte Carlo","text":"<p>Learning outcomes</p> <p>By the end of this lesson, you will be able to: </p> <ol> <li>Understand the difference between learning the expected return and computing it via dynamic programming.</li> <li>Understand the strengths and weaknesses of Monte Carlo (MC) methods.</li> <li>Appreciate that MC methods need to wait until the end of the task to obtain their estimate of the expected return.</li> <li>Compare MC methods with dynamic programming methods.</li> <li>Understand the implication of satisfying and not satisfying the explore-start requirement for MC control and how to mitigate it via the reward function.</li> <li>Understand how to move from prediction to control by extending the \\( V \\) function to a \\( Q \\) function and make use of the idea of generalized policy iteration (GPI).</li> <li>Understand how policy gradient methods work and appreciate how they differ from value function methods.</li> </ol>"},{"location":"unit2/lesson6/lesson6.html#bellman-equations","title":"Bellman Equations","text":"<p>As we have seen in a previous lesson, the Bellman equations form the foundation of many RL methods. They define the relationship between the value of a state and the values of successor states.</p> <p>For a given policy \\( \\pi \\), the Bellman Equation for the state-value function is:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma V^\\pi(S_{t+1}) \\mid S_t = s \\right] \\] <p>For the action-value function \\( Q^\\pi(s, a) \\):</p> \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a \\right] \\]"},{"location":"unit2/lesson6/lesson6.html#dynamic-programming-is-model-based","title":"Dynamic Programming is Model-Based","text":"<p>As we have seen in the last lesson, Dynamic Programming uses Bellman equations as update rules to iteratively compute value functions and policies using two key steps:</p> <ol> <li>Policy Evaluation: Uses the Bellman equation to obtain an evaluation for a given policy.</li> <li>Policy Improvement: Uses the Bellman optimality equation to update the policy towards a better policy.</li> </ol> <p>However, DP has a major limitation: it requires complete knowledge of the dynamics model \\( p(s',r | s, a) \\), or transition model \\( p(s'| s, a) \\), which is often unavailable in real-world scenarios. In other words, it is a model-based method.</p>"},{"location":"unit2/lesson6/lesson6.html#monte-carlo-model-free-methods","title":"Monte Carlo: Model-Free Methods","text":"<p>Monte Carlo (MC) methods are a class of RL algorithms used to estimate value functions and optimize policies by using sampled episodes instead of full models of the environment. Unlike Dynamic Programming (DP), which requires a known dynamics model \\( p(s',r | s, a) \\), Monte Carlo methods learn directly from experience by averaging observed rewards.</p> <p>Monte Carlo (MC) methods estimate value functions without requiring a dynamics model. Instead, they rely on sampled episodes of experience. The key idea is to approximate value functions by averaging observed returns over multiple episodes.</p> <p>The fundamental principle behind MC methods is that averaging samples from a distribution approximates its expectation. Since state-value and action-value functions are defined as expected returns, averaging returns over a sufficiently large number of episodes provides a reliable estimate of these functions.</p> <p>There are some technical conditions that must be met for this approximation to hold, which we will reference when necessary.</p>"},{"location":"unit2/lesson6/lesson6.html#return-definition","title":"Return Definition","text":"<p>For an episode consisting of states, actions, and rewards:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\] <p>The Monte Carlo estimate of \\( V(s) \\) is the average return from all episodes where state \\( s \\) was visited.</p> \\[ V(s) \\approx \\frac{1}{N} \\sum_{i=1}^{N} G_i(s) \\] <p>Where:</p> <ul> <li>\\(i\\) refers to the index over the episodes, not the time steps.</li> <li>\\( G_i(s) \\) is the return (sum of discounted rewards) observed from episode \\(i\\), starting at state \\(s\\).</li> <li>\\(N\\) is the total number of episodes used to average the returns.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#first-visit-monte-carlo-policy-evaluation-prediction","title":"First-Visit Monte Carlo Policy Evaluation (Prediction)","text":"<p>Because MC methods depend entirely on experience, a natural way to approximate the cumulative future discounted reward is by taking their average once they become available through experience. So we must collect the cumulative future discounted reward once this experience has elapsed. In other words, we need to take the sum after the agent has finished an episode for all the rewards obtained from the current state to the end of the episode. Then we average those returns over all of the available episodes. Note that MC methods only apply for episodic tasks, which is one of its limitations, in addition to having to wait until the episode is finished.</p> <p>Note also that the agent can visit the same state more than once inside the same episode. One question that arises from the above averaging is: which visit should be counted? </p> <p>We can take the sum starting from the first visit, or every visit, each yielding a different algorithm. The first-visit algorithm is more suitable for tabular methods, while the every-visit algorithm is more suitable when using function approximation methods (such as neural networks).</p> <p>One could argue that since we want to estimate the value of a state based on the full horizon of rewards obtained until the end of an episode, we should include only the first visit of the state in the average. This is the basis of the First-visit Monte Carlo (MC) policy evaluation method. First-visit MC estimates the value of a state by averaging the returns from the first time that state is encountered in each episode. Below we show the pseudocode for this algorithm.</p> \\[ \\begin{array}{ll} \\textbf{Algorithm: }  \\text{First-Visit Monte Carlo Policy Evaluation} \\\\ \\textbf{Input: } \\text{Episodes generated under policy } \\pi \\\\ \\textbf{Initialize: }  V(S) \\leftarrow 0, N(S) \\leftarrow 0, \\forall S \\in \\mathcal{S} \\\\ \\textbf{For each episode: } &amp; \\\\ \\quad \\text{Generate an episode: } (S_0, A_0, R_1, S_1, \\dots, S_T) &amp; \\\\ \\quad G \\leftarrow 0 &amp; \\\\ \\quad \\textbf{For each step } t \\textbf{ from } T-1 \\textbf{ to } 0: &amp; \\\\ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} &amp; \\\\ \\quad \\quad \\text{If } S_t \\text{ appears first in the episode:} &amp; \\\\ \\quad \\quad \\quad N(S_t) \\leftarrow N(S_t) + 1 &amp; \\\\ \\quad \\quad \\quad V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G - V(S_t)) &amp; \\\\ \\textbf{Return: } V(S), \\forall S \\in \\mathcal{S} \\\\ \\end{array} \\]"},{"location":"unit2/lesson6/lesson6.html#random-walk-problem","title":"Random Walk Problem","text":"<p>The Random Walk problem is a simple Markov Reward Process (MRP) used to illustrate value estimation methods. It consists of a finite, linear chain of states, where an agent moves randomly left or right until reaching one of the two terminal states.</p>"},{"location":"unit2/lesson6/lesson6.html#problem-setup","title":"Problem Setup","text":"<ul> <li>Typically, there are five non-terminal states labeled \\( A, B, C, D, E \\) on a 1-d grid world. Another variation uses 21 states.</li> <li>Two terminal states exist at both ends.</li> <li>The agent starts in the center and moves randomly left or right with equal probability.</li> <li>The episode ends when the agent reaches a terminal state.</li> <li>A reward of +1 is received upon reaching the right terminal state, while the left terminal state gives a 0 reward.</li> </ul> <p>This random walk problem is often used to demonstrate Monte Carlo and Temporal-Difference (TD) learning methods for estimating state-value functions. Below we show this problem.</p> <p> </p> <pre><code>        A     B     C     D     E\n</code></pre> <p>Let\u2019s now run the MC1st algorithm with a useful visualization. The plot will display the true state values of the random walk as black points connected by a black line. The agent\u2019s estimated values are represented by blue points and a blue line, allowing us to visually assess how closely the learned values for states A\u2013D match their actual values. This visualization is achieved by setting <code>plotV=True</code>.</p> <p>Next to this, an error plot tracks the total error per episode, providing insight into the learning process. Each episode starts from the middle state (C) and ends upon reaching either the far-left or far-right terminal states. These terminal states are not labeled since they do not have values to estimate. This tracking of errors is achieved by setting <code>plotE=True</code>.</p> <p><pre><code>mc = MC1st(env=randwalk(), episodes=100, plotV=True, plotE=True, seed=1).interact()\n</code></pre> </p> <p>We use **demoV to implicitly pass plotE=True, plotV=True, animate=True, whenever we want to demo a prediction algorithm.</p>"},{"location":"unit2/lesson6/lesson6.html#policies","title":"Policies","text":"<p>Before we move into control, we need to briefly discuss types of policies that balance exploration and exploitation.</p>"},{"location":"unit2/lesson6/lesson6.html#epsilon-greedy-policy","title":"\\(\\epsilon\\)-Greedy Policy","text":"<p>The \\(\\epsilon\\)-greedy policy is a popular action selection strategy that balances exploration and exploitation. The policy chooses the action with the highest estimated value most of the time, but with probability \\(\\epsilon\\), it selects an action randomly to encourage exploration.</p> <p>Mathematically, the \\(\\epsilon\\)-greedy policy can be defined as:</p> \\[ \\pi(a|s) =  \\begin{cases}  \\frac{\\epsilon}{|A|}, &amp; \\text{with probability } \\epsilon \\\\ 1 - \\epsilon + \\frac{\\epsilon}{|A|}, &amp; \\text{for the action with the highest value} \\\\ 0, &amp; \\text{for all other actions} \\end{cases} \\] <p>Where: - \\(\\epsilon\\) is the probability of exploring (random action). - \\(|A|\\) is the total number of actions available. - The action with the highest value \\(Q(s,a)\\) is selected with probability \\(1-\\epsilon + \\frac{\\epsilon}{|A|}\\).</p>"},{"location":"unit2/lesson6/lesson6.html#softmax-policy","title":"Softmax Policy","text":"<p>The softmax policy selects actions based on a probability distribution that is a function of the action values. The policy assigns a higher probability to actions with higher expected returns, and the probability of selecting action \\(a\\) in state \\(s\\) is proportional to the exponential of its action-value \\(Q(s, a)\\).</p> <p>Mathematically, the softmax policy is given by:</p> \\[ \\pi(a|s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{b \\in A} e^{Q(s,b)/\\tau}} \\] <p>Where: - \\(\\tau\\) is the temperature parameter that controls the level of exploration. A high \\(\\tau\\) encourages more exploration (more uniform distribution), and a low \\(\\tau\\) leads to more exploitation (choosing the highest value action). - \\(Q(s,a)\\) is the action-value function.</p> <p>In this way, the softmax policy ensures that actions with higher values are more likely to be chosen, but there is always a chance to explore other actions.</p>"},{"location":"unit2/lesson6/lesson6.html#first-visit-mc-control","title":"First-visit MC control","text":"<p>Now let us extend our first-visit MC prediction to control by updating the Q action-value function instead of the state-value function V.</p> <p>\\( \\begin{array}{ll} \\textbf{Algorithm: }  \\text{First-Visit Monte Carlo Control (Exploring Starts)} \\\\ \\textbf{Input: } \\text{Episodes generated under an exploring-starts policy} \\\\ \\textbf{Initialize: }  Q(S, A) \\leftarrow 0, N(S, A) \\leftarrow 0, \\forall S \\in \\mathcal{S}, A \\in \\mathcal{A}(S), \\pi(S) \\leftarrow \\text{arbitrary policy}, \\forall S \\in \\mathcal{S} \\\\ \\textbf{For each episode: } &amp; \\\\ \\quad \\text{Generate an episode: } (S_0, A_0, R_1, S_1, A_1, \\dots, S_T) &amp; \\\\ \\quad G \\leftarrow 0 &amp; \\\\ \\quad \\textbf{For each step } t \\textbf{ from } T-1 \\textbf{ to } 0: &amp; \\\\ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} &amp; \\\\ \\quad \\quad \\text{If } (S_t, A_t) \\text{ appears first in the episode:} &amp; \\\\ \\quad \\quad \\quad N(S_t, A_t) \\leftarrow N(S_t, A_t) + 1 &amp; \\\\ \\quad \\quad \\quad Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)}(G - Q(S_t, A_t)) &amp; \\\\ \\quad \\quad \\text{Update policy: } \\pi(S_t) \\leftarrow \\arg\\max_A Q(S_t, A) &amp; \\\\ \\textbf{Return: } Q(S, A), \\pi(S), \\forall S \\in \\mathcal{S}, A \\in \\mathcal{A}(S) \\\\ \\end{array} \\)</p>"},{"location":"unit2/lesson6/lesson6.html#applying-mc-on-a-control-problem","title":"Applying MC on a control problem","text":"<p>Similar to what we did for prediction, we get help from a dictionary that stores a set of useful configurations that we use often. In the case of control, the most useful is plotting the number of steps the agent took to reach a terminal state in each episode or the sum of rewards the agent collected in each episode. Each one of these plots can be useful for certain tasks. Bear in mind that if the reward is given only for reaching the goal location or terminal state, the sum of the rewards plot would be a constant line that does not convey useful information. Below we show each.</p> <p>Unfortunately, applying the MC control algorithm with the default reward function will not yield a useful policy. This is because the explore-start condition is not satisfied (refer to section 5.4 of our book). In addition, averaging solutions may not perform well because they do not track a changing policy well for non-stationary problems (most of the control problems are non-stationary). To see this, uncomment the lines in the cell below and run it. (Note that we have set up the priorities of the actions in a way that will show this issue (right comes before left and down before up). demoQ is a dictionary that passes visualization values to the MDP control algorithm.</p> <p><pre><code>mc = MC1stControl(env=grid(), \u03b3=1, episodes=200,  seed=10, **demoQ()).interact()\n</code></pre> </p>"},{"location":"unit2/lesson6/lesson6.html#exploration-exploitation-and-exploring-starts","title":"Exploration-Exploitation and Exploring Starts","text":"<ul> <li>Exploration-Exploitation Tradeoff: In reinforcement learning, the agent must balance exploration (trying new actions to discover better long-term rewards) with exploitation (choosing known actions that yield high rewards based on current knowledge).</li> </ul> <p>In exploration, the agent tries actions that it has not explored enough yet. This is necessary to gather more information about the environment. In exploitation, the agent uses its current knowledge to select actions that maximize the immediate reward based on its existing value estimates. Both of these need to happen for the agent to learn effectively. \\(\\epsilon\\)-Greedy and softmax policies have built-in exploration-exploitation capabilities.</p> <ul> <li>Exploring Starts: This is a method used to ensure complete exploration of the state-action space. In exploring starts, each episode begins with a random state and a random action. This guarantees that over many episodes, every state-action pair will eventually be visited, ensuring unbiased value estimation. This condition dictates that all states must be randomly started with to guarantee convergence. It is needed for First-visit MC Control (1stMCC) since it relies on getting enough coverage for all of the states.</li> </ul> <p>Exploring starts guarantees initial exploration and helps ensure all state-action pairs are visited, making it easier for the agent to discover good policies without biases. Exploration-exploitation policies (like \\(\\epsilon\\)-greedy or softmax) are methods to balance exploration and exploitation during the agent's learning process. These policies allow for exploration to continue throughout the agent's learning, which is particularly useful when the environment is unknown. Exploration-exploitation balance plays an important role when we move into online methods. But they are still important to achieve the end of the episode, and \\(\\epsilon\\)-greedy is useful to avoid the exploring-start condition.</p> <p>In short, exploring starts ensures full exploration at the beginning of episodes, while exploration-exploitation policies control the balance between exploration and exploitation throughout the learning process.</p>"},{"location":"unit2/lesson6/lesson6.html#the-role-of-the-discount-factor-gamma-for-delayed-reward","title":"The role of the discount factor \\(\\gamma\\) for delayed reward","text":"<p>Important Note It is always the case that when we use a delayed reward (which is the default reward for our Grid class), the discount factor \\(\\gamma\\) must not be set to 1. This is because the sum of the discounted rewards of each visited state will be equal to the delayed reward itself, which will not give any particular advantage to follow a shorter path, yielding a useless policy. Therefore, we can solve this issue: 1. either by providing a discounted value for \\(\\gamma\\) that is less than 1. 2. or by changing the reward to have intermediate steps reward, which, when accumulated, will provide distinguished sums for the different paths and hence help distinguish the shortest path or the policy that will yield an optimal reward.</p>"},{"location":"unit2/lesson6/lesson6.html#solution-1","title":"Solution 1","text":"<p>Below we show how we can simply reduce \\(\\gamma\\) to solve this issue.</p> <p><pre><code>mc = MC1stControl(env=grid(), \u03b3=.99, episodes=30, seed=10, **demoTR()).interact()\n</code></pre> </p>"},{"location":"unit2/lesson6/lesson6.html#solution-2","title":"Solution 2","text":"<p>Also, we can compensate for the above issue by setting up a reward function that allows the agent to quickly realise when it is stuck in a non-useful policy. One way to do this is by designing a reward structure that gives immediate feedback for the agent's actions, guiding it towards more efficient policies. This can be done by:</p> <ol> <li> <p>Providing Intermediate Rewards: Instead of only rewarding the agent when it reaches the terminal state, we can give small rewards for progressing towards the goal. This helps the agent understand the immediate consequences of its actions and prevents it from being stuck in a loop without rewards.</p> </li> <li> <p>Penalising Suboptimal Actions: Introducing penalties for actions that lead to non-productive states can guide the agent away from unhelpful policies. For example, moving further away from the goal could result in a small negative reward, helping the agent learn to avoid those actions.</p> </li> <li> <p>Shaping the Reward Function: A reward shaping approach can give the agent additional clues about how good or bad its actions are in a more fine-grained manner. This can be especially useful in environments where the final reward is too delayed or sparse to provide clear guidance.</p> </li> <li> <p>Negative Rewards for Unproductive States: If an agent repeatedly takes actions that lead to dead ends or unhelpful paths, it can be penalised with negative rewards, encouraging it to explore more efficient alternatives.</p> </li> </ol> <p>With these adjustments, the agent can more effectively learn from its interactions with the environment and avoid getting stuck in poor policies. By incorporating some or all of these strategies into the reward function, we give the agent more guidance and the ability to identify and correct non-optimal behaviours.</p> <p>Below we show approach 1 (intermediate rewards) since we are dealing with a simple environment.</p> <p><pre><code>env1 = grid(reward='reward_1')\nmcc = MC1stControl(env=env1, episodes=30, seed=0, **demoTR()).interact()\n</code></pre> </p> <p>Compare the above policy with the one produced by the DP solution in Lesson 2. You will notice that the MC solution does not provide a comprehensive solution from all states because the starting position is fixed. In contrast, Dynamic Programming (DP) assumes access to a complete model of the environment and typically considers all possible state-action pairs during its computation. This allows DP to evaluate and improve policies more systematically across the entire state space.</p> <p>The exploration nature of the MC policy, however, allows the agent to develop an understanding of the environment through its Q function, which helps the agent figure out the best action to take when it finds itself in a particular state. The Markovian property ensures that the agent\u2019s decisions are based on the current state, and the agent can use this information to guide its learning process effectively, even if the start state is fixed. In this way, the agent gradually builds a policy by sampling trajectories and averaging the returns over time.</p> <p>One thing you might have noticed is that, despite the simplicity of the task, the agent occasionally took detours from the most straightforward path to the goal. This happens because we are using an \u03b5-greedy policy by default, which means that there is a 10% chance of the agent taking random exploratory actions at each step. This randomness can cause the agent to deviate from the optimal path, even though the policy is generally headed in the right direction. </p> <p>However, this should not prevent the maxQ policy (the one that chooses the action with the highest action-value) from guiding the agent towards the goal. The detours happen because of the inherent nature of Monte Carlo methods, which rely on sampling and averaging across multiple episodes. The stochastic nature of MC methods means that, although the agent\u2019s policy points towards the goal on average, individual episodes might show some variation due to exploration.</p> <p>In the next section, we will explore how we can mitigate these detours and overcome the inherent randomness of the \u03b5-greedy policy. One possible approach is to fine-tune the exploration rate, but this requires a fair amount of trial and error and is not a straightforward process. While exploring the balance between exploration and exploitation is a key part of reinforcement learning, achieving a fully optimal policy will involve careful consideration of how exploration is managed and possibly modifying the reward function or policy to limit unnecessary exploration.</p> <p>We can play with the exploration but that needs lots of trail and is not straightforward.</p> <pre><code>mc = MC1stControl(env=grid(), \u03b3=.97, episodes=50, \u03b5=.5, d\u03b5=.99, seed=20, **demoTR()).interact()\n</code></pre> <p></p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), \u03b3=.97, episodes=100, \u03b5=0.9, d\u03b5=.999, seed=20, **demoTR()).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#every-visit-mc-prediction","title":"Every-visit MC Prediction","text":"<p>In every-visit Monte Carlo, the value of a state is updated based on the average of returns observed from all visits to that state, not just the first visit. For a state \\( s \\), let \\( G_j(s) \\) be the return from the \\( j \\)-th occurrence of \\( s \\) in some episode. The every-visit MC estimate of \\( V^\\pi(s) \\) is given by: </p> \\[ V^\\pi(s) \\approx \\frac{1}{N_s} \\sum_{j=1}^{N_s} G_j(s) \\] <p>where: - \\( N_s \\) is the total number of times state \\( s \\) was visited across all episodes. - \\( G_j(s) \\) is the return obtained from the \\( j \\)-th visit to \\( s \\), computed as the sum of rewards from that visit to the end of the episode.  </p>"},{"location":"unit2/lesson6/lesson6.html#first-visit-vs-every-visit-mc","title":"First-Visit vs. Every-Visit MC","text":"First-Visit MC Every-Visit MC Update Rule Uses the return from the first visit to a state in each episode Uses returns from all visits to a state in each episode Bias/Variance Lower bias but higher variance Higher bias but lower variance Suitability Works well when visits to a state within an episode are correlated Suitable when visits are independent and representative of overall state behavior"},{"location":"unit2/lesson6/lesson6.html#constant-mc-incremental-every-visit-mc-prediction","title":"Constant-\u03b1 MC: Incremental Every-Visit MC Prediction","text":"<p>We now extend the every-visit Monte Carlo (MC) prediction algorithm by making it incremental. Incrementality is crucial in reinforcement learning, as it allows an algorithm to interact with the environment and learn continuously, rather than waiting until the end of the experience to make updates. However, since MC methods require computing the return \\( G_t \\), we must still wait until the end of each episode to obtain the full sequence of rewards. The key difference is that instead of averaging all returns at once, we update the estimates incrementally.</p>"},{"location":"unit2/lesson6/lesson6.html#incremental-update-rule","title":"Incremental Update Rule","text":"<p>To achieve this, we adjust how averaging is performed over time. Suppose we observe state \\( s \\) at time step \\( t \\) with return \\( G_t \\), and the number of times \\( s \\) has been visited is \\( N_s \\). We aim to update the value estimate \\( V^\\pi(s) \\) based on our previous estimate \\( V^\\pi_k(s) \\). Since \\( N_s - 1 \\) represents the number of times \\( s \\) was visited before, the updated estimate is given by:</p> \\[     \\begin{align*}         V^\\pi_{k+1}(s) &amp;= \\frac{1}{N_s} \\left(\\sum_{j=1}^{N_s-1} G_j(s) + G_t(s)\\right) \\\\                         &amp;= \\frac{1}{N_s} \\left((N_s - 1)V^\\pi_k(s) + G_t(s)\\right) \\\\                         &amp;= V^\\pi_k(s) + \\frac{1}{N_s} \\left(G_t -  V^\\pi_k(s) \\right)     \\end{align*} \\] <p>This approach mirrors the concept used in the simple bandit algorithm.</p>"},{"location":"unit2/lesson6/lesson6.html#addressing-the-diminishing-update-problem","title":"Addressing the Diminishing Update Problem","text":"<p>One issue with standard averaging is that as \\( N_s \\) increases, the influence of newly observed returns \\( G_t \\) diminishes because \\( \\frac{1}{N_s} \\) becomes very small. To counteract this, we replace \\( \\frac{1}{N_s} \\) with a fixed constant \\( \\alpha \\). This ensures that the agent continues to adjust its estimates meaningfully over time, which is particularly beneficial when the policy is evolving (i.e., in non-stationary environments).</p> <p>The incremental update rule then becomes:</p> \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\left( G_t - V(S_t) \\right) \\] <p>where:</p> <ul> <li>\\( V(s) \\) is the estimated value of state \\( s \\).</li> <li>\\( G_t \\) is the return (sum of discounted rewards) obtained from state \\( s \\) onwards.</li> <li>\\( \\alpha \\) is the constant step-size parameter (\\( 0 &lt; \\alpha \\leq 1 \\)).</li> </ul> <p>This approach allows updates to be made incrementally and ensures that the agent continues refining its estimates as more experience is gathered. It is particularly effective in scenarios where multiple visits to the same state provide valuable learning opportunities.</p>"},{"location":"unit2/lesson6/lesson6.html#algorithm-incremental-constant-monte-carlo-prediction","title":"Algorithm: Incremental Constant-\u03b1 Monte Carlo Prediction","text":"<p>Below is the full pseudocode for the algorithm:</p> \\[ \\begin{array}{ll} \\textbf{Algorithm: }  \\text{Incremental Constant-}\\alpha \\text{ Monte Carlo Prediction} \\\\ \\textbf{Input: } \\text{Episodes generated under policy } \\pi \\\\ \\textbf{Initialize: }  V(s) \\leftarrow 0, \\forall s \\in \\mathcal{S}, 0 &lt; \\alpha \\leq 1 \\\\ \\textbf{For each episode: } &amp; \\\\ \\quad \\text{Generate an episode: } (S_0, R_1, S_1, \\dots, S_T) &amp; \\\\ \\quad G \\leftarrow 0 &amp; \\\\ \\quad \\textbf{For each step } t \\textbf{ from } T-1 \\textbf{ to } 0: &amp; \\\\ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} &amp; \\\\ \\quad \\quad \\text{Update state-value estimate:} &amp; \\\\ \\quad \\quad \\quad V(S_t) \\leftarrow V(S_t) + \\alpha (G - V(S_t)) &amp; \\\\ \\textbf{Return: } V(s), \\forall s \\in \\mathcal{S} \\\\ \\end{array} \\] <p>On important advantage for this methods is that it ensures that the agent continuously learns from its experiences while adapting to changes in the environment (assuming we are updating the estimates between episodes).</p>"},{"location":"unit2/lesson6/lesson6.html#mrp-mdp-and-pg-classes","title":"MRP, MDP, and PG Classes","text":"<p>To evaluate the effectiveness of prediction methods, we apply them to a random walk problem. This setup isolates the prediction aspect of an algorithm, focusing solely on estimating the state-value function without involving decision-making. By doing so, we can assess whether a given update rule or algorithm is effective in the prediction setting.</p> <p>Once we grasp the prediction process, we can extend these methods to control by modifying the update rule to incorporate action-value estimates (\\( Q \\)-values). This transition is typically done within the Markov Decision Process (MDP) framework and is a crucial step in value-based reinforcement learning methods.</p> <p>Beyond value-based approaches, another class of methods exists, known as policy-based or policy gradient (PG) methods. These methods optimise the policy directly instead of relying on a value function. Unlike value-based methods, policy gradient approaches are typically applied to control problems, such as grid-world mazes, rather than random walk problems.</p> <p>We define three parent classes to structure our reinforcement learning algorithms:</p> <ul> <li>MRP (Markov Reward Process) Class: Used for any model-free prediction method that has access to \\( V \\). The MRP class does not include a policy, as it operates under an arbitrary policy.</li> <li>MDP (Markov Decision Process) Class: Used for any model-free control method, which has access to both \\( Q \\) (action-value function) and an \\( \\epsilon \\)-greedy policy. This allows the agent to make decisions based on value estimates.</li> <li>PG (Policy Gradient) Class: Used for any policy-gradient method. The PG class has access to both \\( V \\) and \\( Q \\), as well as a softmax policy for selecting actions.</li> </ul> <p>By structuring our approach in this way, we create a clear distinction between prediction and control methods, ensuring a systematic transition from estimating values to learning optimal policies.</p>"},{"location":"unit2/lesson6/lesson6.html#incremental-constant-alpha-mc-prediction-with-python","title":"Incremental Constant-\\(\\alpha\\) MC (Prediction) with Python","text":"<p>Below, we present a direct interpretation of the previously discussed pseudocode for the model-free prediction method into Python. This implementation follows an incremental approach, applying updates using a constant step-size parameter \\( \\alpha \\). </p> <p>To maintain a structured and modular design, we define this implementation as a subclass of the MRP class. This ensures that the algorithm focuses purely on state-value estimation under a fixed policy, without incorporating decision-making or action-value functions.</p> <p>By leveraging the MRP parent class, we maintain consistency across different prediction methods and enable seamless extension to more complex reinforcement learning algorithms.</p> <pre><code>class MC(MRP):\n    def init(self):\n        self.store = True # stores the trajectory of the episode, always needed for offline\n    # ------------------ \ud83c\udf18 offline, MC learning: end-of-episode learning ------------------\n    def offline(self): # called at the end of the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):          # go throgh experience backwards as per Gt in update\n            s, rn = self.s[t], self.r[t+1]       # retrieve the state and reward for past step t \n            Gt = self.\u03b3*Gt + rn                  # calculate the return for past time step t\n            self.V[s] += self.\u03b1*(Gt - self.V[s]) # update the state-value function\n</code></pre> <p>This type of algorithmic design is more flexible and is generally preferred in reinforcement learning. Instead of requiring the storage of cumulative sums or averages, the incremental approach allows for real-time updates, making it more adaptable to dynamic environments.</p> <p>By using this approach, we can efficiently estimate the state-value function without the computational overhead of storing past returns. This is particularly useful in large-scale problems where maintaining full episode histories would be impractical.</p> <p>Now, let\u2019s apply our newly implemented prediction algorithm to the random walk problem and observe how well it estimates the state values.</p> <p><pre><code>mc = MC( \u03b1=.02, episodes=50, **demoV()).interact()\n</code></pre> </p> <p>As can be seen, the algorithm behaves as expected, gradually converging to an optimal solution\u2014albeit slowly. This is a general characteristic of Monte Carlo (MC) algorithms, as they rely on sampling and require numerous episodes to achieve accurate estimates. </p>"},{"location":"unit2/lesson6/lesson6.html#mc-vs-dp-computational-and-convergence-speed","title":"MC vs DP: Computational and Convergence Speed","text":"<p>Despite their slower convergence compared to dynamic programming methods and other methods that we cover later (such as TD), MC algorithms are advantageous in environments where the model is unknown, making them well-suited for model-free reinforcement learning. Monte Carlo (MC) methods are generally slower than Dynamic Programming (DP) in terms of convergence (not). When we take computaitonal complexity into consideration, the comaprison is more nuanced.</p>"},{"location":"unit2/lesson6/lesson6.html#convergence-speed-in-monte-carlo-mc","title":"Convergence Speed in Monte Carlo (MC)","text":"<ul> <li>MC methods are slower to converge compared to DP due to their reliance on sampling and waiting until the end of each episode to compute returns.  </li> <li>MC algorithms update state-value estimates incrementally after each episode. The updates rely on the sampled trajectories, and each state\u2019s estimate is refined slowly across multiple episodes. This means that for large state spaces or long episodes, it can take a lot of episodes for MC to converge to an accurate value.</li> <li>The rate of convergence in MC is also influenced by the exploration strategy (such as \\(\\epsilon\\)-greedy). If exploration is high, the agent may not follow the optimal path immediately, requiring more episodes to refine its policy and reach convergence. This is especially important in problems where exploring new actions is key to learning optimal values.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#convergence-speed-in-dynamic-programming-dp","title":"Convergence Speed in Dynamic Programming (DP)","text":"<ul> <li>DP methods tend to converge faster per iteration than MC. This is because DP directly uses a full model of the environment (transition probabilities, rewards, etc.) and updates the value function across the entire state space in a single pass.  </li> <li>Since DP updates all states at once, each iteration provides more substantial progress towards convergence. As long as the state space is not prohibitively large, DP can refine the value function quickly in a relatively small number of iterations.</li> <li>Despite the faster convergence per iteration, DP requires multiple iterations to converge to the optimal value function. While each individual update is quick, the overall process of achieving convergence may still take several iterations depending on the size and complexity of the environment.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#computational-complexity-of-mc","title":"Computational Complexity of MC","text":"<ul> <li>MC is computationally more efficient per episode because each update only requires sampling from the environment during the episode. This means that during each episode, MC methods only need to process the steps taken and compute returns after the episode ends. </li> <li>The time complexity of each episode depends on the number of states and actions, but the updates themselves are relatively simple. Thus, per episode, the computational cost can be considered \\(O(T)\\), where \\(T\\) is the number of time steps in the episode.</li> <li>Since MC needs multiple episodes to converge, the total computational cost is proportional to the number of episodes and the length of each episode. If the environment is large or the episodes are long, MC\u2019s total complexity will be \\(O(N \\cdot T)\\), where \\(N\\) is the number of episodes, and \\(T\\) is the number of steps in each episode.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#computational-complexity-of-dp","title":"Computational Complexity of DP","text":"<ul> <li>DP is computationally more expensive per iteration because it requires processing the entire state space to update all states in each pass. Each update involves iterating over all states and performing calculations based on the known model of the environment.</li> <li>The time complexity of one iteration of DP depends on the number of states \\(S\\) in the environment. The update rule for each state requires examining all possible actions and computing the expected returns based on the transition model and reward function, which results in a per-iteration complexity of \\(O(S \\cdot A)\\), where \\(A\\) is the number of actions.</li> <li>Since DP requires multiple iterations, the total complexity depends on the number of iterations \\(K\\) required for convergence. If the environment has a large state space, the computational cost can become significant, resulting in \\(O(K \\cdot S \\cdot A)\\), where \\(K\\) is the number of iterations needed to converge.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#summary-on-convergence-speed-and-computational-complexity","title":"Summary on Convergence Speed and Computational Complexity","text":"<ul> <li> <p>MC is slower to converge because it is based on sampling and episodic updates. It requires many episodes to converge to an optimal solution, and its computational complexity scales with the number of episodes and steps within each episode. While MC can be computationally cheaper per episode, it may require many episodes to converge, resulting in high total computational cost.</p> </li> <li> <p>DP converges faster per iteration, but it requires processing the entire state space during each iteration, making it computationally more expensive per update. However, DP typically requires fewer iterations to converge, which can make it faster overall in terms of convergence speed when the environment model is known. The computational cost can still become prohibitive for large state spaces due to the higher cost per iteration.</p> </li> </ul> <p>In summary, MC is computationally lighter per update but slower in convergence due to its reliance on sampling and multiple episodes. DP converges faster per iteration but is more computationally expensive per update, especially as the state-action space grows. The trade-off between convergence speed and computational efficiency depends on the environment and whether the environment model is available.</p>"},{"location":"unit2/lesson6/lesson6.html#incremental-mcc-every-visit-mc-control","title":"Incremental MCC: Every-visit MC Control","text":"<p>Incremental MC Control (Every-visit Monte Carlo Control) is an approach used for estimating optimal policies through interaction with the environment. It incrementally updates both the state-action value function \\( Q \\) and the policy using every visit to a state-action pair. The method involves updating the action-value function based on the observed returns, and it uses a constant step-size parameter \\( \\alpha \\) to control the magnitude of updates:</p> \\[ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( G_t - Q(S_t, A_t) \\right) \\] <p>Where:</p> <ul> <li>\\( Q(S, A) \\) is the action-value function,</li> <li>\\( G_t \\) is the return from the state-action pair \\( (S, A) \\),</li> <li>\\( \\alpha \\) is the constant step-size parameter.</li> </ul> <p>This method is applied iteratively to improve the policy, choosing actions greedily with respect to the estimated \\( Q \\)-values. Below we show the pseudocode for this algorithm.</p> \\[ \\begin{array}{ll} \\textbf{Algorithm: }  &amp; \\text{Incremental Constant-}\\alpha \\text{ Monte Carlo Control} \\\\ \\textbf{Input: } &amp; \\text{Episodes generated under an } \\varepsilon\\text{-greedy policy } \\pi \\\\ \\textbf{Initialize: } &amp; Q(S, A) \\leftarrow 0, \\forall S \\in \\mathcal{S}, A \\in \\mathcal{A}(S), 0 &lt; \\alpha \\leq 1 \\\\ \\textbf{For each episode: } &amp; \\\\ \\quad \\text{Generate an episode: } &amp; (S_0, A_0, R_1, S_1, A_1, \\dots, S_T) \\\\ \\quad G \\leftarrow 0 &amp; \\\\ \\quad \\textbf{For each step } t \\textbf{ from } T-1 \\textbf{ to } 0: &amp; \\\\ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} &amp; \\\\ \\quad \\quad \\text{Update action-value estimate:} &amp; \\\\ \\quad \\quad \\quad Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (G - Q(S_t, A_t)) &amp; \\\\ \\quad \\quad \\text{Update policy: } &amp; \\pi(S_t) \\leftarrow \\arg\\max_A Q(S_t, A) \\text{ (with } \\varepsilon\\text{-greedy exploration)} \\\\ \\textbf{Return: } &amp; Q(S, A), \\pi(S), \\forall S \\in \\mathcal{S}, A \\in \\mathcal{A}(S) \\\\ \\end{array} \\]"},{"location":"unit2/lesson6/lesson6.html#incremental-constant-mc-with-python","title":"Incremental constant-\u03b1 MC with Python","text":"<p>Below we show a direct interpretation of the above pseudocode into Python code. We use the MDP parent class for this control model-free method.</p> <pre><code>class MCC(MDP()):\n    def init(self):\n        self.store = True\n    # ------------------ \ud83c\udf18 offline, MC learning: end-of-episode learning \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udfeb ---------------\n    def offline(self):  # called at the end of the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s, a, rn = self.s[t], self.a[t], self.r[t+1] # retrieve the state, action and reward for past step t \n            Gt = self.\u03b3*Gt + rn                          # update Gt incrementally\n            self.Q[s,a] += self.\u03b1*(Gt - self.Q[s,a])     # update the action-value function\n</code></pre> <pre><code>mcc = MCC(env=grid(reward='reward1'), \u03b1=.2, episodes=1, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see, although we solved the issue of tracking a non-stationary policy by using a constant learning rate \\( \\alpha \\), and we attempted to use a reward function that provides immediate feedback at each step instead of a delayed reward, the performance is still not as good as expected. This is due to our final issue: the action precedence that we set up, which prefers left over right. If we change this precedence, it will help the agent find the goal more quickly. However, we set it up this way to make the problem more challenging. Consider changing this precedence to observe the effect.</p>"},{"location":"unit2/lesson6/lesson6.html#reinforce-mc-for-policy-gradient","title":"REINFORCE: MC for Policy Gradient","text":"<p>So far, we have only seen how to estimate a value function to deduce a policy from this value function and then improve the policy by preferring a greedy action with a bit of exploration (as in the \u03b5-greedy policy). When we allow the agent to act according to this new policy, its value function might change, so we must re-estimate the value function. We go into iterations of this process until the policy and value function are both stable (converge). We also saw that we could integrate both operations seamlessly into one iteration, as in the value-iteration algorithm in Dynamic Programming. We can even do both stages in one step as in Q-learning or Sarsa, as we shall see in the next lesson. The policy improvement theorem and the Generalised Policy Iteration process guarantee all of this. </p> <p>The primary approach we took to achieve learning for an action-value method is to minimise an error function between our estimate of a value function and the actual value function. Since the real value function is unavailable, we replaced it with some samples (unbiased as in MC and biased as in TD that we will see later).</p> <p>Policy gradient algorithms, on the other hand, attempt to maximise an objective function instead of minimising an error function. Can you think of a function that, if we maximise, will help us solve the RL problem? Pause for a moment and think.</p> <p>As you might have guessed, the value function can be used as an objective function. The objective here is to change the policy to maximise the value function. Directly estimating the policy means we are not using a value function to express the policy, as in the \u03b5-greedy method. Instead, we are using the value function to learn the policy directly. So, our algorithm does not need to learn the value function explicitly; it can learn a set of parameters that will maximise the value function without knowing what the value function is. It will come as a consequence of learning a policy. In the same way that we did not need to learn a policy in the value-function approach, we learned a value function, and as a consequence of minimising the error, we can deduce a policy from the learned value function. This is the fundamental difference between value-function approaches and policy-gradient approaches.</p> <p>Estimating the policy directly means we do not need to restrict the policy parameters to value-function estimates and their ranges. The policy parameters that represent the preferences to select an action are free to take on any range of values, as long as they comparatively form a cohesive policy that maximises the value function by dictating which action to choose in a specific state. This is a major advantage because the value function is strictly tied to the sum of reward values, while a policy need not have this coupling. This will give us more freedom in using classification architectures when we use function approximation, which excels in deducing the best action for a state, instead of using a regression architecture to regress a value function, which is usually more prone to initial condition issues and is harder to train.</p> <p>The best policy representation in a policy-gradient method is the action-selection softmax policy we came across in our last few lessons. This is a smooth function that, unlike \u03b5-greedy, allows the changes in the probabilities to be continuous and integrates very well with policy-gradient methods. One of the significant advantages of policy-gradient methods (the policy is differentiable everywhere, unlike stepwise \u03b5-greedy functions) is that it provides better guarantees of convergence than \u03b5-greedy due to this smoothness (\u03b5-greedy can change abruptly due to small changes in the action-value functions, while softmax just smoothly increases or decreases the probability of selecting an action when its action-value function changes).</p> <p>We start our coverage of policy-gradient methods with an offline method: REINFORCE. REINFORCE is an algorithm that takes a policy gradient approach instead of an action-value function approach. The idea is simple: given that an episode provides a sample of returns for the visited states, at the end of an episode, we will take the values of the states and use them to guide our search to find the optimal policy that maximises the value function.</p> <p>The softmax is the default policy selection procedure for Policy Gradient methods. \\( \\tau \\) acts like an exploration factor (more on that later), and we need one-hot encoding for the actions.</p> <p>Now we are ready to define our REINFORCE algorithm. This algorithm and other policy-gradient algorithms always have two updates: one for \\( V \\) and one for \\( Q \\). In other words, the action-value function update will be guided by the state-value update. We usually call the first update that deals with \\( V \\), the critic, and the second update that deals with \\( Q \\), the actor. Below we show the Python code for this algorithm.</p> <pre><code>class REINFORCE(PG()):\n    def init(self):\n        self.store = True\n    # -------------------- \ud83c\udf18 offline, REINFORCE: MC for policy gradient methdos ----------------------\n    def offline(self):\n        \u03c0, \u03b3, \u03b1, \u03c4 = self.\u03c0, self.\u03b3, self.\u03b1, self.\u03c4\n        # obtain the return for the latest episode\n        Gt = 0\n        \u03b3t = \u03b3**self.t                           # efficient way to calculate powers of \u03b3 backwards\n        for t in range(self.t, -1, -1):          # backwards\n            s, a, rn = self.s[t], self.a[t], self.r[t+1]\n            Gt = \u03b3*Gt + rn                        # update Gt incrementally\n            \u03b4 = Gt - self.V[s]                    # obtain the error\n            self.V[s]   += \u03b1*\u03b4                    # update V as per the error\n            self.Q[s,a] += \u03b1*\u03b4*(1 - \u03c0(s,a))*\u03b3t/\u03c4  # update Q as per the erro with complement of the policy \u03c0\n            \u03b3t /= \u03b3\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#the-role-of-discount-factor-gamma-in-policy-gradient-methods","title":"The Role of Discount Factor \\( \\gamma \\) in Policy Gradient Methods","text":"<p>\\( \\gamma \\) seems to play a more important role in policy-gradient methods than in action-value methods. The next few examples show how \\( \\gamma \\) can make the difference between convergence and divergence.</p> <p>The main issue is, as usual, whether the reward is delayed or there is an intermediate reward. If the reward is delayed, we would need to assign \\( \\gamma \\) values that are less than 1 so that the sum of the rewards is discounted, which helps the agent differentiate between longer and shorter paths. However, \\( \\gamma \\) also plays a role in convergence when the reward is not delayed. It complements the role that \\( \\tau \\) plays in the SoftMax policy.</p> <p>Therefore, instead of tuning \\( \\tau \\), we can reduce \\( \\gamma \\) specifically when the goal reward is 0, and the intermediate reward is -1 (reward_0) function. Let us see some examples:</p> <p>Below, we increase the value of \\( \\tau \\) to deal with this issue of divergence.</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=2, \u03b3=1, episodes=100, seed=10 , **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see REINFORCE converged when we increase \\(\\tau\\) which helped the values in SoftMax to become appropriatly smaller to help the algorithm to converge.</p> <p>Let us now decrease the value of \\(\\gamma&lt;1\\) and keep \\(\\tau=1\\)</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=1, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see, decreasing \\( \\gamma \\) helped REINFORCE immensely to converge. Although the reward that we used is intermediate reward ('reward_1'), which is not delayed, discounting the return helped the value function to be more meaningful for the problem at hand. This, in turn, helped the policy to be more appropriate for the problem.</p> <p>Let us now increase \\( \\tau \\) and keep \\( \\gamma &lt; 1 \\). This will reveal another role for \\( \\tau \\).</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=2, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see, increasing \\( \\tau \\) while using \\( \\gamma &lt; 1 \\) did not help. We will mostly therefore use \\( \\gamma &lt; 1 \\) for our policy gradient methods.</p> <p>Note how exploration led to a fully covered environment but slower convergence.</p>"},{"location":"unit2/lesson6/lesson6.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we studied the properties of Monte Carlo algorithms for prediction and control. We started by covering a basic first-visit MC method that averages the returns, similar to what we did in lesson 1, this time for the associative problem (i.e., when we have states that we select specific actions for, un-associated problems do not have states and have been studied in lesson 1). We then created an incremental MC algorithm that allows us to average the returns in a step-by-step manner. To that end, we developed an essential MRP class that will carry the step-by-step and episode-by-episode interaction with an MRP environment and then added a useful set of visualization routines. We further inherited the MRP class in an MDP class that defines policies that depend on the Q function to obtain a suitable policy for an agent (i.e., control).</p> <p>We noted that MC needed to wait until the episode was finished to carry out updates. In the next unit, we will study full online algorithms that mitigate this shortcoming of MC with the cost of bootstrapping. We will be using the MRP and MDP classes that we developed here.</p> <p>Monte Carlo methods provide a powerful alternative to Dynamic Programming for RL problems where the environment\u2019s transition model is unknown. They estimate value functions from sampled episodes and improve policies using exploration strategies like \\( \\epsilon \\)-greedy. We covered two types of methods:</p> <ul> <li>Policy Evaluation: Use MC to estimate state values.</li> <li>Policy Control: Improve policies using MC-based action-value estimates.</li> </ul>"},{"location":"unit2/lesson6/lesson6.html#advantages","title":"Advantages","text":"<ul> <li>Model-free: No need to know the environment\u2019s transition probabilities.  </li> <li>Simple and intuitive: Works by averaging sampled returns.  </li> <li>Works well for episodic tasks.  </li> </ul>"},{"location":"unit2/lesson6/lesson6.html#limitations","title":"Limitations","text":"<ul> <li>Requires complete episodes, which may not always be feasible.  </li> <li>Convergence can be slow compared to Temporal-Difference (TD) methods.  </li> </ul> <p>In the next unit, we will explore Temporal-Difference (TD) learning methods, which update state-value estimates without requiring complete episodes. Instead, TD methods rely on one-step updates, much like Value Iteration incrementally improves policies. We will also introduce TD-based action-value methods for control, including SARSA and Q-learning, which use the Q action-value function to learn optimal policies.</p>"},{"location":"unit2/lesson6/lesson6.html#reading","title":"Reading:","text":"<p>For further reading, you can consult chapter 5 from the Sutton and Barto book. The policy gradient sections in this lesson, and the next are based on chapter 13 of our book. They can be read as they appear in the notebook or delayed until the end of lesson 9.</p>"},{"location":"unit2/lesson6/lesson6.html#your-turn","title":"Your turn","text":"<p>Now it is time to experiment further and interact with the code in worksheet6.</p>"},{"location":"unit2/lesson7/lesson7.html","title":"Lesson 7: Introduction to Intelligent Mobile Robots","text":"<p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the basics of intelligent mobile robots, its different types and sensors.</li> <li>discuss why AI is required for a mobile robot to perform its task full autonomously. </li> <li>understand the concepts of openloop and closedloop control schemes. </li> </ul>"},{"location":"unit2/lesson7/lesson7.html#introduction","title":"Introduction","text":"<p>The very first question that comes in mind is that how a mobile robot gets its intelligence. Let's start with some definitions:</p>"},{"location":"unit2/lesson7/lesson7.html#artificial-intelligence","title":"Artificial Intelligence:","text":"<p>There are several definitions of artificial intelligence or AI. The most specific one is \"AI is the attempt to get the robots to do things that, for the moment, people are better at.\" </p>"},{"location":"unit2/lesson7/lesson7.html#intelligent-robots","title":"Intelligent Robots:","text":"<p>Thus, we can define that \"A robot with ability to learn from its environment and perform some tasks autonomously with high precision is called an Intelligent robot\".  </p>"},{"location":"unit2/lesson7/lesson7.html#kinds-of-robots","title":"Kinds of Robots:","text":"<p>When we talk about mobile robots, there are several kinds of robots that come under the umbrella.</p> <ul> <li>Ground mobile systems: Various types of mobile platforms can be found here such as mobile vehicles with wheels or caterpillars, legged robots (humanoids or animal mimicking), or robots that mimic some other type of animal locomotion, for example, snakes. Ground mobile systems with wheels or caterpillars that do not carry the operator are often referred to as unmanned ground vehicles.</li> <li>Aerial mobile systems: This group consists of mobile systems that fly in a certain aerial space (airplanes, helicopters, drones, rockets, animal-mimicking flying systems; when used without a pilot they are referred to as unmanned aerial vehicles) or orbit the Earth or some other celestial body (satellites).</li> <li>Water and underwater mobile systems: In this group we find different types of ships, boats, submarines, autonomous underwater vehicles, etc.</li> </ul> <p>We will be talking about the wheeled mobile robot in the course. There are several types of wheeled mobile robots.</p> <ul> <li>Differential Drive Robots: Two independently driven wheels (common in robots and small vehicles).</li> <li>Holonomic Robots: Wheels that can move in any direction (high maneuverability).</li> <li>Omnidirectional Robots: Wheels with rollers for translation and rotation (e.g., omnidirectional wheels).</li> <li>Tracked Robots: Continuous tracks for stability and off-road capabilities (e.g., tank-like robots).</li> </ul> <p>A robot can be represented by its kinematics model and dynamic model.</p> <ul> <li>The kinematic model describes geometric relationships that are present in the system. It describes the relationship between input (control) parameters and the behavior of a system given by state-space representation. A kinematic model describes system velocities and is presented by a set of differential first-order equations.</li> <li>Dynamic models describe a system motion when forces are applied to the system. This model includes the physics of motion where forces, energies, system mass, inertia, and velocity parameters are used.# Introduction to Intelligent Mobile Robots</li> </ul> <p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the basics of intelligent mobile robots, its different types and sensors.</li> <li>discuss why AI is required for a mobile robot to perform its task full autonomously. </li> <li>understand the concepts of openloop and closedloop control schemes. </li> </ul>"},{"location":"unit2/lesson7/lesson7.html#introduction_1","title":"Introduction","text":"<p>The very first question that comes in mind is that how a mobile robot gets its intelligence. Let's start with some definitions:</p>"},{"location":"unit2/lesson7/lesson7.html#artificial-intelligence_1","title":"Artificial Intelligence:","text":"<p>There are several definitions of artificial intelligence or AI. The most specific one is \"AI is the attempt to get the robots to do things that, for the moment, people are better at.\" </p>"},{"location":"unit2/lesson7/lesson7.html#intelligent-robots_1","title":"Intelligent Robots:","text":"<p>Thus, we can define that \"A robot with ability to learn from its environment and perform some tasks autonomously with high precision is called an Intelligent robot\".  </p>"},{"location":"unit2/lesson7/lesson7.html#kinds-of-robots_1","title":"Kinds of Robots:","text":"<p>When we talk about mobile robots, there are several kinds of robots that come under the umbrella.</p> <ul> <li>Ground mobile systems: Various types of mobile platforms can be found here such as mobile vehicles with wheels or caterpillars, legged robots (humanoids or animal mimicking), or robots that mimic some other type of animal locomotion, for example, snakes. Ground mobile systems with wheels or caterpillars that do not carry the operator are often referred to as unmanned ground vehicles.</li> <li>Aerial mobile systems: This group consists of mobile systems that fly in a certain aerial space (airplanes, helicopters, drones, rockets, animal-mimicking flying systems; when used without a pilot they are referred to as unmanned aerial vehicles) or orbit the Earth or some other celestial body (satellites).</li> <li>Water and underwater mobile systems: In this group we find different types of ships, boats, submarines, autonomous underwater vehicles, etc.</li> </ul> <p>We will be talking about the wheeled mobile robot in the course. There are several types of wheeled mobile robots.</p> <ul> <li>Differential Drive Robots: Two independently driven wheels (common in robots and small vehicles).</li> <li>Holonomic Robots: Wheels that can move in any direction (high maneuverability).</li> <li>Omnidirectional Robots: Wheels with rollers for translation and rotation (e.g., omnidirectional wheels).</li> <li>Tracked Robots: Continuous tracks for stability and off-road capabilities (e.g., tank-like robots).</li> </ul> <p>A robot can be represented by its kinematics model and dynamic model.</p> <ul> <li>The kinematic model describes geometric relationships that are present in the system. It describes the relationship between input (control) parameters and the behavior of a system given by state-space representation. A kinematic model describes system velocities and is presented by a set of differential first-order equations.</li> <li>Dynamic models describe a system motion when forces are applied to the system. This model includes the physics of motion where forces, energies, system mass, inertia, and velocity parameters are used.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#kinematics-of-wheeled-mobile-robots","title":"Kinematics of Wheeled Mobile Robots","text":"<p>Several types of kinematic models exist:</p> <ul> <li>Internal kinematics: explains the relation between system internal variables (e.g., wheel rotation and robot motion).</li> <li>External kinematics: describes robot position and orientation according to some reference coordinate frame.</li> <li>Direct kinematics and inverse kinematics: Direct kinematics describes robot states as a function of its inputs (wheel speeds, joint motion, wheel steering, etc.). From inverse kinematics, one can design a motion planning, which means that the robot inputs can be calculated for a desired robot state sequence.</li> <li>Motion constraints appear when a system has less input variables than degrees of freedom (DOFs). Holonomic constraints prohibit certain robot poses while a nonholonomic constraint prohibits certain robot velocities (the robot can drive only in the direction of the wheels\u2019 rotation).</li> </ul> <p>Figure below depicts an abstract control scheme for mobile robot systems that we will use throughout this course. This figure identifies many of the main bodies of knowledge associated with mobile robotics.</p> <p></p>"},{"location":"unit2/lesson7/lesson7.html#kinematics-mobile-robot-vs-robotic-manipulator","title":"Kinematics: Mobile Robot Vs Robotic Manipulator","text":"<ul> <li> <p>Workspace: A manipulator robot\u2019s workspace is crucial because it defines the range of possible positions that can be achieved by its end effector relative to its fixture to the environment. A mobile robot\u2019s workspace is equally important because it defines the range of possible poses that the mobile robot can achieve in its environment.</p> </li> <li> <p>Controllability: The robot arm\u2019s controllability defines the manner in which active engagement of motors can be used to move from pose to pose in the workspace. Similarly, a mobile robot\u2019s controllability defines possible paths and trajectories in its workspace.</p> </li> <li> <p>Dynamic Places: Robot dynamics places additional constraints on workspace and trajectory due to mass and force considerations. The mobile robot is also limited by dynamics; for instance, a high center of gravity limits the practical turning radius of a fast, car like robot because of the danger of rolling.</p> </li> </ul> <p>But the significant difference between a mobile robot and a manipulator arm also introduces a significant challenge for position estimation. A manipulator has one end fixed to the environment. Measuring the position of an arm\u2019s end effector is simply a matter of understanding the kinematics of the robot and measuring the position of all intermediate joints. The manipulator\u2019s position is thus always computable by looking at current sensor data.</p> <p>On the other hand, mobile robot is a self-contained automaton that can wholly move with respect to its environment. There is no direct way to measure a mobile robot\u2019s position instantaneously. Instead, one must integrate the motion of the robot over time. Add to this the inaccuracies of motion estimation due to slippage and it is clear that measuring a mobile robot\u2019s position precisely is an extremely challenging task.</p> <p>For the further reading on the topic, you are encouraged to read topic 3.2, 3.3 and 3.4 from the Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D., 2011. Introduction to autonomous mobile robots. MIT press.</p>"},{"location":"unit2/lesson7/lesson7.html#what-is-the-real-challenge-then","title":"What is the real Challenge then?","text":"<p>The major question while designing a mobile robot is \"under what conditions can a mobile robot travel from the initial pose to the goal pose in bounded time?\". Answering this question requires knowledge, both knowledge of the robot kinematics and knowledge of the control systems that can be used to actuate the mobile robot. Mobile robot control is therefore a return to the practical question of designing a real-world control algorithm that can drive the robot from pose to pose using the trajectories demanded for the application.</p>"},{"location":"unit2/lesson7/lesson7.html#control-design-for-mobile-robots","title":"Control Design for Mobile Robots","text":"<p>There are two classical ways to design a control system for a mobile robot.</p> <ul> <li>Open-loop Control Design</li> <li>Closed-loop Control Design</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#open-loop-control","title":"Open-loop Control","text":"<p>The objective of a open-loop kinematic controller is to follow a trajectory described by its position or velocity profile as a function of time. This is often done by dividing the trajectory (path) in motion segments of clearly defined shape, for example, straight lines and segments of a circle. The control problem is thus to pre compute a smooth trajectory based on line and circle segments that drives the robot from the initial position to the final position (figure below).</p> <p></p> <p>This approach can be regarded as open-loop motion control, because the measured robot position is not fed back for velocity or position control. It has several disadvantages:</p> <ul> <li>It is not at all an easy task to pre compute a feasible trajectory if all limitations and constraints of the robot\u2019s velocities and accelerations have to be considered.</li> <li>The robot will not automatically adapt or correct the trajectory if dynamic changes of the environment occur.</li> <li>The resulting trajectories are usually not smooth, because the transitions from one trajectory segment to another are, for most of the commonly used segments (e.g., lines and part of circles), not smooth. This means there is a discontinuity in the robot\u2019s acceleration.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#feedbackclosed-loop-control","title":"Feedback/Closed-loop Control","text":"<p>A more appropriate approach in motion control of a mobile robot is to use a real-state feedback controller. With such a controller the robot\u2019s path-planning task is reduced to setting intermediate positions (subgoals) lying on the requested path.</p> <p>Closed-loop control is a form of motion control in which the path, or trajectory, of the device is corrected at frequent intervals. After motion begins, a position sensor detects possible errors in the trajectory. If an error is detected, the sensor outputs a signal that operates through a feedback circuit to bring the manipulator back on course. The term derives from the fact that the feedback and control-signal circuits together constitute a closed loop. The main asset of closed-loop control is accuracy. In addition, closed-loop control can compensate for rapid, localized, or unexpected changes in the work environment. The principal disadvantages are greater cost and complexity than simpler schemes such as ballistic control. The most common type of classical control used in industry is Proportional Integral Derivative (PID) controller.</p> <p>A general control system follows the methodology given in picture below. </p> <p></p> <p>The actuators are usually the DC motors, step-by-step motors etc. The end effector, as the name say, is the general purpose tool or gripper or hand to mimic the grasping capability for the robotic manipulator. There are different kinds of Sensors Proprioceptive like encoder, gyro, etc. and Exteroceptive sensors like bumpers, rangefinders (infrared, ultrasonic), laser and vision sensors like mono, stereo etc. In order to make useful decision based on these sensor measurements, there are two level of control in each loop: Low-level control and High-level control. </p> <p>The low level control is shown in figure below. In the low level control, following are the fundamental ideas:</p> <p></p> <ul> <li>High-gain PI controllers control the robot\u2019s motors so that the robot moves according to the desired speed profile. </li> <li>The low-level control deals only with the robot actuators control according to the high-level control instructions.</li> <li>If the gains are high enough, the low-level control makes the robot a purely kinematic system.</li> </ul> <p>On the other hand, the high level controller is given in picture below.</p> <p></p> <ul> <li>It processes and computes the signals to send to the low-level controller using data coming from sensors.</li> <li>From its point of view, the robot behaves as a purely kinematic system.</li> <li>For mobile robots, speed control signals are used.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#example-control-of-wmr","title":"Example: Control of WMR","text":"<p>For the sake of an example, lets think about a wheeled mobile robots. The two controllers can be expressed as:</p> <p>Low-Level Control:  - Internal loop on the motors side for controlling the robot actuation. - It is a simple PI for electric drives (linear systems). - It is not affected by the non-holonomic constraints introduced by the wheels. - Known and solved issues</p> <p>High-Level Control - It defines the motion and the behavior of the robot based on the task to be performed - It must consider the kinematic model - Subject to the constraints of the wheels - It has to control a nonlinear and complex system</p> <p>Now we know some basics about controlling the mobile robot, the real challenge comes how a robot see the world.</p>"},{"location":"unit2/lesson7/lesson7.html#sensing","title":"Sensing","text":"<p>The most important question for a mobile robot to go from some initial position to a desired goal position is to know about the following questions:</p> <ul> <li>Where am I relative to the world and how can the robot model/recognize the environment?</li> <li>What is around me?</li> </ul> <p>In order to get answers for these questions, a robot is equipped with onboard sensors like vision, stereo, range sensors, LIDAR. The sensors are classified in two major categories, i.e. proprioceptive/exteroceptive and passive/active.</p> <ul> <li>Proprioceptive sensors measure values internal to the system (robot); e.g. motor speed, wheel load, robot arm joint angles, battery voltage.</li> <li>Exteroceptive sensors acquire information from the robot\u2019s environment; e.g. distance measurements, light intensity, sound amplitude. Hence exteroceptive sensor measurements are interpreted by the robot in order to extract meaningful environmental features.</li> <li>Passive sensors measure ambient environmental energy entering the sensor. Examples of passive sensors include temperature probes, microphones and CCD or CMOS cameras.</li> <li>Active sensors emit energy into the environment, then measure the environmental reaction. Because active sensors can manage more controlled interactions with the environment, they often achieve superior performance.</li> </ul> <p>The readers are encouraged to read Chapter 4 of Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D., 2011. Introduction to autonomous mobile robots. MIT press, for a more details perspective of different sensors and perception for mobile robots.</p>"},{"location":"unit2/lesson7/lesson7.html#perception","title":"Perception","text":"<p>Perception for mobile robots refers to the ability of a robotic system to gather information about its environment through various sensors and then process and interpret that information to make informed decisions and navigate autonomously. Perception is a fundamental component of mobile robot autonomy, as it allows the robot to understand and interact with its surroundings.</p> <p>Key aspects of perception for mobile robots include:</p> <ul> <li> <p>Sensing: Mobile robots are equipped with a variety of sensors, such as cameras, LiDAR (Light Detection and Ranging), ultrasonic sensors, radar, and more. These sensors collect data about the robot's surroundings, including information about objects, obstacles, terrain, and other relevant environmental features.</p> </li> <li> <p>Sensor Fusion: Often, mobile robots use multiple sensors of different types to create a more comprehensive understanding of their surroundings. Sensor fusion techniques combine data from various sensors to provide a more accurate and robust perception of the environment.</p> </li> <li> <p>Object Detection and Recognition: Mobile robots need to detect and recognize objects in their environment. This can include identifying obstacles, people, other robots, landmarks, or any other objects relevant to their task.</p> </li> <li> <p>Mapping: Mobile robots create maps of their surroundings, known as occupancy grids or environmental maps. These maps help the robot understand where it is located in relation to the environment and plan paths or make navigation decisions.</p> </li> <li> <p>Localization: Localization is the process of determining the robot's position in a known map or within its environment. Mobile robots often use techniques like SLAM (Simultaneous Localization and Mapping) to estimate their position and orientation.</p> </li> <li> <p>Obstacle Avoidance: Robots must be able to detect and avoid obstacles in real-time to navigate safely. Perception systems provide information about the location and shape of obstacles, allowing the robot to plan a collision-free path.</p> </li> <li> <p>Environment Understanding: Perception systems may also provide information about the type of terrain, the presence of specific landmarks, or environmental conditions like lighting or weather, which can impact the robot's behavior and decision-making.</p> </li> <li> <p>Semantic Understanding: Some advanced mobile robots can understand the semantics of their environment. For example, they can recognize specific objects or understand human commands and gestures.</p> </li> </ul> <p>Perception for mobile robots is essential for various applications, such as autonomous vehicles, delivery robots, search and rescue robots, agricultural robots, and more. Advances in sensors, computer vision, machine learning, and artificial intelligence have significantly improved the perception capabilities of mobile robots, enabling them to operate safely and effectively in a wide range of environments and tasks.</p>"},{"location":"unit2/lesson7/lesson7.html#why-a-robot-needs-intelligence","title":"Why a robot needs Intelligence?","text":"<p>It is usually fine to ask robot to perform a same task repeatedly without any human intervention under ideal conditions. But this is usually not the case. In a real world scenario, the robot has to learn from its environment, surroundings and interactions with things like we human do. The field of AI can help the robot to get this insight. There are seven main areas in AI which helps the robot to perform tasks intelligently. </p>"},{"location":"unit2/lesson7/lesson7.html#knowledge-representation","title":"Knowledge representation.","text":"<p>An important, but often overlooked, issue is how the robot represents its world, its task, and itself. Suppose a robot is scanning a room for an elderly person to assist. What kind of data structures and algorithms would it take to represent what a human looks like or what the human might need? How does a program capture everything important so as not to become computationally intractable? AI robotics explores the tension between the symbolic world representations that are easy for computers to create optimal paths through versus the direct perception used by animals that work directly from perception.</p>"},{"location":"unit2/lesson7/lesson7.html#understanding-natural-language","title":"Understanding natural language.","text":"<p>Natural language is deceptively challenging, apart from the issue of recognizing words which is now being done by commercial products such as Siri and Alexa. It is not just a matter of looking up words, which is the subject of the following apocryphal story about AI. The story goes that after Sputnik went up, the US government needed to catch up with the Soviet scientists. However, translating Russian scientific articles was time consuming and not many US citizens could read technical reports in Russian. Therefore, the US decided to use these newfangled computers to create translation programs. The day came when the new program was ready for its first test. It was given the proverb: the spirit is willing, but the flesh is weak. The reported output: the vodka is strong, but the meat is rotten. AI robotics explores the implicit and explicit communication needed for comfortable social interaction with robot.</p>"},{"location":"unit2/lesson7/lesson7.html#learning","title":"Learning.","text":"<p>Imagine a robot that could be programmed by just watching a human, or that a robot could learn by just repeatedly trying trying a new task by itself. Or that a robot experimented with a task through trial and error to generate a new solution. AI robotics is a study of the different types of learning and how learning can be applied to different functions.</p>"},{"location":"unit2/lesson7/lesson7.html#planning-and-problem-solving","title":"Planning and problem solving.","text":"<p>Intelligence is associated with the ability to plan actions needed to accomplish a goal and solve problems when those plans fail. One of the early childhood fables, the Three Pigs and the Big, Bad Wolf, involves two unintelligent pigs who do not plan ahead and an intelligent pig who is able to solve the problem of why his brothers\u2019 houses have failed, as well as generate a new plan for an unpleasant demise for the wolf. AI robotics relies on planning and problem solving to cope with the unpredictability of the real world.</p>"},{"location":"unit2/lesson7/lesson7.html#inference","title":"Inference.","text":"<p>Inference is generating an answer when there is not complete information. Consider a planetary rover looking at a dark region on the ground. Its range finder is broken and all it has left is its camera and a fine AI system. Assume that depth information cannot be extracted from the camera. Is the dark region a canyon? Is it a shadow? The rover will need to use inference either to actively or passively disambiguate what the dark region is (e.g., kick a rock at the dark area versus reason that there is nothing nearby that could create that shadow). AI robotics techniques are increasingly engaging in inference.</p>"},{"location":"unit2/lesson7/lesson7.html#search","title":"Search.","text":"<p>Search does not necessarily mean searching a large physical space for an object. In AI terms, search means efficiently examining a knowledge representation of a problem (called a \u201csearch space\u201d) to find the answer. Deep Blue, the computer that beat World Chess master Garry Kasparov, won by searching through almost all possible combinations of moves to find the best choice. The legal moves in chess, given the current state of the board, formed the search space. Data mining or Big Data is a form of search. AI robotics uses search algorithms in generating optimal solutions in navigation or searching a knowledge representation.</p>"},{"location":"unit2/lesson7/lesson7.html#vision","title":"Vision.","text":"<p>Vision is possibly the most valuable sense humans have. Studies by Harvard psychologist, Steven Kosslyn, suggest that much of human problem solving capabilities stems from the ability to visually simulate the effects of actions in our head. As such, AI researchers have pursued creating vision systems both to improve robotic actions and to supplement other work in general machine intelligence. AI robotics relies heavily on computer vision to interpret video data and also the RGBD cameras, such as Microsoft\u2019s Kinect.</p>"},{"location":"unit2/lesson7/lesson7.html#your-turn","title":"Your turn:","text":"<p>Follow Worksheet7 to understand how to deal with ros2 programmatically.    </p>"},{"location":"unit2/lesson7/lesson7.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulations of RL all of which assumed that we use a table representation for our state space. In the next unit, we will study other offline and fully online RL algorithms that use bootstrapping. Additionally, we will study planning algorithms and then use function approximation instead of a table to represent the state space that can be continuous and infinite.</p>"},{"location":"unit2/lesson7/worksheet7.html","title":"Working with ROS 2 Packages, Nodes, and Topics: A Tutorial Using Turtlesim","text":"<p>This tutorial introduces the basics of working with ROS 2 by guiding you through creating a simple package, developing a Python node, and using topics to control a turtle in the Turtlesim simulation. You'll learn how to publish movement commands, build and run the package, and inspect topics to enhance your understanding of ROS 2 communication and node development.</p> <p>Learning Outcomes</p> <ol> <li>Generate a Python-based ROS 2 package using ros2 pkg create.</li> <li>Write a Python node that publishes velocity commands to control a turtle.</li> <li>Build and run the package to control the turtle in Turtlesim.</li> <li>Inspect and interact with ROS 2 topics using ros2 topic list and ros2 topic echo.</li> <li>Modify the node to add more complex controls, such as rotation.</li> </ol> <p>Turtlesim is a simple simulation package that can be used to learn ROS 2 fundamentals. We will walk through creating a ROS 2 package, writing nodes, and using topics to control a simulated turtle.</p>"},{"location":"unit2/lesson7/worksheet7.html#step-1-create-a-ros-2-package","title":"Step 1: Create a ROS 2 Package","text":"<p>To create a ROS 2 package, navigate to your ROS 2 workspace:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> <p>create a new ROS 2 package, run the following command:</p> <pre><code>ros2 pkg create --build-type ament_python my_turtle_pkg\n</code></pre> <p>Once the package is created, navigate to the newly created package:</p> <pre><code>cd my_turtle_pkg\n</code></pre>"},{"location":"unit2/lesson7/worksheet7.html#step-2-create-a-simple-node-to-control-the-turtle","title":"Step 2: Create a Simple Node to Control the Turtle","text":"<p>In the my_turtle_pkg package, create a Python script to control the turtle. Start by creating a turtle_control.py file in the my_turtle_pkg/my_turtle_pkg directory.</p> <p>Create the Python file:</p> <pre><code>touch my_turtle_pkg/turtle_control.py\n</code></pre> <p>Open turtle_control.py and add the following code:</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\n\nclass TurtleControl(Node):\n    def __init__(self):\n        super().__init__('turtle_control')\n        self.publisher_ = self.create_publisher(Twist, '/turtle1/cmd_vel', 10)\n        self.timer = self.create_timer(1.0, self.publish_velocity)\n\n    def publish_velocity(self):\n        msg = Twist()\n        msg.linear.x = 2.0  # move forward\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    turtle_control_node = TurtleControl()\n\n    rclpy.spin(turtle_control_node)\n    turtle_control_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>TurtleControl is a node that publishes velocity commands to the /turtle1/cmd_vel topic. It creates a publisher to send Twist messages to control the turtle. The publish_velocity function sends commands to move the turtle forward every 1 second.</p>"},{"location":"unit2/lesson7/worksheet7.html#step-3-build-and-run-the-package","title":"Step 3: Build and Run the Package","text":"<p>Now, go back to your ROS 2 workspace and build the package:</p> <p><pre><code>cd ~/ros2_ws\ncolcon build --packages-select my_turtle_pkg\n</code></pre> In a new terminal, launch the Turtlesim simulation:</p> <pre><code>ros2 run turtlesim turtlesim_node\n</code></pre> <p>This will open a window with a turtle in a 2D world.</p> <p>Now, run your turtle_control.py node:</p> <pre><code>ros2 run my_turtle_pkg turtle_control\n</code></pre> <p>You should see the turtle in the Turtlesim window moving forward.</p>"},{"location":"unit2/lesson7/worksheet7.html#step-4-use-ros2-topics-to-control-the-turtle","title":"Step 4: Use ROS2 Topics to Control the Turtle","text":"<p>You can view the topic being used by your node by running the following command:</p> <p><pre><code>ros2 topic list\n</code></pre> This will display all active topics. You should see /turtle1/cmd_vel listed.</p> <p>To see the messages being published to the topic, run: <pre><code>ros2 topic echo /turtle1/cmd_vel\n</code></pre></p> <p>You can modify the TurtleControl node to add more complex control, such as rotating the turtle. Modify the publish_velocity method:</p> <pre><code>def publish_velocity(self):\n    msg = Twist()\n    msg.linear.x = 2.0  # move forward\n    msg.angular.z = 1.0  # rotate\n    self.publisher_.publish(msg)\n    self.get_logger().info('Publishing: \"%s\"' % msg)\n</code></pre> <p>To stop the turtle, simply press Ctrl+C in the terminal where the node is running.</p>"},{"location":"unit2/lesson7/worksheet7.html#summary","title":"Summary","text":"<p>This tutorial introduces the core concepts of working with ROS 2 by guiding users through the creation of a simple ROS 2 package and node to control a turtle in the Turtlesim simulation. </p>"},{"location":"unit3/lesson10/lesson10.html","title":"10. Planning in RL(optional)","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit3/lesson10/lesson10.html#lesson-9-tabular-methods-planning-and-learning","title":"Lesson 9- Tabular Methods: Planning and Learning","text":"<p>Learning outcomes 1. understand how to embed model learning withing an a reinforcement learning algorithms 2. understand mode-based RL approach 3. appreciate the boost in performance obtained due to infusing planning within the RL framework 4. appreciate that planning is usually possible for tabular RL while function approximation is still work in progress (mainly via replaying)</p> <p>In this lesson, we cover a set of methods that use a blend of model-based and model-free to achieve planning and learning simultaneously. Planning involves using some model of the environment to predict the next action or reward of the environment and then use that to aid it in obtaining an estimate of the value function. We have already seen model-based algorithms in the early lesson. Can you remember what they are?... Dynamic programming it is. In this lesson, however, we take these concepts one step ahead, and we will develop a unified view of model-free (MC and TD etc.) and model-based (DP and other) algorithms. </p> <p>Model-based methods can be stochastic or deterministic this depending on the problem, but even if the environment itself is deterministic, we might want to come up with a stochastic model of it that will give us a distribution over all the states that represent the belief that an agent would be in state sn given that it was in state s, these are called distribution models. </p> <p>In fact, we had already seen such an approach when we dealt with the policy_evaluation() function in the DP lesson, where this function depends on the dynamics \\(p(s',r|s,a)\\) to account for all of the different next state and rewards combinations to find an estimate. Distribution models are powerful because they allow us to sample from them at any point in time, but they are computationally expensive.</p> <p>Another possibility is to use sampling. This might appear to contradict what we said earlier about model-free, which uses sampling to estimate the value function. The difference here is that we would use sampling to obtain what might sn be according to the model and then obtain an estimate of the value function. </p> <p>When the environment is deterministic, sampling can be used relatively efficiently to build the dynamics. In fact, we had already seen such an approach when we dealt with the dynamics() function in the DP lesson, where this function tries to estimate the dynamics of the environment by observing the next state and reward from a current state and action combination.</p> <p>In all cases, the main advantage of planning is that we can plan ahead many steps without actually taking action, unlike non-planning algorithms such as TD. All we would be doing is to assume that the agent took action and then build on that. We can produce a whole imaginary episode using sampling, which is what we call a simulated experience which means it is an experience that the agent might take but is not real. With model-free sampling methods, we use an actual experience that the agent went through to estimate the value function. With distribution models, in theory, we can produce all possible episodes that the agent can take with their probabilities. Of course, that would mean accounting for many possibilities in both sampling and distribution models, and this is the main source of complexity in planning algorithms because they usually have a complexity of \\(O(n^2)\\) where n is the number of states.</p> <p>However, this difference between using a simulated or real experience is irrelevant from the update rules perspective. So, we can apply many of the covered methods in a simulated experience and a real experience. This is quite convenient since it will require less treatment and help us develop our unified view of planning and learning. Note that in planning, we will not deal with prediction. We will cover control only because executing a plan naturally involves an advanced control scenario.</p> <p>Finally, we should point out that due to the above complexity and assumptions, planning is usually confined to the tabular methods, and we will not see it when we move to function approximation which assumes that the state space can be infinite.</p> <p>Reading: The accompanying reading of this lesson is chapter 8 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *  # an RL base alg lib, value-based and policy-grad RL tabular algorithms\nimport bisect        # this will help to insert in a sorted Queue list\n</code></pre> <p>Let us start by covering a basic algorithm, that shows the basic idea of planning using sampling.  Below we show the Random sample one-step Q-planning method.</p>"},{"location":"unit3/lesson10/lesson10.html#dyna-q-integrating-model-learning-and-q-learning","title":"Dyna-Q: Integrating Model learning and Q Learning","text":"<p>In the DP lesson, we have built a dynamics() function used in the policy_iteration() and other functions. The dynamics() function is an example of model learning. We have used this function to provide a model for the DP algorithms, assuming it is already available. Hence, we started each of them with the model-learning step. This section will integrate the model learning with the direct RL learning we have seen in other lessons (like TD and MC). We use the real experience the agent is going through to improve both our model l of the environment (model learning) and our value-function estimation (direct RL learning).</p> <pre><code>class DynaQ(MDP()):\n    def __init__(self, \u03b1=.1, m=10, **kw):\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.m = m          # how many steps to plan ahead\n        self.store = False  # no need to store experience, the Model will give us what we need\n        self.Model = np.zeros((self.env.nS, self.env.nA, 3)) # self.env.nR, self.env.nS))\n        self.stop_early = self.\u03c0isoptimal # stop experiment when policy is optimal, need to set Tstar to take effect\n\n    def init(self):\n        self.nUpdates = 0 # number of updates to optimal solution\n        super().init()\n\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n        self.Model[s,a] = [rn, sn, done]\n        self.nUpdates +=1\n\n        sa = np.argwhere(self.Model[:,:,1]!=np.array(None))\n        for _ in range(self.m):    \n            # get s,a that has been visited before and randomly select a pair of them\n\n            ind = randint(sa.shape[0])\n            s, a = sa[ind]\n\n            rn,sn,done = self.Model[s,a]; sn=int(sn)        \n            self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n            self.nUpdates +=1\n\n#             s = sn\n#             rn,sn,done = self.Model[sn,self.Q[sn].max()]\n</code></pre> <pre><code>dynaQ = DynaQ(env=maze(), episodes=2, seed=10, m=50, **demoQ()).interact()\n</code></pre> <p></p> <p>Let us see how many updates DynaQ had to do to achieve what it has achieved in the first 2 episodes!</p> <pre><code>dynaQ.nUpdates\n</code></pre> <pre><code>10659\n</code></pre> <p>For comparison let us do the same for Q learning, ( DynaQ is equivalent to Q learning fro m=0). This agent will take a bit of time sine we are training for 2 episodes.</p> <pre><code>qlearn = DynaQ(env=maze(), episodes=2, seed=1, m=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>qlearn.nUpdates\n</code></pre> <pre><code>1011\n</code></pre> <p>As we can see the difference is huge between both, this difference shown on the performance. Let us study how the performance varies with n (the number of planning steps)</p>"},{"location":"unit3/lesson10/lesson10.html#the-effect-of-planning-steps-on-dynaq","title":"The effect of planning steps on DynaQ","text":"<pre><code>def mazePlanning(runs=30, algo=DynaQ, label='DynaQ', yticks=True):\n    if yticks: plt.yticks([14, 200, 400, 600, 800])\n    for m in [0, 5, 50]:\n        DynaQ0 = Runs(algorithm=algo(env=maze(), \u03b1=.1, \u03b3=.95, m=m, episodes=50), \n                      runs=runs, plotT=True).interact(label=label+' m = %d'%m)\nfig_8_2 = mazePlanning\n</code></pre> <pre><code>fig_8_2()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n</code></pre>"},{"location":"unit3/lesson10/lesson10.html#prioritized-sweeping","title":"Prioritized Sweeping","text":"<p>In DynaQ, the algorithm uniformly chooses n number of previously visited states and re-do their updates. This is ok when the environment is small. However, when the environment becomes big, the performance will dip, and the algorithm will need to do more updates per step to reach an optimal solution. This is where prioritising more important updates over less important ones optimises compute resource usage and achieves more in fewer steps. This is the idea of prioritized sweeping algorithms; it will sweep through different previous states' updates but prioritise those needing the most changes. It measures this priority by the absolute value of the TD error (yes, TD again!). The higher this error value is, the higher its corresponding priority is. It uses a queue to add states that need to update the most (as per their TD error) on the top of the queue. Then it will go through the queue one by one until its number of planning steps is done or until the queue is empty. The algorithm is shown below.</p> <pre><code>class Psweeping(MDP(MRP)):\n    def __init__(self, \u03b1=.1, m=5, \u03b8=1e-4, **kw):\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.m = m\n        self.\u03b8 = \u03b8\n\n        self.store = False # no need to store experience, the Model will give us what we need\n        self.Model = np.zeros((self.env.nS, self.env.nA, 3))\n        self.stop_early = self.\u03c0isoptimal # stop experiment when policy is optimal, needs to set Tstar to take effect\n\n\n    def init(self):\n        self.PQueue = [] # collections.deque()# native list is more efficent than a linked list\n        self.nUpdates = 0 # number of updates to optimal solution\n        super().init()\n\n    def insert(self, P,s,a): # P is the priority\n        found_lowerP = False # found an entry with a lower priority\n        for i, (P_old, s_old, _) in enumerate(self.PQueue[-2*self.m:]):\n            if s==s_old:\n                if P&gt;=P_old: self.PQueue.pop(i)\n                else:  found_lowerP = True\n                break\n        # insert the tuple in the right position according to P\n        if not found_lowerP: bisect.insort_left(self.PQueue, (P,s,a))\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n\n        P = abs(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n        if P&gt;=self.\u03b8:  self.insert(P,s,a)\n        self.Model[s,a] = [rn, sn, done]\n\n        i=0\n        while self.PQueue and i&lt;=self.m:\n            _,s,a = self.PQueue.pop()\n            rn, sn, done = self.Model[s,a]; sn=int(sn)\n            self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n            self.nUpdates +=1\n            i+=1\n\n            # to guarantee equivelncy with Qlearning when \u03b8=0, m=0\n            if self.m==0: break \n            # go backward to previous states and actions sp, ap\n            for sp,ap in np.argwhere(self.Model[:,:,1]==s):\n                r,_,done= self.Model[sp,ap] #_==s\n                P = abs(r + (1- done)*self.\u03b3*self.Q[s].max() - self.Q[sp,ap])\n                if P&gt;=self.\u03b8: self.insert(P,sp,ap)\n</code></pre> <p>Note that for the insert() function, we check first if state s is already in the queue, and if so, remove its entry if it has less priority than the new entry. Otherwise, do not add anything. </p> <p>PQueue is an ascending sorted queue according to P, not to s, and hence we cannot do a binary search for s, so we had to reside to linear search we tried to cut the search time by limiting the queue scope to 2n from the end of the queue.</p> <p>Let us ensure that Qlearning and prioritized sweeping are almost identical for \u03b8=0 and m=0.</p> <pre><code>psweep0 = Runs(algorithm=Psweeping(env=maze(), \u03b1=.1, \u03b3=.95, m=0, \u03b8=0, episodes=50),runs=10, plotT=True).interact(label='psweep n = %d'%0)\nqlearn = Runs(algorithm=Qlearn(env=maze(), \u03b1=.1, \u03b3=.95, episodes=50),runs=10, plotT=True).interact(label='Qlearn')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p> <pre><code>psweeping = Psweeping(env=maze(), episodes=2, seed=1, m=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>3\n</code></pre> <p>Note how prioritised sweeping done only 3 updates and compare this to DynaQ.  Ok so now let us test with m=50</p> <pre><code>psweeping = Psweeping(env=maze(), episodes=2, seed=1, m=50, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>2754\n</code></pre> <p>Compare the above 5k number of updates for prioritised sweeping with the 32k number of update for DynaQ for the same number of planning states m=50. The difference is huge.</p>"},{"location":"unit3/lesson10/lesson10.html#the-effect-of-planning-steps-on-prioritised-sweeping","title":"The effect of planning steps on Prioritised Sweeping","text":"<p>Ok let us conduct the same experiment as before and test to see how the performance changes with n.</p> <pre><code>mazePlanning(runs=10, algo=Psweeping, label='Prioritised Sweeping', yticks=False)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p>"},{"location":"unit3/lesson10/lesson10.html#prioritized-sweeping-on-different-maze-sizes","title":"Prioritized sweeping on different maze sizes","text":"<pre><code>def maze(rows=6, cols=9, **kw):\n    return Grid(gridsize=[rows,cols], s0=int((rows)/2)*cols, goals=[rows*cols-1], style='maze', **kw)\n</code></pre> <pre><code>class mazes:\n    def __init__(self, m=6, **kw):\n        gridsizes = [(6, 9), (9 , 12), (12, 18), (16, 26), (24, 34), (32, 50), (39, 81), (60, 104)][:m]\n        self.env = []\n        for rows,cols in gridsizes:\n            self.env.append(maze(rows,cols, **kw))\n\n    def __getitem__(self, i): return self.env[i]\n\n    def sizes(self):\n        sizes = [0]\n        for mz in  self.env: sizes.append(mz.nS_available())\n        return sizes\n\nprint(mazes().sizes())\n</code></pre> <pre><code>[0, 47, 92, 188, 372, 748, 1500]\n</code></pre> <pre><code>mazes()[0].render()\n</code></pre> <pre><code>psweeping = Psweeping(env=mazes(figsize=[19,3.5])[3], \u03b1=.5, episodes=50, seed=1, m=5, **demoQ()).interact()\n</code></pre> <p>Let us examine the number of steps required by prioritised sweeping and compare it to the number of updates</p> <pre><code>psweeping.t\n</code></pre> <pre><code>48\n</code></pre> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>36288\n</code></pre> <pre><code>%time dynaQ = DynaQ(env=mazes(figsize=[19,3.5])[3], \u03b1=.5, episodes=50, seed=1, \\\n                    m=5, animate=True, **demoQ()).interact()\n</code></pre> <pre><code>CPU times: user 43.1 s, sys: 7.77 s, total: 50.9 s\nWall time: 25.7 s\n</code></pre> <p></p> <pre><code>dynaQ.t\n</code></pre> <pre><code>63\n</code></pre> <pre><code>dynaQ.nUpdates\n</code></pre> <pre><code>240648\n</code></pre> <p>As we can see there is a big difference between the number of updates for each algorithms.</p> <p>After experimenting with the different mazes environment we can roughly come up with a reasonably flexible optimal number of steps for each environment which we stored in Tstar and is used in the below function.</p>"},{"location":"unit3/lesson10/lesson10.html#runs-and-comparison-for-planning-algorithms","title":"Runs and Comparison for Planning Algorithms","text":"<p>We need to write some new functions to perform several runs and comparisons while allowing the algorithm to stop before finishing all the episodes. The reason we cannot use our original Runs class is due to allowing for stopping before finishing all the episodes. We show this simple function below.</p> <pre><code>def PlanningRuns(algorithm, runs=1, envs=mazes(), Tstar = [17,23,38,55,75,110,300]):\n    nUpdates = np.ones((runs,len(envs.sizes())))*10\n    for run in range(runs):\n        for i, env in enumerate(envs):\n            nUpdates[run,i+1] = algorithm(env=env, \u03b1=.5, episodes=60, m=5, Tstar=Tstar[i], seed=(i+1)*(run+10)).interact().nUpdates\n    return nUpdates\n</code></pre> <pre><code>class PlanningCompare:\n\n    def __init__(self, algos=[DynaQ, Psweeping], runs=1, m=5):\n        self.envs = mazes(m=m)\n        self.algos = algos\n\n        self.nUpdates = []\n        for i, algo in enumerate(algos):\n            print('\\nMaze Planning with %s...........................'%algo.__name__)\n            self.nUpdates.append(PlanningRuns(algorithm=algo, runs=runs, envs=self.envs ))\n\n    def Plot(self):\n        plt.yscale('log')\n        sizes = self.envs.sizes()\n        plt.xticks(sizes)\n        for i, nUpdate in enumerate(self.nUpdates):\n            plt.plot(sizes, self.nUpdates[i].mean(0), label=self.algos[i].__name__)\n\n        plt.xlabel('Gridworld size (#states)')\n        plt.ylabel('Updates until optimal solution')\n        plt.legend()\n        plt.show()\n</code></pre> <pre><code>def example_8_4():\n    PlanningCompare().Plot()\n</code></pre> <pre><code>%time example_8_4()\n</code></pre> <pre><code>Maze Planning with DynaQ...........................\nexperience stopped at episode 5\nexperience stopped at episode 5\nexperience stopped at episode 11\n\nMaze Planning with Psweeping...........................\nexperience stopped at episode 1\nexperience stopped at episode 4\nexperience stopped at episode 15\nexperience stopped at episode 32\nexperience stopped at episode 20\n</code></pre> <p></p> <pre><code>CPU times: user 28.8 s, sys: 646 ms, total: 29.5 s\nWall time: 29.3 s\n</code></pre>"},{"location":"unit3/lesson10/lesson10.html#conclusion","title":"Conclusion","text":"<p>This lesson covered two major planning algorithms, namely the Dyna-Q and the Prioritised Sweeping. These have variants, but what we covered are quite dominant algorithms in the RL planning landscape. We saw that Dyna-Q is quite good at finding a complete solution with the cost of a higher number of the uniformly selected past update. Prioritised Sweeping, on the other hand, is selective in its update and prioritises those expected to need the most attention due to the latest update, and they propagate backwards towards previously visited states. Prioritised sweeping is faster and more promising. However, we need to tune an extra hyperparameter of the threshold \u03b8. In practice, a small value for \u03b8 seems to work fine, but we have to pay attention to the learning rate \u03b1 as it also dictates the rate at which the update propagates backwards. If \u03b1 is set to a small value, prioritised sweeping can suffer from a significant slowness in its performance.</p>"},{"location":"unit3/lesson10/lesson10.html#your-turn","title":"Your turn","text":"<ol> <li>consider what you would need to change in the prioritised sweeping in order to deal with a stochastic environment</li> </ol>"},{"location":"unit3/lesson11/lesson11.html","title":"Lesson 10: Mobile Robot Localization and SLAM","text":"<p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the issues and problems associated with a mobile robot localising its position within an environment</li> <li>discuss the sources of uncertainty in robot localization, particularly motion noise, sensor noise, sensor aliasing, and initial uncertainty</li> <li>apply your knowledge of robot localization to simulate a mobile robot navigating an environment</li> </ul> <p>In this lesson, you learn about the localization and some of the issues and problems associated with a mobile robot localising its position within an environment. We also discuss how a localization and mapping work in parallel.</p>"},{"location":"unit3/lesson11/lesson11.html#localization","title":"Localization","text":"<p>A mobile robot navigating in an environment needs to know its position within that environment. We call this robot localization and often refer to the robot's environment as the \"map\" when discussing robot localization. The localization task is not always straightforward for a robot because there are many sources of noise that confuse the robot about its location.</p> <p></p> <p>Localization tries to answer the question of how a mobile robot can know where it is in any given environment. Consider the mobile waiter robot in Figure 3.1 that needs to navigate around a restaurant environment. The waiter robot has the task of taking orders from customers and delivering their food from the kitchen when it is ready. To complete this task it needs to move around the tables, or obstacles, in this environment. For the robot to successfully move to a particular table or to the kitchen, it needs to know its current location (the start of moving) and its desired location (goal of moving).</p> <p>As the robot waiter navigates around the restaurant, it needs to know where it is, or localize itself. We can make solving the question of localization easier by assuming that when the restaurant owner first turns on the robot, they tell it where it is. This means that the waiter robot knows perfectly where it is at the beginning, because the restaurant owner set its start position. In the real world, there are factors that make it difficult to be certain of a robot's start position, but to introduce localization, we will assume that the restaurant owner is able to do that accurately in our restaurant example.</p>"},{"location":"unit3/lesson11/lesson11.html#dead-reckoning-or-odometry","title":"Dead reckoning or odometry","text":"<p>Next, the restaurant opens, and the owner sets the robot to work waiting on tables. The waiter robot starts to move to complete tasks. When it does this, it tries to keep track of its pose using dead reckoning.</p> <ul> <li>Pose is the location and rotation for our waiter robot.</li> <li>Dead reckoning (also called odometry) is the process the robot uses to calculate its current position, using a previously determined position and estimated speeds over an elapsed time.</li> </ul> <p>The waiter robot can use knowledge it has of its wheels in order to complete dead reckoning. It knows the orientation and size of its wheels, plus the speed at which they are rotating, and it is able to compute its own velocity at any point in time. It can integrate these velocities to calculate its current position as it continues to move in an environment.</p> <p>Example: \"Waiter robot calculates its new position\"</p> <p>Using its knowledge of its wheels (size, orientation, velocity of rotation):</p> <ul> <li>the waiter robot waiting at starting position, X, can calculate that if it moves in a forward direction for one metre at a known velocity, it can update its position in its internal environment map by one metre forward, X + 1.</li> </ul>"},{"location":"unit3/lesson11/lesson11.html#effector-noise-or-motion-uncertainty","title":"Effector noise or motion uncertainty","text":"<p>The problem with dead reckoning is that even if the restaurant owner commands the robot to move with a certain velocity, its actual motion will be different. This can be due to:</p> <ul> <li>imperfections in robot structure, for example, one wheel is not fully inflated and does not move as far as the robot expects;</li> <li>the environment, for example, there are bumps on the ground, or a surface which cause the wheels to slip or skid.</li> </ul> <p>These factors mean that when the robot completes the calculation to move forward one metre, it is likely to move less than one full metre forward. The robot will not be in the pose that its calculation suggests it will be located.</p> <p>Therefore, there will always be imperfections in the way a robot moves, even if these are very minor, because of multiple imperfections in the environment. These imperfections add up. If the robot only considers the rotation of its wheels, there will be a significant difference between the data the robot is using to calculate its location and what the restaurant owner observes about the position of their waiter robot. In the real world, there are always factors that mean a robot never moves exactly as its owner intended it to move.</p> <p>These factors are called effector noise or sometimes motion uncertainty, or actuator noise. The term effector noise relates to special sensors found on a robot that we call effectors. Effectors are similar to other sensors in that they receive information about the environment, but robots also use them to move in an environment.</p> <p>Errors in a robot's knowledge of its position tend to accumulate over time. In this short video, we explains why this happens.</p> <p>Link to the Video</p> <p>The video is approximately 3-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#sensors-used-to-identify-environmental-features","title":"Sensors used to identify environmental features","text":"<p>So far, we have established the idea that even if the robot knows its starting pose perfectly, as it moves in the real world, its knowledge of its location becomes more and more uncertain. We can solve this uncertainty by adding sensors to a robot that enable it to gather data on environmental features to figure out where it exactly is.</p> <p>In this video, we explains the principles of using sensors so that a robot can estimate its own position as it moves within an environment. He uses the example of a single beam range sensor to explain why it is common to have uncertainty over the exact position of a robot, or localization uncertainty, and introduces the term sensor noise.</p> <p>Link to the Video</p> <p>The video is approximately 8-minutes long. Download transcript (PDF).</p> <p>\"Summary of localization uncertainty from the video\"</p> <p></p> <p>In the video, you saw that both effector noise and sensor noise result in localization uncertainty for a robot. We noted that:</p> <ul> <li> <p>we can use sensors to aid a robot to localize its position in an environment. For example, in Figure 3.2 above, we can predict that the robot will be somewhere in the grey region of the environment</p> </li> <li> <p>a single reading from a sensor does not usually provide an accurate enough localization for a robot. A robot needs to take many readings to be more certain of its location, and as it moves, it will need to continually take sensor readings to update its location</p> </li> <li> <p>in the real world, it will never be possible to narrow down a robot\u2019s position to a single point due to sensor noise.</p> </li> </ul> <p>Having considered sensor noise, you saw that there is another source of uncertainty, for example when a robot moves parallel to the surface of an object. We call this uncertainty sensor aliasing, when different robot poses result in the same sensor values.</p> <p>We observed a robot in the environment shown in Figure 3.3 below, and saw that when it reads that it is 100cm from the wall, it is actually somewhere in the grey region due to the combined uncertainty due to sensor noise and sensor aliasing.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#localization-algorithms","title":"Localization algorithms","text":"<p>Localization algorithms enable a robot to estimate its pose, or localize, in a given map as accurately as possible in the face of all these uncertainties.</p> <p>Over the next three Lessons we will study how to use sensors and apply this to a simulated TurtleBot in the Activities that accompany some of the Lessons. We will integrate the information coming from the sensors with the information coming from the wheels to localize a robot in a given environment.  We are focusing on this because it is the most general solution to this problem of localization for your robot. It will use sensors and actuators and as it moves, you will integrate all this information.</p> <p>Here are other ways to make it easy for a robot to localize if you are able to structure the environment artificially.</p>"},{"location":"unit3/lesson11/lesson11.html#example-1-robot-system-in-a-factory","title":"Example 1: Robot system in a factory","text":"<p>In this example, where only robots inhabit a factory space, you can place markers in the environment. In Figure 3.4 is a typical distribution warehouse. You can see small white squares (similar to the Unique marker shown in Figure 3.5) at equal distances between each other on the floor surface of the factory. The robots have cameras that look down onto the floor. When a camera sees the marker, it can wirelessly ask the computer system where that marker is in the warehouse. Each marker has a unique position in the warehouse, so this system allows the robot to know exactly where it is within this engineered environment.</p> <p></p> <p>Figure 3.5 Unique marker with known absolute 2D position in the map. These markers are sometimes called \u00abAR marker\u00bb due to their popular use in Augmented Reality.</p>"},{"location":"unit3/lesson11/lesson11.html#beacon-based-localization-with-augmented-reality-markers","title":"Beacon-based localization with augmented reality markers","text":"<p>Flying robots can also use similar markers on the ground to the unique marker shown in Figure 3.5. These markers are sometimes called \u00abAR marker\u00bb due to their popular use in Augmented Reality. Each marker is different has the location information encoded into it. For flying robots using this method of beacon-based localization, the robot has a camera that it uses to localize itself using these markers in the environment.</p>"},{"location":"unit3/lesson11/lesson11.html#motion-capture-systems","title":"Motion capture systems","text":"<p>You can also arrange it the other way with a marker on the robot and cameras in the environment in a motion capture system. With cameras around the edge of an environment (circle shape in Figure 3.6 and pyramid shapes in Figure 3.7), you can place markers on the robots moving in this environment. This method facilitates having many robots in an environment such as Figure 3.6, because cameras in the environment can detect each robot's unique marker. You can estimate the location of each robot via its unique marker because you can accurately set the position of each camera in the environment.</p> <p>See how in Figure 3.7 we can also use this technology in augmented reality. With sensors placed on a person moving in the environment, you can build structures of a person\u2019s motion, giving this method the name motion capture systems.</p> <p>Cameras for a motion capture system usually have the following properties:</p> <ul> <li>High resolution (from VGA up to 16 Mpixels)</li> <li>Very high frame rate (several hundreds of Hz)</li> <li>Good for ground truth reference and multi-robot control strategies</li> <li>Popular brands:</li> <li>VICON (10kCHF per camera),</li> <li>OptiTrack (2kCHF per camera)</li> </ul> <p>Example \"Making a TurtleBot motion capture system\"</p> <p>Think about the Turtlebot you will use in this module. You can set up a motion capture system with cameras in the corners of the room and a visible marker on the Turtlebot. You will have an accurate position for your TurtleBot in the environment at any point in time. This requires you to engineer the environment, and consider the location of the camera so that the robot is visible at all times. If the robot is not visible to the camera, then you will not be able to locate the robot.</p>"},{"location":"unit3/lesson11/lesson11.html#different-approaches-to-localization","title":"Different Approaches to Localization","text":"<p>The first approach to localization is probabilistic approach. </p>"},{"location":"unit3/lesson11/lesson11.html#probabilistic-approach-to-localization","title":"Probabilistic approach to localization","text":"<p>The best way to address the localization problem is by viewing these issues mathematically as a probabilistic inference problem. In other words, we represent the robot\u2019s pose in the environment as a probability distribution, instead of a single value. We then update this probability distribution according to how the robot moves and what the robot sensors perceive.</p> <p></p> <p>When you place a robot in an environment and switch it on, it will not know where it is. It needs some method or algorithms determine its location using the methods we discussed in the last lesson, i.e.:</p> <ul> <li>odometry or dead reckoning</li> <li>localization based on external sensors, beacons or landmarks</li> </ul> <p>In this video, we explains a third option combining both these methods in a new method of Probabilistic Map Based Localization to determine our robot\u2019s starting location.</p> <p>Link to the video</p> <p>The video is approximately 15-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#representing-uncertainty-using-probability-theory","title":"Representing uncertainty using probability theory","text":"<p>We use a probabilistic approach for mobile robot localization because measurement errors affect the data coming from a robot\u2019s sensors, meaning we can only compute the probability that the robot is in a given configuration.</p> <p>The key idea in probabilistic robotics is to represent uncertainty using probability theory: instead of giving a single best estimate of the current robot configuration, probabilistic robotics represents the robot configuration as a probability distribution over all possible robot poses.</p> <p>This probability distribution is called belief. It is represented by \\(bel\\).</p> <p>The probability distribution graph you saw in the video in Lesson step 5.1, and in Figure 5.2 below are a representation of this belief probability.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-probability-distributions","title":"Example probability distributions","text":"<p>Here we introduce you to four examples of probability distributions:</p> <ul> <li>Uniform</li> <li>Multimodal</li> <li>Dirac</li> <li>Gaussian</li> </ul> <p>In the following examples, we consider a robot that is constrained to move along a straight rail i.e. the problem is one-dimensional.</p> <p>The robot configuration is the position \\(x\\) along the rail shown in Figure 5.3.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-1-uniform-distribution","title":"Example 1: uniform distribution","text":"<p>We use this when the robot does not know where it is. It means that the information about the robot configuration is null. With uniform distribution, given a certain domain, the probability of every point is the same. Figure 5.4 below represents our one-dimensional environment of size \\(n\\).</p> <p>In this environment, with a uniform distribution, the height of the probability graph is</p> \\[ \\frac{1}{N} \\] <p>Because the sum of the area of the graph must add up to one.</p> <p></p> <p>Remember that in order to be a probability distribution a function \\(p(x)\\) must always satisfy the following constraint, i.e. equal one:</p> \\[ \\int_{-\\infty}^{+\\infty} p(x) d x=1 \\]"},{"location":"unit3/lesson11/lesson11.html#example-2-multimodal-distribution","title":"Example 2: Multimodal distribution","text":"<p>With multimodal distribution, there are certain points where we think the robot will be. In Figure 5.5 below, the robot is in the region around location \\(a\\) or \\(b\\).</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-3-dirac-distribution","title":"Example 3: Dirac distribution","text":"<p>Our next example is Dirac distribution. If our robot ever knew its location perfectly then we could put a probability of 1 to that position \\(a\\), where the robot currently is. i.e. the robot has a probability of 1.0 (i.e. 100%) to be at position \\(a\\).</p> <p>As shown in Figure 3.13, this would correspond to Dirac distribution. Note, this is represented as an upwards arrow in Figure 5.6 because it just goes infinitely high. The width of the arrow is an infinitely small point, with an infinitely high distribution at that point.</p> <p>The Dirac function \\(\\delta(x)\\) is defined as:</p> \\[ \\delta(x)=\\left\\{\\begin{array}{cc} \\infty &amp; \\text { if } x=0 \\\\ 0 &amp; \\text { otherwise } \\end{array} \\quad \\text { with } \\int_{-\\infty}^{+\\infty} \\delta(x) d x=1\\right. \\] <p>The Dirac function as infinity for \\(x\\) is equal to zero (the argument to the Dirac function is zero). It is zero at all other points on the distribution graph where the robot could be located. The Dirac function needs to integrate to one because it is still a probability distribution.</p> <p>We almost never have this with robot localization because we almost never perfectly know the robot's location.</p>"},{"location":"unit3/lesson11/lesson11.html#example-4-gaussian-distribution","title":"Example 4: Gaussian distribution","text":"<p>We frequently estimate robot poses in robotics using Gaussian distribution.  The Gaussian distribution has the shape of a bell and we define it using the following formula:</p> \\[ p(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} \\] <p>We usually have a mean \\(\u03bc\\). This is shown in Figure 5.7 as the point along the \\(x\\) axis with the peak of the bell curve. We have the standard deviation represented by sigma \\(\u03c3\\). The variance, square root of \\(\u03c3\\), is how wide the distribution is, shown by the horizontal arrow in Figure 5.7.</p> <p>The Gaussian distribution is also called normal distribution and is usually abbreviated with \\(N(\u03bc ,\u03c3)\\).</p> <p></p> <p>The Gaussian distribution is also called normal distribution and this explains why we use a capital \\(N\\) and abbreviate it to \\(N(\u03bc ,\u03c3)\\).</p>"},{"location":"unit3/lesson11/lesson11.html#example","title":"Example","text":"<p>For example, if you have \\(N(2 ,5)\\).</p> <p>This means that the mean is located at two along the \\(x\\) axis and the standard deviation, or width of the bell curve either side of this is five.</p>"},{"location":"unit3/lesson11/lesson11.html#solution-to-the-probabilistic-localization-problem-action-and-perception-updates","title":"Solution to the probabilistic localization problem: action and perception updates","text":"<p>In this video, we brings together all you have learnt so far about probabilistic localization methods to introduce two key steps in robot localization:</p> <ul> <li>the action (or prediction) update step</li> <li>the perception (or measurement) update step</li> </ul> <p>The video finishes with the Bayes filter localization algorithm that solves the probabilistic localization problem. This is a longer video, so please feel free to pause it to support your learning needs.</p> <p>Link to the video</p> <p>The video is approximately 17-minutes long. You may wish to pause it in the middle.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#review-summary-of-mathematics-presented-in-the-video","title":"Review \"Summary of mathematics presented in the video\"","text":"<p>Here, we summarize the algorithms for the two key steps in robot localization from the video, because you will need to remember these for the rest of Unit 1.</p> <p>In robot localization, we distinguish two update steps:</p> <p>1. Action (or prediction) update: the robot moves and estimates its position through its proprioceptive sensors.</p> <p></p> <p>During this step, the robot uncertainty grows.</p> <p>2. Perception (or measurement) update: the robot makes an observation using its exteroceptive sensors and corrects its position by combining its belief before the observation with the probability of making   exactly that observation.</p> <p>During this step, the robot uncertainty shrinks.</p> <p></p> <p>Solving the probabilistic localization problem consists in solving separately the Action and Perception updates.</p> <p>How do we solve the Action and Perception updates?</p> <p>Action update uses the Theorem of Total probability</p> \\[ p(x)=\\int_{y} p(x \\mid y) p(y) d y \\] <p>Perception update uses the Bayes rule</p> \\[ p(x \\mid y)=\\frac{p(y \\mid x) p(x)}{p(y)} \\] <p>(because of the use of the Bayes rule, probabilistic localization is also called Bayesian localization).</p> <p>Bayers filter localization algorithm</p> <p>for all \\(x_{t}\\) do</p> \\[     \\overline{\\operatorname{bel}}\\left(x_{t}\\right)=\\int p\\left(x_{t} \\mid u_{t}, x_{t-1}\\right) \\operatorname{bel}\\left(x_{t-1}\\right) d x_{t-1} \\] <p>(action, or prediction, update)</p> \\[     \\operatorname{bel}\\left(x_{t}\\right)=\\eta p\\left(z_{t} \\mid x_{t}, M\\right) \\overline{\\operatorname{bel}}\\left(x_{t}\\right) \\] <p>(perception, or measurement, update)</p> <p>end for</p> <p>return \\(\\operatorname{bel}\\left(x_{t}\\right)\\)</p> <p>In this final video in Lesson 3, we returns to examples he used at the beginning of the lesson to demonstrate how the action and perception updates work in a real world scenario.</p> <p>The video is approximately 4-minutes long.</p> <p>Download transcript (PDF).</p> <p>Link to the Video</p>"},{"location":"unit3/lesson11/lesson11.html#introduction-to-slam","title":"Introduction to SLAM","text":"<p>For the path planning, there are different methods, namely,  global and local methods of path planning for autonomous mobile robots. A mobile robot uses these methods to simultaneously localize and generate a map (SLAM). </p> <p>In most simplest of the cases, it is assumed that the map of the environment is already available to the robot. A robot tries to localize itself in respect to something, i.e. it uses the features of the map to identify its pose. At times, a map is not available and then, it is necessary for a mobile robot to generate its own map of the environment, and use this map during localization.</p> <p>Now, we consider how a robot can generate such a map. As a starting point, consider the option of you, as a robot operator, wanting to use a robot to clean your house. You could start by drawing a map by hand. You can measure the walls and objects, say, in your house, and build the map manually. You can give this map to your robot and it will use it to identify its pose. The problem with this solution is accuracy. How confident are you that you have hand drawn this map accurately enough for your robot to avoid obstacles?</p> <p>For accurate localization, we must very accurately measure and include the position of all landmarks (e.g., walls, artificial beacons, etc.) that a robot uses for localising in a map. Completing this map building task is often difficult, costly and time consuming, especially for large environments. For example, a robot working in a large museum with over forty rooms will need to map every individual room.</p> <p>Another consideration is a dynamic environment (i.e. the position of obstacles is changing). We will need to regularly update, re-measure and redraw a map of such an environment. Going back to the cleaner robot working in your house, you would need to re-draw the map every time you left something on the floor, didn't have time to tidy, or changed the layout of a room. This would probably be more work than doing the cleaning yourself!</p> <p>A better alternative is to get a robot to do the task of map building itself: we call this automatic map building.</p> <p>Next, in this video, we have explained how a robot can automatically create a map of an environment using sensors.</p> <p>Link to the Vodeo</p> <p>The video is approximately 7-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#occupancy-grid-mapping-algorithm","title":"Occupancy grid mapping algorithm","text":"<p>You are now ready to look at mathematical methods for automatic map building. One such method is Occupancy grid mapping algorithm. With the occupancy grid method, a robot represents a map of the environment as an evenly spaced field of cells. Each cell represents the presence or absence of an obstacle at that location in the environment.</p> <p>This method relies on two observations:</p> <ul> <li> <p>As the robot moves around an environment, it keeps a counter for every cell in the grid. You can move your robot in the environment for a long time, ideally visiting the same place over and over to account for noise. Then:</p> </li> <li> <p>if a laser beam returns a hit for a cell, then the algorithm increments the counter of that cell. For example, if all the cells start as zero and the robot moves around the environment, if the robot gets a hit, it changes to one. If the same cell gets a second hit, then the counter changes to two</p> </li> <li> <p>if a laser beam travels through a cell, then the algorithm decrements the counter for that cell</p> </li> <li> <p>When the robot has completed its survey around the environment, you can threshold the counter value in each cell, to determine whether a cell is occupied or empty. For example:</p> </li> <li> <p>if the counter value is greater than the threshold, particularly a cell with many hits, then you can make a reasonable assumption that it is an occupied cell</p> </li> <li>If the counter value is lower than the threshold, then you can assume it is likely to be a free cell</li> </ul>"},{"location":"unit3/lesson11/lesson11.html#the-threshold","title":"The threshold","text":"<p>The threshold is a parameter that you can adjust, based on how conservative you want to be about your robot bumping into an obstacle, or how unsure you are as to the accuracy of your map of the environment. A good threshold parameter has a value slightly lower than the initial counter value you start with.</p> <p>The threshold accounts for a tricky scenario that can happen: it is possible for a cell to still be at its initial counter value after a robot has completed its survey of the environment. In this scenario, because the robot did not travel through this cell, we do not know if the robot will 'hit' an obstacle when it later moves through this cell, or whether it is actually a free cell.</p> <p>One solution you might think will work, is to move the robot around the environment enough times to make sure every cell has either a hit or a through. In the real world, this is not possible for a number of reasons for this:</p> <ul> <li>there are always some corner cells that remain at their initial counter value</li> <li>the cell may be inside an obstacle, meaning you can never hit it, because you always hit the outer boundary of the obstacle</li> <li>or the cell may be in an area which your robot has not explored</li> </ul> <p>This means that the safest thing to do in this scenario is for you to mark as occupied, all cells still at their initial counter value at the end of the mapping.</p> <p>For example, if the initial counter value is zero, the free cell counter value is one, and the occupied counter value is two, then after using your threshold parameter, all the cells still showing as zero will be converted to two. As your robot continues to move around the environment, it will continually update these counters to increase the accuracy of the map. This method is also useful for a dynamic environment, so that, if an obstacle moves in an environment, the counter value for those cells will automatically start decreasing.</p> <p>Example \"Example: Your cleaning robot\"</p> <p>For example, if your cleaning robot is mapping your bedroom and you decide to move your bed from one corner into the middle of the wall. To start with, the counter had many hits in the corner of the room. Your robot rightly thinks there is an obstacle in the corner. Later, your robot gets many through in that same corner. It will gradually decrement the contours and, after a while, it realizes that the corner is now free for it to use.</p> <p>In this video, we first gives an example of a map built by a robot using its senses to complete automatic map building. Then he reviews the assumptions we made when we followed the process of occupancy grid mapping in the first video in this lesson.</p> <p>Link to the Video</p> <p>The video is approximately 4-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#14-simultaneous-localization-and-mapping-slam","title":"1.4 Simultaneous localization and mapping (SLAM)","text":"<p>Localization and Mapping are like chicken and egg. Both require each other to exist.</p> <ul> <li> <p>If you have a good map, then you can perform good localization using the methods we discussed in Unit 1.</p> </li> <li> <p>If you have good localization (perhaps through artificial markers you put on your robot and a special camera in the environment), then you can construct a good map.</p> </li> </ul> <p>If you do not have a good map, and you do not have artificial markers for your robot, what can you do?</p> <p>For example, you place a Turtlebot in a new room at the start of a session. You do not have an environment map and you do not accurately know the location for your robot, what can you do at this starting point when you do not have any information?</p> <p>The solution is to try building the map and localising the robot at the same time.</p> <p>There are algorithms that try to build the map and localize simultaneously. We call them simultaneous localization and mapping, or SLAM, algorithms.</p> <p>In this video, we explained the idea of SLAM before we discuss SLAM algorithms in the next step of this lesson.</p> <p>Link to the Video</p> <p>The video is approximately 10-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#slam-algorithms","title":"SLAM algorithms","text":"<p>In this lesson, we have introduced you to the high-level general idea of SLAM. There are different algorithms implementing this general SLAM idea.</p> <p>One algorithm is called Particle Filtering SLAM, which extends the Particle Filter Localization (Monte Carlo Localization) to SLAM.</p> <p>All these algorithms keep a belief (a probability distribution) about the robot pose and the structure of the map. This is different to what we learnt in Unit 1, where the belief was only about the robot\u2019s pose.</p> <p>For example, in Particle Filtering SLAM, each particle is not only a hypothesis for the pose for the robot, but is also a hypothesis about the structure of the map (e.g. occupancy of cells, or location of features).</p> <p>In this video, we showed you an example SLAM environment map created by Sebastian Thrun using Particle Filtering SLAM. Sebastian was the first director of the Google Car Project.</p> <p>Link to the Video</p> <p>The video is approximately 5-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#autonomous-car-technology","title":"Autonomous car technology","text":"<p>These SLAM algorithms are at the heart of today\u2019s autonomous car technologies.</p> <p>You can read more about driverless cars in these press releases (all open in a new browser tab):</p> <p>BBC: How driverless cars will change our world.</p> <p>The Guardian: Uber riders in Pittsburgh can hail self-driving Ford Fusions in test program.</p> <p>BBC: US releases highway code for robots.</p>"},{"location":"unit3/lesson11/lesson11.html#further-reading","title":"Further Reading","text":"<p>For a detailed understanding of classical robotics localization algorithms and how to approach this problem further, see these two books</p> <ul> <li> <ol> <li>Introduction to Autonomous Mobile Robots (Siegwart et. al, 2011)</li> </ol> </li> <li> <ol> <li>Introduction to AI Robotics, 1<sup>st</sup> and 2<sup>nd</sup> Edition, Robin R Murphy (Murphy, 2000, 2019)</li> </ol> </li> </ul>"},{"location":"unit3/lesson11/lesson11.html#lesson-complete","title":"Lesson complete","text":"<p>Having completed this lesson, you should be able to recall there are four main sources of uncertainty for a robot\u2019s location:</p> <ul> <li>initial pose uncertainty because it is not easy to know a robot's start pose in the environment</li> <li>effector noise also called motion noise, motion uncertainty and actuator uncertainty</li> <li>sensor noise the actual errors that your sensor makes</li> <li>sensor aliasing when your environment looks similar from different positions</li> <li>explain why it is important for a robot to have a map of its environment so that it can navigate and avoid obstacles</li> <li>describe fundamental methods and algorithms that we use for a robot to localize its pose in an environment</li> <li>discuss the benefits and challenges of methods for creating a map of an environment for a robot</li> <li>outline the key benefits of Simultaneous localization and mapping, or SLAM</li> <li>recall applications for the SLAM algorithm, including its use in autonomous car technology</li> </ul>"},{"location":"unit3/lesson8/lesson8.html","title":"Introduction to Bootstrapping","text":"<p>In this and subsequent units, we cover a set of RL algorithms that use bootstrapping, a powerful idea that allows us to create online updates that do not wait until the end of an episode to learn from the experience, live as it comes. We will continue on the tabular method, cover planning, and then move to function approximation methods. Along the way, we cover encoding techniques for state space traditionally used in RL, such as tile coding. On the function approximation, we will assume a linear model in this unit. We cover non-linear models from an application perspective in the subsequent unit. We are mainly concerned with regression not classification from a machine learning perceptive.</p> <p>The settings are still the same as that of an MDP. However, we assume that the state space is large and may not be practical to represent each state as an entry in a table. The states might also not manifest themselves clearly, and only we can obtain some observations about them. These observations result in a set of numerical, categorical or boolean features which we can then numerically deal with them as we did in earlier modules.</p> <p>Unit 3: Learning Outcomes By the end of this unit, you will be able to:  </p> <ol> <li>Assess the role of bootstrapping in RL and its impact on learning efficiency.  </li> <li>Explain n-step methods and the trade-offs associated with different values of n.  </li> <li>Compare n-step backup action-value-based control methods with direct policy estimation methods.  </li> <li>Evaluate how Temporal Difference (TD) methods obtain biased but low-variance estimates through environment interaction.  </li> <li>Analyze how actor-critic methods achieve biased but low-variance estimation through interaction with the environment.  </li> <li>Discuss the trade-offs between online and offline RL algorithms.  </li> <li>Design planning methods that incorporate model learning into RL.  </li> </ol>"},{"location":"unit3/lesson8/lesson8.html#lesson-7-tabular-methods-temporal-difference-learning","title":"Lesson 7-Tabular Methods: Temporal Difference Learning","text":"<p>Learning outcomes</p> <ol> <li>understand the idea of bootstrapping and how it is being used in TD</li> <li>understand the differences between MC and TD and appreciate their strengths and weaknesses</li> <li>understand how to use the ideas of TD to extend it to a control method such as Sarsa and Q-learning</li> </ol> <p>In this lesson, we cover the Temporal Difference learning method. TD is one of the fundamental ideas in RL. It uses bootstrapping to improve its predictions. The idea behind bootstrapping is to use (own estimation) to improve (own estimation) with an indication from the ground truth in the form of a reward. This sound surprising since we are not using a direct ground truth to revert to when we are improving the prediction. However, it turns out that there are theoretical guarantees that the method will converge to a solution that is usually close to optimal. The one constant stream of ground truth the agent keeps receiving is the rewards in each state. One of the major strengths of TD is that it can be used online without having to wait till the end of the episode as we did in the Monte Carlo methods. This also makes it extremely efficient and allows it to converge faster in practice *than MC. TD uses ideas similar to what we did in GPI: slightly improving the prediction and *not waiting until everything is clear (at the end of an episode). This idea is similar to what we did in stochastic mini-batch updates in ML. We will call it eagerness to learn. I.e., to grab whatever information is available and whenever it becomes available but at the same time keep accumulating a stock of this information to help us improve and sharpen our prediction. We will then move into designing control algorithms that depend on TD, we will tackle old and new algorithms, including Sarsa, Expected Sarsa, Q-learning and double Q-learning, and we will test them extensively using the infrastructure that we developed in the previous lesson. Finally, we conclude by studying a policy gradient algorithm for control, namely actor-critic, that depends on TD and REINFORCE.</p> <p>Plan As usual, in general there are two types of RL problems that we will attempt to design methods to deal with  1. Prediction problem For These problems we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p> <ol> <li>Control problems  For These problems we will design Value Iteration methods which utilise the idea of Generalised Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy then moving to a better and improved policy by choosing a greedy action often. We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</li> </ol> <p>Ok, so we start by implementing the TD algorithm. Due to the way we structured our code and classes, it is relatively simple and straightforward to define any online and offline methods. TD is an online method that will be called in each step during an episode. We, therefore, can turn off the storage because we do not need it, but leaving it will not hurt the grid problems we are tackling. It will consume some memory and a few extra milliseconds of processing. For more difficult problems, we need to utilise the memory to train anyway, as we shall see in the Application unit.</p> <p>We also would need to pass a learning step as we did for the MC algorithm. A learning step dictates how much error percentage will be considered when we update the value function. Sometimes we could go all the way \u03b1=1 when the algorithm is tabular, and the problem is simple. For most of the problems and algorithms we tackle, however, this is not desirable, and we set \u03b1=.1 or less to ensure the algorithm performs well on the common states and is acceptable on less common states. MC, however, is particularly sensitive towards this \u03b1, and we often would need to set it to smaller values such as .01.</p>"},{"location":"unit3/lesson8/lesson8.html#temporal-difference-td-learning-prediction","title":"Temporal-Difference (TD) Learning (prediction)","text":"<p>Eralier in a previous lesson, we saw how constant-\\(\\alpha\\) MC prediction method, have an update rule of the form:</p> \\[     V(S_t) \\leftarrow V(S_t) + \\alpha \\left( G(S_t) - V(S_t) \\right) \\] <p>The main idea of several well-known RL algorithms is to replace \\(G_t\\) with an estimation. Temporal-Difference (TD) learning is a key reinforcement learning method that updates value estimates based on bootstrapping, meaning it uses its own current estimates to update future predictions. The TD methods updates the state-value function \\(V(s)\\) using one-step lookahead, i.e it replaces \\(G_t\\) by \\(R_{t+1} + \\gamma V(S_{t+1})\\) in the constant-\\(\\alpha\\) MC update rule: </p> \\[     V(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right) \\] <p>Where:</p> <ul> <li>\\( \\alpha \\) is the step-size (learning rate),</li> <li>\\( r_{t+1} \\) is the reward received after taking action in \\( s_t \\),</li> <li>\\( \\gamma \\) is the discount factor,</li> <li>\\( V(s_{t+1}) \\) is the estimate of the next state's value.</li> </ul> <p>Unlike Monte Carlo (MC) methods, which require complete episodes (including constant \\(\\alpha\\) MC), TD does not require a complete episode, like constant \\(\\alpha\\) MC, TD methods update values incrementally after each time step, making them more efficient for continuous or long-horizon problems. The replacement of \\(G_t\\) sample with an estimate leads to infusing bias into TD, but the use of bootstrapping leads to a lower variance. TD tends to be faster than MC and more efficient, so the trad-off of the bias-variance is well worth it. Add to that its ability to truely incrmentally update the estimate as rewards are collected.</p> <p>Below we show the pseudocode for the TD algorithm.</p> <p>\\( \\begin{array}{ll} \\textbf{Algorithm: } \\text{TD(0) Prediction} \\\\ \\textbf{Input: } \\text{Policy } \\pi, \\text{ step-size } \\alpha, \\text{ discount factor } \\gamma \\\\ \\textbf{Initialize: }  V(s) \\leftarrow 0, \\forall s \\in S \\\\ \\textbf{Loop for each episode:} \\\\ \\quad \\text{Initialize } s \\\\ \\quad \\textbf{Loop for each step } t \\textbf{ until episode ends:} \\\\ \\quad \\quad \\text{Take action } a \\sim \\pi(s), \\text{ observe } r, s' \\\\ \\quad \\quad V(s) \\leftarrow V(s) + \\alpha \\left( r + \\gamma V(s') - V(s) \\right) \\\\ \\quad \\quad s \\leftarrow s' \\\\ \\textbf{Return: } V(s), \\forall s \\in S \\\\ \\end{array} \\)</p>"},{"location":"unit3/lesson8/lesson8.html#td-vs-monte-carlo-mc","title":"TD vs. Monte Carlo (MC)","text":"Feature Temporal-Difference (TD) Monte Carlo (MC) Update After each time step After full episode Exploration Requirement Can learn from incomplete episodes Requires complete episodes Variance Lower variance due to bootstrapping Higher variance since full returns are used Bias More biased as it relies on current estimates Less biased since it uses true returns Sample Efficiency More efficient, updates per time step Less efficient, updates once per episode Suitability Better for continuous/long tasks Works well for episodic tasks <p>TD methods blend bootstrapping (like Dynamic Programming) and sampling (like MC), making them a flexible and powerful approach for reinforcement learning.</p> <p>Below we provide you with the Python code to implement this algorithm.</p> <p><pre><code>class TD(MRP):\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        self.V[s] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.V[sn] - self.V[s])\n</code></pre> That it, this is all what you need to implement TD!. Note how close the implementation is to the update rule. Note also how we multiplied the value \\(V[s_{t+1}]\\) by (1- done). This is to ensure that when the episode is finished (ex., the agent is at goal or has achieved the task), we want only the final reward \\(r_{t+1}\\) to participate in the update and not \\(V[s_{t+1}]\\). This multiplication will appear in all of the updates we use. This saves us from having to treat the goal states in a special way on the environment level (ex. we could have set the value \\(V[s_{t+1}]\\)=0 by checking if \\(s_{t+1}==goal\\) or by checking done in the environment or by treating done inside the s_() function when we use function approximation in later lessons). We felt that this would disguise this information, and it is always better to be explicit when possible.</p> <p>Note also that we didn't use a and an in the online() function because we are making predictions in TD (no control yet). In addition, we do not store the experience for this one-step online algorithm while we had to for MC, which is again one of the advantages of online methods.</p> <p>Let us test our brand new TD algorithm on the random walk prediction problem. Note that randwalk is the default environment for MRP anyway and hence no need to pass it.</p> <p><pre><code>TDwalk = TD(episodes=100, v0=.5, **demoV())\nTDwalk.interact(label='TD learning')\n</code></pre> </p> <p>Note that we did not need to store the episodes trajectories in a pure online method, hence these methods are usually more memory efficient that there offline counterpart! Note how TD performed far better and converged faster in fewer episodes than MC</p>"},{"location":"unit3/lesson8/lesson8.html#offline-td","title":"Offline TD","text":"<p>In this section, we develop an offline TD algorithm. This is not a common algorithm as it usually defies the reason for using TD. That is, we usually use TD because it is an online algorithm. Nevertheless, studying this algorithm allows us to appreciate the strengths and weaknesses of TD and to compare its performance with other offline algorithms, such as MC.</p> \\[ \\begin{array}{ll} \\textbf{Algorithm: }  \\text{Offline Temporal-Difference Policy Evaluation} \\\\ \\textbf{Input: } \\text{Episodes generated under policy } \\pi \\\\ \\textbf{Initialize: } V(S) \\leftarrow 0, \\forall S \\in \\mathcal{S}, \\alpha &gt; 0 \\\\ \\textbf{Repeat until convergence: } &amp; \\\\ \\quad \\text{For each episode: } &amp; \\\\ \\quad \\quad \\textbf{For each step } t \\textbf{ from } 0 \\textbf{ to } T-1: &amp; \\\\ \\quad \\quad \\quad \\delta_t \\leftarrow R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) &amp; \\\\ \\quad \\quad \\quad \\text{Store } (S_t, \\delta_t) \\text{ for batch update} &amp; \\\\ \\quad \\text{End episode loop} &amp; \\\\ \\quad \\textbf{For each state } S_t \\textbf{ in batch:} &amp; \\\\ \\quad \\quad V(S_t) \\leftarrow V(S_t) + \\alpha \\sum \\delta_t \\text{ (update using accumulated } \\delta_t \\text{)} &amp; \\\\ \\textbf{Return: } V(S), \\forall S \\in \\mathcal{S} \\\\ \\end{array} \\] <p>Below we provide you with the Python implementation of the offline TD.</p> <p><pre><code>class TDf(MRP):\n    def init(self):\n        self.store = True\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        for t in range(self.t+1):\n            s = self.s[t]\n            sn = self.s[t+1]\n            rn = self.r[t+1]\n            done = self.done[t+1]\n\n            self.V[s] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.V[sn]- self.V[s])\n</code></pre> Note that we can do it the changes backwards, you can try both and see the difference.</p> <p><pre><code>TDwalk = TDf(\u03b1=.05, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> </p> <p>Note how we overrode the offline function in our MRP class that we covered in the previous lesson. The first three lines inside the for loop are to make the update format of the online and offline identical. We could have also made the algorithm go backwards, similar to MC. Each has its advantage and disadvantage, although for TD since it uses the temporal difference error, it usually makes little difference. You can uncomment the backward loop and try it yourself.</p>"},{"location":"unit3/lesson8/lesson8.html#conducting-trialsseveral-runs-of-experiments","title":"Conducting trials(several runs) of experiments","text":"<p>Let us now use a useful handy class called 'Runs' that summarises several runs for us to reach a reliable and unbiased conclusions when we compare algorithms performances.</p> <p>Note that the class allows us to run several experiments efficiently. The main assumption is that the algorithms are inherited from an MRP class which applies for the majority of the classes that we will deal with in our units.</p> <p>Let us now see how we can use this new class to easily run experiments to study how an algorithm behaves. Below we show a function that compares TD with MC on different learning rates. You can read about this comparison and the associated figure in Example 6.2 of the book (hence the function's name). We will follow this trend of naming functions after their counterpart examples or figures in the book.</p> <p><pre><code>def TD_MC_randwalk(env=randwalk(), alg1=TDf, alg2=MC):\n    plt.xlim(0, 100)\n    plt.ylim(0, .25)\n    plt.title('Empirical RMS error, averaged over states')\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=alg1(env=env, \u03b1=\u03b1, v0=.5), runs=100, plotE=True).interact(label='TD \u03b1= %.2f'%\u03b1, frmt='-')\n\n    for \u03b1 in [.01, .02, .03, .04]:\n        MCs = Runs(algorithm=alg2(env=env, \u03b1=\u03b1, v0=.5), runs=100, plotE=True).interact(label='MC \u03b1= %.2f'%\u03b1, frmt='--')\n\ndef example_6_2(**kw): return TD_MC_randwalk(**kw)\n\nexample_6_2()\n</code></pre> </p> <p>We have already imported MC to compare its performance with our newly defined offline TD. Remember that MC is also offline algorithm.</p>"},{"location":"unit3/lesson8/lesson8.html#optimality-of-td","title":"Optimality of TD","text":"<p>In this section, we study the optimality of TD. We develop two algorithms, Batch TD and Batch MC. Both of these algorithms operate in a supervised learning fashion. We collect a set of episodes and then deal with them as mini-batches, and then we run a set of epochs that repeatedly present the so-far experience until the algorithm converges. We use TD and MC updates inside the algorithm to see which value each converges to. By doing so, we have levelled up the strength of both algorithms (both are offline and wait until the end of each episode to accommodate all past experiences after each episode), and we laid their performance on pure convergence terms.</p> <pre><code>class MRP_batch(MRP):\n    def __init__(self, **kw):\n        super().__init__(**kw)\n        self.store = True # store the full experience\n    # we will redfine the allocate to store the full experience instead of only latest episode\n    def allocate(self): \n        self.r = np.zeros((self.max_t, self.episodes))\n        self.s = np.ones ((self.max_t, self.episodes), dtype=np.uint32) *(self.env.nS+10)  \n        self.a = np.zeros((self.max_t, self.episodes), dtype=np.uint32)  # actions and states are indices        \n        self.done = np.zeros((self.max_t, self.episodes), dtype=bool) \n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        # store one trajectory(sarsa) in the rigth episode buffer\n        if s  is not None: self.s[t, self.ep] = s\n        if a  is not None: self.a[t, self.ep] = a\n        if rn is not None: self.r[t+1, self.ep] = rn\n        if sn is not None: self.s[t+1, self.ep] = sn\n        if an is not None: self.a[t+1, self.ep] = an\n        if done is not None: self.done[t+1, self.ep] = done\n    # returns the agent's trace from latest episode buffer\n    def trace(self):\n            return self.s[:self.t+1, self.ep]\n</code></pre> <p>Below we inherit the above class to allow us to conduct batch TD learning. This form of learning is usually not practical, but it is listed here for studying the behaviour of TD to gain insight into what kind of target it has and compare it with MC. The point is to prove that TD, in practice, indeed has a different goal than MC and is more efficient in converging to this target, which in turn, usually reduces the error more effectively than MC does.</p> <pre><code>class TD_batch(MRP_batch):\n    def __init__(self, \u03b1=.001, **kw):\n        super().__init__(\u03b1=\u03b1, **kw)\n    # ------------------------\ud83c\udf18 offline learning----------------------- \n    def offline(self):\n        # epochs\n        while True:\n            \u0394V = self.V*0\n            # each episode acts like a mini-batch in supervised learning\n            for ep in range(self.ep+1): \n                for t in range(self.Ts[ep]):#-1, -1, -1):\n                    s  = self.s[t, ep]\n                    sn = self.s[t+1, ep]\n                    rn = self.r[t+1, ep]\n                    done = self.done[t+1, ep]\n\n                    \u0394V[s] += rn + (1- done)*self.\u03b3*self.V[sn]- self.V[s]\n            \u0394V *= self.\u03b1\n            # exit the epochs loop if there is no more meaningful changes (method converged)\n            if np.abs(\u0394V).sum() &lt; 1e-3:  break #; print('exit')\n            self.V += \u0394V\n</code></pre> <p><pre><code>TDwalk_batch = TD_batch(episodes=100, v0=-1, **demoV()).interact()\n</code></pre> </p> <p>Note how the batch updates have much smoother and faster convergence per-episodes than a usual TD or MC. However, they have a much higher computational cost that makes them not suitable for practical problem.</p> <pre><code>class MC_batch(MRP_batch):\n    def __init__(self, \u03b1=.001, **kw):\n        super().__init__(\u03b1=\u03b1,**kw)\n\n    # -----------------------------------\ud83c\udf18 offline learning------------------------------------- \n    def offline(self):\n        # epochs\n        while True:\n            \u0394V = self.V*0\n            # each episode acts like a mini-batch in supervised learning\n            for ep in range(self.ep+1):\n                Gt = 0\n                for t in range(self.Ts[ep]-1, -1, -1):\n                    s  = self.s[t, ep]\n                    rn = self.r[t+1, ep]\n\n                    Gt = rn + self.\u03b3*Gt \n                    \u0394V[s] += Gt - self.V[s]\n\n            \u0394V *= self.\u03b1\n            # exit the epochs loop if there is no more meaningful changes (method converged)\n            if np.abs(\u0394V).sum() &lt; 1e-3: break #;print('exit')\n            self.V += \u0394V\n</code></pre> <pre><code>MCwalk_batch = MC_batch(episodes=100, v0=-1, **demoV()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#batch-runs","title":"Batch runs","text":"<p>Now it is time to run experiments to specify which algorithm is better. We follow the experiments conducted in figure 6.2 in the book. Note that we initialise to -1 this time to smoothen the resultant figure and remove any advantages the algorithms had when starting from .5 probabilities. This means that the algorithm would have to guess all the way from -1 to the probability of starting in a state s and ending up in the right terminal state. </p> <p>We start with 10 runs to show the full range that the algorithm will take in the early episodes, and then in the definition of figure_6_2( ), we restrict the figure's limit to show the interesting trend of each algorithm. Note that the algorithms could have been made more efficient by some further optimization which we left out for pedagogical reasons.</p> <p><pre><code>\u03b1=.001\nTDB = Runs(algorithm=TD_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=3, plotE=True).interact(label= 'Batch TD, \u03b1= %.3f'%\u03b1)\nMCB = Runs(algorithm=MC_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=3, plotE=True).interact(label='Batch MC, \u03b1= %.3f'%\u03b1)\n</code></pre> </p> <pre><code>def figure_6_2():\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)\n    plt.xlim(0,100)\n    plt.ylim(0, .25)\n    plt.title('Batch Training')\n\n    \u03b1=.001\n    TDB = Runs(algorithm=TD_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=100, plotE=True).interact(label= 'Batch TD, \u03b1= %.3f'%\u03b1)\n    MCB = Runs(algorithm=MC_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=100, plotE=True).interact(label='Batch MC, \u03b1= %.3f'%\u03b1)\n</code></pre> <pre><code>figure_6_2()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-on-policy-control-using-td-update-for-control","title":"Sarsa on-policy control (using TD Update for Control)","text":"<p>In this section, we deal with TD updates to achieve control.  Using the previously shown TD algorithm directly is not suitable for control, we must adapt it so that it changes the Q tabel not the V table. We cover mainly two algorithms one is Sarsa which is an on-policy control algorithm (meaning the followed policy is the same as the policy we are learning about). The second main algorithm is the famous Q-learning algorithm which is an off-policy algorithm. In the case of Q-learning, the agent is acting according to an \u03b5-greedy algorithm while it is learning about a greedy algorithm.</p> <p>Similar to what we did earlier we will use the two dictionaries demoQ and demoR to make the calls more concise.</p> <pre><code>class Sarsa(MDP()):\n\n    def init(self): #\u03b1=.8\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn,an] - self.Q[s,a])\n</code></pre> <p>Note that we do not store the experience for this one-step online algorithm while we had to for MC, and this is again one of the advantages of online methods.</p> <p>Let us now apply the Sarsa on a simple grid world environment. The goal is directly facing the start position. However, to make the problem more difficult for the algorithm we have deprioritised the right action and we place the order of the actions as follows: left, right, down and up. This simple change made the agent pick going left before going right and made the problem only a bit more difficult. Let us see how the Sarsa performs on it.</p> <p><pre><code>sarsa = Sarsa(env=grid(), \u03b1=.8, episodes=50, seed=10, **demoQ()).interact()\n</code></pre> </p> <p><pre><code>mc = MCC(env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, **demoQ()).interact()\n</code></pre> </p> <p>Note how Sarsa performed better and converged faster in fewer episodes than MCC although it did cover the full environment.</p> <pre><code>sarsa = Sarsa(env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, plotT=True).interact(label='Sarsa')\nmcc   = MCC  (env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, plotT=True).interact(label='MCControl')\n</code></pre> <p></p> <p>Of course we change the seed the performance will change for both. Also if we change the learning rate \u03b1 the performance will vary (change the seed to 0 and run). This is why it is important to conduct several runs in order to obtain the performance of the algorithms on average.</p> <p><pre><code>sarsa_large = Sarsa(env=maze_large(), \u03b1=.1, episodes=500, seed=0 , **demoQ()).interact()\n</code></pre> </p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-on-windy-environment","title":"Sarsa on windy environment","text":"<p>In this section we show how Sarsa behaves on the windy environment that we have shown in lesson 2. The idea to show that TD is able of learning to deal with the upward wind in a manner that allows it to reach the goal effectively. This study can be seen in Example 6.5 in the book.</p> <pre><code>def Sarsa_windy():\n    return Sarsa(env=windy(reward='reward1'), \u03b1=.5, seed=1, **demoQ(), episodes=170).interact(label='TD on Windy')\n\nexample_6_5 = Sarsa_windy\n\ntrainedV = example_6_5()\n\nplt.subplot(133).plot(trainedV.Ts.cumsum(), range(trainedV.episodes),'-r')\nplt.show()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#q-learning-off-policy-control","title":"Q-learning off-policy control","text":"<p>Now we move to the Q-learning algorithm. Q-learning is one of the most successful algorithms in RL. Although it is an off-policy (not offline) algorithm, it usually performs better than the Sarsa. Q-learning also allowed for a control algorithm's first proof of convergence due to its simple update rules. </p> <p>Important Note that Q-learning does not require changing the step function because it does not require knowing the next action in advance (unlike Sarsa). Hence it uses a simple algorithmic schema that is almost identical to TD.</p> <pre><code>class Qlearn(MDP()):\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n</code></pre> <p>As you can see, we did not use the action an in Qlearning() because we take the max of the action and assume that it is the one that the agent will pick (although this might not be the case, and hence it is an off-policy learning algorithm because we are learning about a fully greedy policy while the agent is acting according to an \u03b5greedy policy). Also note that we do not store the experience for this one-step online algorithm while we had to for MC, which is again one of the advantages of online methods.</p> <p><pre><code>qlearn = Qlearn(env=grid(), \u03b3=1, \u03b1=.8, episodes=40, seed=10, **demoQ()).interact()\n</code></pre> </p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-and-q-learning-on-a-cliff-edge","title":"Sarsa and Q-Learning on a Cliff Edge!","text":"<p>This section compares the performance of on-policy Sarsa and off-policy Q-learning algorithms to show how each act on a specific problem. The problem that we will tackle is a cliff-edge world. This is a grid world of 12x4, with a goal location on the far-right bottom corner and the start location on the far-left bottom corner. There are no obstacles. However, there is a cliff between the start and the goal locations on the bottom. If the agent trespasses on it, it falls off the cliff, receives a penalty of -100 and will be relocated back to the start location without starting a new episode. The agent receives a reward of -1 everywhere, including the goal location. We will use the sum of rewards metric to measure the performance of algorithms on this problem.</p> <pre><code>sarsa = Sarsa(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p> <pre><code>sarsa = Qlearn(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p> <pre><code>def Sarsa_Qlearn_cliffwalk(runs=200, \u03b1=.5, env=cliffwalk(), alg1=Sarsa, alg2=Qlearn):\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)    \n    plt.yticks([-100, -75, -50, -25])\n    plt.ylim(-100, -10)\n\n\n    SarsaCliff = Runs(algorithm=alg1(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Sarsa')\n    QlearnCliff = Runs(algorithm=alg2(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Q-learning')\n    return SarsaCliff, QlearnCliff\n\ndef example_6_6(**kw): return Sarsa_Qlearn_cliffwalk(**kw)\n</code></pre> <p><pre><code>SarsaCliff, QlearnCliff = Sarsa_Qlearn_cliffwalk()\n</code></pre> </p>"},{"location":"unit3/lesson8/lesson8.html#expected-sarsa","title":"Expected Sarsa","text":"<p>In this section, we cover the expected Sarsa algorithm. This algorithm is very similar to the Q-learning algorithm and has the same schematic structure (unlike Sarsa, it does not require obtaining the next action in advance). It takes all the probabilities of the different actions and forms an expectation of the next action.</p> <pre><code>class XSarsa(MDP()):\n\n    # ------------------------------------- \ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):      \n        # obtain the \u03b5-greedy policy probabilities, then obtain the expecation via a dot product for efficiency\n        \u03c0 = self.\u03c0(sn)\n        v = self.Q[sn].dot(\u03c0)\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*v - self.Q[s,a])\n</code></pre> <p>Note that the policy is assumed to be \u03b5-greedy, if you want to deal with other policies then a different implementation is required</p> <pre><code>xsarsa = XSarsa(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#double-q-learning","title":"Double Q-learning","text":"<pre><code>class DQlearn(MDP()):\n\n    def init(self):\n        self.Q1 = self.Q\n        self.Q2 = self.Q.copy()\n\n    # we need to override the way we calculate the aciton-value function in our \u03b5greedy policy\n    def Q_(self, s=None, a=None):\n            return self.Q1[s] + self.Q2[s] if s is not None else self.Q1 + self.Q2\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------\n    def online(self, s, rn,sn, done, a,_): \n        p = np.random.binomial(1, p=0.5)\n        if p:    self.Q1[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q2[sn].max() - self.Q1[s,a])\n        else:    self.Q2[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q1[sn].max() - self.Q2[s,a])\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#comparing-sarsa-expected-sarsa-q-learning-and-double-q-learning","title":"Comparing Sarsa, Expected Sarsa, Q-learning and Double Q-learning","text":"<p>Ok now we can compare all 4 algorithms on the different environments to see their performances. </p>"},{"location":"unit3/lesson8/lesson8.html#comparison-on-cliff-walking","title":"Comparison on cliff walking","text":"<pre><code>def XSarsaDQlearnCliff(runs=300, \u03b1=.5):\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)    \n    plt.yticks([-100, -75, -50, -25])\n    plt.ylim(-100, -10)\n    env = cliffwalk()\n\n    XSarsaCliff = Runs(algorithm=XSarsa(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='XSarsa')\n    DQlearnCliff = Runs(algorithm=DQlearn(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Double Q-learning')\n\n    return XSarsaCliff, DQlearnCliff\n</code></pre> <pre><code>SarsaCliff.plot(label='Sarsa', frmt='-')\nQlearnCliff.plot(label='Q-learning', frmt='-')\nXSarsaCliff, DQlearnCliff = XSarsaDQlearnCliff()\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#comparison-on-the-maze","title":"Comparison on the Maze","text":"<pre><code>def compareonMaze(runs=100, \u03b1=.5):\n\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)\n\n    env=Grid(gridsize=[10,20], style='maze', s0=80, reward='reward1') # this is bit bigger than the defualt maze\n    env.render()\n\n    SarsaMaze = Runs(algorithm=Sarsa(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Sarsa')\n    XSarsaMaze = Runs(algorithm=XSarsa(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='XSarsa')\n\n    QlearnMaze = Runs(algorithm=Qlearn(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Q-learning')\n    DQlearnMaze = Runs(algorithm=DQlearn(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Double Q-learning')\n\n    return SarsaMaze, XSarsaMaze, QlearnMaze, DQlearnMaze\n</code></pre> <p><pre><code>SarsaMaze, XSarsaMaze, QlearnMaze, DQlearnMaze = compareonMaze(\u03b1=.5)\n</code></pre> </p>"},{"location":"unit3/lesson8/lesson8.html#actor-critic-td-for-policy-gradient-methods","title":"Actor-Critic: TD for Policy Gradient Methods","text":"<p>Earlier, we saw how REINFORCE could perform well in the grid environment. REINFORCE is a policy gradient method that attempts to directly estimate a policy instead of estimating an action-value function. This is done by using the value function as an objective function that we would want to maximise (instead of minimising an error function as in Sarsa or Q-learning).</p> <p>Like Monte Carlo, REINFORCE is an offline method that needs to wait until the end of an episode to estimate the value function. The question, then, is there an algorithm similar to REINFORCE but online? The method should be derived similarly to Sarsa and Q-learning, which depends on the next step estimate of the value function. The answer is yes, and the method is called Actor-critic, which does that exactly. The algorithm general unified update attempts to estimate its policy by directly maximising the returns with respect to a baseline (see section 13.4). When the algorithm replaces its returns with an estimate of the returns (section 13.5, the difference between the return estimate and the baseline becomes a TD error), the algorithm can be thought of as having two distinctive parts an actor and a critic. The actor maximises its start-state-value function, while the critic attempts to improve its estimates of the state-value function for all states. Both of them use the Temporal Difference (TD) error to improve their estimates, meaning they can work online. Like REINFORCE, the actor-critic uses a SoftMax policy to select an action according to the actor policy parameters. So, to maximise the value, the actor takes the derivative of the \\(\\nabla \\log v(S_0)\\). </p> <p>Actor-critic is one of the oldest RL algorithms, and it avoids several issues that arise from the use of \\(\\epsilon\\)-greedy policy. The most obvious one is that the policy changes the probability of selecting an action gradually and continuously when the parameters change, unlike \\(\\epsilon\\)-greedy, which can change the maximum value action abruptly due to a small change in the parameters. This also allows it to provide better convergence guarantees.</p> <pre><code>class Actor_Critic(PG()):\n\n    def step0(self):\n        self.\u03b3t = 1 # powers of \u03b3, must be reset at the start of each episode\n\n    def online(self, s, rn,sn, done, a,an): \n        \u03c0, \u03b3, \u03b3t, \u03b1, \u03c4, t = self.\u03c0, self.\u03b3, self.\u03b3t, self.\u03b1, self.\u03c4, self.t\n        \u03b4 = (1- done)*\u03b3*self.V[sn] + rn - self.V[s]  # TD error is based on the critic estimate\n\n        self.V[s]   += \u03b1*\u03b4                          # critic\n        self.Q[s,a] += \u03b1*\u03b4*(1- \u03c0(s,a))*\u03b3t/\u03c4         # actor\n        self.\u03b3t *= \u03b3\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#delayed-reward","title":"Delayed Reward","text":"<p>First let us establish the baseline performance.</p> <p><pre><code>ac = Actor_Critic(env=grid(), \u03b1=1, \u03c4=.3, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> </p> <p>Note that we set \u03b1=1 which is unusual for an RL algorithm and the method just worked. This is a testimony to the resilience and strength of actor-critic methods. Note how reducing the exploration factor \\(\\tau=.3\\) led to a much faster convergence.</p> <p>Note how we had to increase the number of episodes to converge when we set \\(\\alpha=.1\\) instead of \\(\\alpha=1\\).</p> <p><pre><code>ac = Actor_Critic(env=grid(), \u03b1=.1, \u03c4=.1, \u03b3=1, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> </p> <p>Note how reducing both \\(\\tau\\) and \\(\\alpha\\) helped reach convergence quickly but with a better exploration.</p>"},{"location":"unit3/lesson8/lesson8.html#intermediate-reward","title":"Intermediate Reward","text":"<p><pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=.7, \u03c4=1, \u03b3=.98, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> </p> <p><pre><code>ac = Actor_Critic(env=maze(reward='reward0'), \u03b1=.1, \u03c4=1,  \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> </p> <pre><code>ac_large = Actor_Critic(env=maze_large(), \u03b1=.1, \u03c4=.3, \u03b3=1, episodes=500, seed=0 , **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#model-selection-methods-comparisons-class","title":"Model selection: methods comparisons class","text":"<p>Ok, the question is, which one of these algorithms would perform best regardless of the learning rate \u03b1? To be able to know, we would need to compare the performances on a set of \u03b1 values to see the full picture. To that end, we developed a useful comparison class. It allows us to compare algorithms with different hyperparameters similar to what we did in other machine learning modules. All that is required is to specify which hyperparameter we want to vary and then pass the values we want to test for in a dictionary.</p> <p>We can compare different \u03b1 values to specify which algorithm is dominant. This study can be seen in Figure 6.3 in the book. Here we do 10 runs because it takes longer to do more, but you are welcome to try to run it for 100 runs. Note that the asymptotic study will run for 1000. the idea here is to compare the performances of the above control algorithms and variants of Q-learning and Sarsa in a systematic manner. The domain is the cliff walking environment. We want to see which algorithms (Sarsa, expected Sarsa, Q-learning, double Q-learning) perform best regardless of the learning rate. Such comparison would give us a definitive answer on which algorithm is best for the given problem when we see a pattern of dominance for all learning rate values.</p> <pre><code>def figure_6_3(runs=10, Interim=True, Asymptotic=True, episodes=100,  label=''): #100\n    #plt.ylim(-150, -10)\n    plt.xlim(.1,1)\n    plt.title('Interim and Asymptotic performance')\n    \u03b1s = np.arange(.1,1.05,.05)\n\n\n    algors = [ XSarsa,   Sarsa,   Qlearn]#,      DQlearn]\n    labels = ['XSarsa', 'Sarsa', 'Qlearning']#, 'Double Q learning']\n    frmts  = ['x',      '^',     's']#,         'd']\n\n    env = cliffwalk()\n    Interim_, Asymptotic_ = [], []\n    # Interim perfromance......\n    if Interim:\n        for g, algo in enumerate(algors):\n            compare = Compare(algorithm=algo(env=env, episodes=episodes), runs=runs, hyper={'\u03b1':\u03b1s},\n                             plotR=True).compare(label=labels[g]+' Interim'+label, frmt=frmts[g]+'--')\n            Interim_.append(compare)\n\n    # Asymptotic perfromance......\n    if Asymptotic:\n        for g, algo in enumerate(algors):\n            compare = Compare(algorithm=algo(env=env, episodes=episodes*10), runs=runs, hyper={'\u03b1':\u03b1s}, \n                             plotR=True).compare(label=labels[g]+' Asymptotic'+label, frmt=frmts[g]+'-')\n            Asymptotic_.append(compare)\n\n    plt.gcf().set_size_inches(10, 7)\n    return Interim_, Asymptotic_\n\nInterim_, Asymptotic_ = figure_6_3()\n</code></pre> <p></p> <p>As we can see the expected Sarsa performed best in the interim and on the asymptote.</p>"},{"location":"unit3/lesson8/lesson8.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we have further developed our understanding of important and prominent RL online algorithms that are widely used, all based on the value iteration idea. I.e., we keep improving our policy and refining our value-function iteratively in each step until convergence. All of our algorithms are based on the Temporal Difference method. TD uses bootstrapping in its update; instead of using a true return of a state, it uses the current reward + its own estimation of the return for the next state. It is quite surprising to see how well TD works in practice. TD has been proven to converge to a good solution under some basic conditions regarding the learning rate. In practice, however, we assign a fixed small learning rate that works just fine. It is desirable that the learning rate is not decayed when the environment\u2019s dynamics are expected to change. We have further used TD update in a few control algorithms. Most notable are the Sarsa and Q-learning. The first is an on-policy, while the latter is an off-policy control algorithm. We have compared all algorithms on different problems, studied their strengths and weaknesses, and how they are expected to behave on a certain problem.</p> <p>Further Reading: For further reading you refer chapter 6 from the Sutton and Barto book. There are more rigorous books that take special care for the mathematics guarantees behind the ideas of RL, such as Neuro-Dynamic Programming.</p>"},{"location":"unit3/lesson8/lesson8.html#your-turn","title":"Your turn","text":"<p>Now it is time to experiment further and interact with code in worksheet8.</p>"},{"location":"unit3/lesson9/lesson9.html","title":"9. n-Step Methods","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit3/lesson9/lesson9.html#lesson-8-tabular-methods-n-steps-bootstrapping-tabular-methods","title":"Lesson 8-Tabular Methods: n-steps Bootstrapping Tabular Methods","text":"<p>Learning outcomes 1. understand how to generalize a one-step methods to n-step methods 2. understand the trend associated with n-steps methods  3. understand that intermediate n values usually works the best  4. generalise n-step prediction methods to n-step control methods</p> <p>In the previous lesson, we saw how a TD update rule defined in terms of the next reward and state as the target can effectively converge to useful state and action-value functions. It took the form:</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t}) ]\\)</p> <p>This is a one-step update because one reward available in the current step is used. It can be written as</p> <p>\\(G_{t} = R_{t+1} + \\gamma V(S_{t+1})\\)</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[ G_{t} - V(S_{t}) ]\\)</p> <p>where \\(G_{t}\\) is a one-step return.</p> <p>In this lesson, we study the effect of increasing the number of steps considered for the target. In particular, we study the effect of collecting rewards for n steps and substituting the one-step return with the n-step return written as the discounted sum of the n rewards plus the value function for the n-th step state.</p> <p>In other words, we define the n-step return as</p> <p>\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} +...+ \\gamma^{n-1} R_{t+n} + \\gamma^{n}V(S_{t+n})\\)</p> <p>And the update rule that uses this n-step return is:</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[G_{t:t+n} - V(S_{t}) ]\\)</p> <p>When we want to make in which time step, we are conducting this update, we add a subscript for the V function that represents the time step at which the estimate is referring to </p> <p>\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} +...+ \\gamma^{n-1} R_{t+n} + \\gamma^{n}V_{t+n-1}(S_{t+n})\\)</p> <p>\\(V_{t+n}(S_{t}) = V_{t+n-1}(S_{t}) + \\alpha[G_{t:t+n} - V_{t+n-1}(S_{t}) ]\\)</p>"},{"location":"unit3/lesson9/lesson9.html#mrp-for-multi-steps","title":"MRP for multi-steps","text":"<p>Our MRP class already accommodate waiting for n-1 steps before obtaining the \\(G_{t:t+n}\\). In each step, it update G to obtain the \\(G_{t:t+n}\\). Its stopping criteria waits for extra n-1 steps at the end to ensure we update the latest n-1 state values since we are always lagging n-1 steps during the episode.</p>"},{"location":"unit3/lesson9/lesson9.html#online-tdn","title":"Online TDn","text":"<p>Now we write our TDn class. It is very similar to TD except that we replace the rn+V[t+1] by the \\(G_{t:t+n}\\). Also as long as the agent did do enough steps (n-1) we skip without doing the update.</p> <pre><code>class TDn(MRP):\n\n    def init(self):\n        self.store = True \n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n = self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c4n = \u03c4+n ; \u03c4n = min(\u03c4n, self.t+1 - self.skipstep)\n        \u03c41 = \u03c4+1\n\n        s\u03c4 = self.s[\u03c4 ]\n        sn = self.s[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.V[s\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1- done)*self.\u03b3**n *self.V[sn] - self.V[s\u03c4])\n</code></pre> <p>Let us now apply TDn on our simple 5-steps random walk problem.</p> <p><pre><code>TDnwalk = TDn(env=randwalk(), v0=0, \u03b1=.05, n=4, episodes=100, seed=0, **demoV()).interact()\n</code></pre> </p> <p><pre><code>TDnwalk = TDn(env=randwalk(), v0=0, \u03b1=.05, n=1, episodes=100, seed=0, **demoV()).interact()\n</code></pre> </p>"},{"location":"unit3/lesson9/lesson9.html#tdn-and-mc-runs-on-random-walk","title":"TDn and MC Runs on random walk","text":"<p>Let us now  see how a TDn for n=1 and n=5 as well as MC behaves on our usual 5-states random walk on average. To that end as usual we execute several runs.</p> <p>from MC import MC</p> <pre><code>def nstepTD_MC_randwalk(env=randwalk(), algorithm=TDn, alglabel='TD'):\n    plt.xlim(0, 100)\n    plt.ylim(0, .25)\n    plt.title('Empirical RMS error, averaged over states')\n    n=5\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=algorithm(env=env, n=1,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label='%s \u03b1= %.2f'%(alglabel,\u03b1), frmt='.-')\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=algorithm(env=env,n=n,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label= '%s \u03b1= %.2f n=%d'%(alglabel,\u03b1,n), frmt='-')\n\n    for \u03b1 in [.01, .02, .03, .04]:\n        MCs = Runs(algorithm=MC(env=env,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label='MC \u03b1= %.2f'%\u03b1, frmt='--')\n</code></pre> <p>Note that 5 states random walk environment env=randwalk() is the default environment for MRPs so we did not need to explicitly pass it in the above.</p> <p><pre><code>nstepTD_MC_randwalk()\n</code></pre> </p>"},{"location":"unit3/lesson9/lesson9.html#comparison-of-online-n-step-td-and-mc-for-different","title":"Comparison of online n-step TD and MC for different \u03b1","text":"<p>Ok, let us now study the effect of varying the hyperparameter n. n blends the horizon of all methods between bootstrapping algorithms and non-bootstrapping methods. To that end, we will apply TDn with different n values along with MC on a random walk prediction problem. This time we will use 19 states, and the goal to the left has a reward of -1, the goal on the right has a reward of 1, and all of the 19 intermediate states have a reward of 0.</p> <pre><code>def nstepTD_MC_randwalk_\u03b1compare(env=randwalk_(), algorithm=TDn, Vstar=None, runs=10, envlabel='19', \n                                 MCshow=True, alglabel='online TD'):\n\n    steps0 = list(np.arange(.001,.01,.001))\n    steps1 = list(np.arange(.011,.2,.025))\n    steps2 = list(np.arange(.25,1.,.05))\n\n    \u03b1s = np.round(steps0 +steps1 + steps2, 2)\n    #\u03b1s = np.arange(0,1.05,.1) # quick testing\n\n    plt.xlim(-.02, 1)\n    plt.ylim(.24, .56)\n    plt.title('n-steps %s RMS error averaged over %s states and first 10 episodes'%(alglabel,envlabel))\n    for n in [2**_ for _ in range(10)]:\n        Compare(algorithm=algorithm(env=env, v0=0, n=n, episodes=10, Vstar=Vstar), \n                              runs=runs, \n                              hyper={'\u03b1':\u03b1s}, \n                              plotE=True).compare(label='n=%d'%n)\n    if MCshow:\n        compare = Compare(algorithm=MC(env=env, v0=0, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s}, \n                                  plotE=True).compare(label='MC \u2261 TDn(n=$\\\\infty$)', frmt='-.')\n</code></pre> <pre><code>figure_7_2 = nstepTD_MC_randwalk_\u03b1compare\nfigure_7_2()\n</code></pre> <p></p> <p>Note how when n=\\(\\infty\\) TDn converges to an MC. Hence, we could see that TDn represents the full spectrum of algorithms that completely use bootstrapping (TD) and algorithms that do not use bootstrapping (MC). It nicely blends these algorithms and lets us control bootstrapping parametrically by choosing a suitable value for the n hyperparameter.</p>"},{"location":"unit3/lesson9/lesson9.html#offline-tdn","title":"Offline TDn","text":"<p>Let us now develop an offline TDn method. This method is exactly as its name suggests. We need to be mindful of going n-1 extra steps at the end because of the lag of n-1 steps at the start until the agent accumulates enough steps to obtain the \\(G_{t:t+n}\\).</p> <pre><code>class TDnf(MRP):\n\n    def init(self):\n        self.store = True # must store because it is offline\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        n=self.n        \n        for t in range(self.t+n): # T+n to reach T+n-1\n            \u03c4  = t - (n-1)\n            if \u03c4&lt;0: continue\n\n            # we take the min so that we do not exceed the episode limit (last step+1)\n            \u03c41 = \u03c4+1\n            \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1)\n\n            s\u03c4 = self.s[\u03c4 ]\n            sn = self.s[\u03c4n]\n            done = self.done[\u03c4n]\n\n            # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n            self.V[s\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1- done)*self.\u03b3**n *self.V[sn] - self.V[s\u03c4])\n</code></pre> <p><pre><code>TDnwalk = TDnf(env=randwalk(), v0=0, \u03b1=.1, n=4, episodes=100, seed=0, **demoV()).interact()\n</code></pre> </p> <p>Note that TDf == TDnf for n=1</p>"},{"location":"unit3/lesson9/lesson9.html#tdf-tdnf-and-mc-runs-on-random-walk","title":"TDf , TDnf and MC Runs on random walk","text":"<p>Let us now  see how a TDnf for n=1 and n=5 as well as MC behaves on our usual 5-states random walk on average. To that end as usual we execute several runs.</p> <p><pre><code>nstepTD_MC_randwalk(algorithm=TDnf, alglabel='TDf')\n</code></pre> </p> <p>Let us now double check that both TDnf and TDf are identical for n=1.</p> <p><pre><code>\u03b1s = np.arange(0,1.05,.1)\nn=1\ncompareTDf = Compare(algorithm=TDf(env=randwalk_(), v0=0, episodes=10), runs=2, hyper={'\u03b1':\u03b1s}, \n                     plotE=True).compare(label='TD offline')\n\ncompareTDnf = Compare(algorithm=TDnf(env=randwalk_(), v0=0, n=n, episodes=10), runs=2, hyper={'\u03b1':\u03b1s}, \n                      plotE=True).compare(label='TDn offline n=%d'%n)\n</code></pre> </p>"},{"location":"unit3/lesson9/lesson9.html#offline-tdnf-comparison","title":"Offline TDnf \u03b1 comparison","text":"<p>Let us now compare how offline n-step TD (TDnf) performs with different values for \u03b1 (learning-step hyper parameter.</p> <p><pre><code>nstepTD_MC_randwalk_\u03b1compare(algorithm=TDnf, alglabel='offline TD')\n</code></pre> </p>"},{"location":"unit3/lesson9/lesson9.html#n-step-sarsa-on-policy-control","title":"n-step Sarsa on-policy Control","text":"<p>As you can see, we have imported the class factory MDP to make it inherit the new MRP class that we defined in this lesson (which contains functions to deal with multiple steps updates) without having to restate the definition of MDP again.</p> <pre><code>class Sarsan(MDP(MRP)):\n\n    def init(self):\n        self.store = True        # although online but we need to access *some* of earlier steps,\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c41 = \u03c4+1\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n\n        s\u03c4 = self.s[\u03c4];  a\u03c4 = self.a[\u03c4]\n        sn = self.s[\u03c4n]; an = self.a[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.Q[s\u03c4,a\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1- done)*self.\u03b3**n *self.Q[sn,an] - self.Q[s\u03c4,a\u03c4])\n</code></pre> <p>Let us compare Sarsa and Sarsan performances on the same problem with the same reward.</p> <p><pre><code>sarsa = Sarsa(env=maze(reward='reward1'),  \u03b1=.2, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> </p> <p><pre><code>sarsa = Sarsan(env=maze(reward='reward1'), n=10,  \u03b1=.2, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> </p> <p>Note how the agent reaches a better policy in less number of episodes, i.e. it converges faster when using n-step method. At the end it counted an extra n-1 steps to finish the set of n-1 updates needed at the end of the episode.</p> <p>To further show the effect of multi-step, we show below how an agent policy is affected when by considering 1-step and 3-steps rewards.</p> <p></p>"},{"location":"unit3/lesson9/lesson9.html#n-step-q-learning","title":"n-step Q-learning","text":"<p>Can we implement an n-step Q-learning algorithm?</p> <p>If we think about this question superficially, from an implementation perspective, the answer seems simple. All we have to do is to replace the \\(\u03b3^n*Q[sn,an]\\) with \\(\u03b3^n*Q[sn].max()\\). However, this is not the correct way to implement the n-step off-policy Q-learning. </p> <p>Remember that Q-learning is an action-value learning method that uses TD learning off-policy, where we learn about a greedy policy while following a more exploratory policy, such as \u03b5-greedy. To be able to generalise the n-step TD update to an off-policy action-value method, we need to take into account the following. In each step of the n-steps that we are considering, we need our method to learn what the agent would have done if it had followed a \u03c0=greedy policy (instead of the b=\u03b5greedy). </p> <p>One way to compensate for this discrepancy between the behavioural policy b and the target policy \u03c0 would be to multiply each reward of the \\(G_{t:t+n}\\) with the importance sampling ratio. This ratio divides the probability of taking action a from policy \u03c0 by the probability of taking the same action according to policy b. Importance sampling methods suffer from high variance, and usually, they are not practical for control.</p> <p>Another approach is by using expectation instead of importance sampling. In this case, we use a similar idea to the expected Sarsa but in an off-policy context, where we alter the calculation of \\(G_{t:t+n}\\) in a way that sums over the different actions probabilities in each time step (except the first). This results in an algorithm called the Tree Backup Algorithm.</p> <p>Finally, we can blend the importance sampling with expectation using a hyperparameter \u03c3 which results in the Q(\u03c3) control algorithm. All of the above three approaches are outside the scope of our coverage, and there is a theory behind this choice. See section 7.3 of our textbook.</p>"},{"location":"unit3/lesson9/lesson9.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we have covered the n-step TD algorithms for prediction and control. We have seen how the different values of n represent a trade-off between full bootstrapping, as in the one-step TD (n=1), and no bootstrapping, as in the Monte Carlo algorithm (n=T the number of steps in an episode). Intermediate n values give algorithms that lie within the two extremes of the spectrum of bootstrapping algorithms. We also find that the best value is usually an intermediate value &gt;1. As we have seen, creating such an algorithm is challenging and has its drawbacks in terms of implementation. In later lessons, we will see how to achieve similar results without waiting or counting steps. We will adopt a bootstrapping variation mechanism which turns the n-step countable mechanism into an infinite continuum of value by adopting the \\(\\lambda\\) hyperparameter that takes a real value instead of integers.</p> <p>Further Reading: For further reading you refer chapter 7 from the Sutton and Barto book.</p>"},{"location":"unit3/lesson9/lesson9.html#your-turn","title":"Your turn","text":"<p>Now it is time to experiment further and interact with code in worksheet9.</p>"},{"location":"unit4/lesson12/lesson12.html","title":"Lesson 12: Introduction to Function Approximation Methods","text":"<p>Unit 4: Learning Outcomes By the end of this unit, you will be able to: </p> <ol> <li>Apply RL techniques to control an agent in complex environment representations.  </li> <li>Compare the trade-offs of on-policy and off-policy learning algorithms.  </li> <li>Evaluate the convergence properties of RL algorithms in both tabular and function approximation settings, considering their practical limitations.  </li> </ol> <p>In this and subsequent lessons, we will learn about finding solutions for states represented by features (observations). In a real-world environment, it is hard and unrealistic to expect to be able to identify a state fully. Usually, we can only partially observe the state via a set of features that can help us to recognise or distinguish a state, but that does not mean that we have guarantees that the state is fully identifiable or that the features are unique for each different state. Nevertheless, we should be able to deal with these spaces. After all, RL is meant for real-world problems. We must come to terms with the issue that this partial observability can be dealt with effectively in most practical cases by using suitable features. Because the states that share inner properties usually tend to have similar features, by dealing with these states via these features, we usually succeed in generalising to other similar states. To that end, dealing with the features via a set of parameters is natural, as we did in several machine learning modules. Of course, we can bring along other non-parametric models, such as Gaussian processes. However, this falls outside the scope of our RL treatment.  In some cases, we might need to generalise the RL framework from the MDP\u2019s underlying assumption of full observability to Partially Observable Markov Decision Processes or POMDP. In most cases, this might be unnecessary, and we can get away without dealing with the intricacy of POMDP. Again, POMDP is outside the scope of our coverage. What is left is then to move from a tabular to a parametric representation by adjusting the update rules we have dealt with so far to the parametric representation instead of the tabular one. </p>"},{"location":"unit4/lesson12/lesson12.html#plan","title":"Plan","text":"<p>From a practical perspective, we start by generalising from a tabular to an equivalent vectorised form via a one-hot representation. In this representation, each vector component corresponds with a state in the state space, so the vector size is the same as the number of states, and we use a one-hot encoding. To encode (represent) a state, we turn on (set to 1) the corresponding component (all other features are 0\u2019s) and the update for the weights\u2019 parameters will be applied similar to what we did for the tabular. In fact, each component's weight is really the value function for the corresponding state. We should get results identical to those we obtained for the problems we tackled in the tabular form\u2014random walk, the maze, the cliff walking, etc. The benefit of the one-hot encoding is that it is a vectorised version of the tabular representation and constitutes the first step towards generalising to a linear parametric model. Once we are satisfied that our vectorised form is working, we can then come up with different representations for the problems that have a dimensionality different than the state space, usually smaller and more concise than a continuous state space dimension, which can be infinite (countable or uncountable) or intractable, which is one of the advantages of parametric models. </p> <p>What is left is to find a suitable representation of the problem at hand.  Consequently, we continue converging towards a more general representation by covering state aggregation. In state aggregation, we group a set of states and represent them all in one feature, and we use one-hot encoding again. This time, we turn a feature on whenever the agent is at any of the group of states corresponding to it.  We then move to other constructions, including coarse coding and tile coding, to deal with state representation. The book chapter covers these very well, and you are advised to read about them in section 9.5. Selecting a construction is usually a matter of trial and error as well as preference. We can use a model that helps us automatically find suitable features for the problem at hand. </p> <p>This is where neural networks can come to the rescue.  Neural networks can automatically find a suitable set of features internally through their hidden layers, as we know. Whether to use a deep neural network or a shallow one depends on the complexity of the problem and the complexity that we would want our state representation to have. Neural networks help extract helpful features in the early layers that will be used in later layers to extract a correct value function. In this case, we do not need to use tile or similar coding because the network automatically learns the feature representation. The main issue that we often face, which acted as a deterrence for researchers to use neural networks for a long time, is that there are far fewer guarantees of convergence for neural networks, while several strong guarantees exist for linear regression models. Nevertheless, this impasse was broken with the introduction of DQN, and in practice, RL algorithms have proven resilient to neural networks in general. The picture is different for off-policy methods, and fewer guarantees exist for the tabular, let alone the function approximation methods. </p> <p>Note that we are dealing mainly with regression from an ML perspective. This is because the value function is just a function that maps the state to an actual number, so the answer is continuous values (not discrete). It is just a value, so we face a regression problem. When we deal with function approximation, TD (r + V(s')-V(s)) and other methods do not take the gradient of the target V(s'); we only differentiate V(s). This is because we are bootstrapping, and in ML(supervised learning), the target is a fixed value that is not differentiable (while in RL, it is). Remember, for example, that the update rule for the Monte Carlo method involves (Gt-V(s)), which, calculating its gradient, involves differentiating V(s) only. Because of this, we call the methods that do not take the target's gradient a semi-gradient method. We then use the action-value function to establish a suitable policy as we did earlier (ex., e-greedy). </p> <p>Finally, we move to a different type of RL control algorithms that deal directly with the policy and attempts to learn a policy directly without going through the intermediate step of fitting a value function. These methods might use regression or classification models called policy-gradient methods because they differentiate the policy \\(\\pi\\) itself, not \\(Q\\). These methods are amongst the most promising methods in RL and have proven more resilience with more convergence guarantees than action-value functions methods. </p> <p>Ok, with all of that in mind, let us get started.</p>"},{"location":"unit4/lesson13/lesson13.html","title":"13. Linear Approximation for Prediction","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit4/lesson13/lesson13.html#lesson-12-state-value-approximation-methods-for-prediction","title":"Lesson 12: State-Value Approximation Methods for Prediction","text":"<p>Learning outcomes</p> <ol> <li>understand the intractability of some real-world state space</li> <li>understand state space representation via a set of features and its advantages</li> <li>understand the properties of different function approximation models</li> <li>understand how to generalize tabular on-policy prediction methods to function approximation methods</li> </ol> <p>In this lesson, we deal with function approximation to represent the state. We will use different encoding regimes. One straightforward idea is to represent the whole space as a binary vector, where each entry represents a state. Another idea is to combine multiple entries/components from multiple vectors to represent the state and more. It is a good idea to start by reading section 9.5 of our book.</p> <p>Reading: The accompanying reading of this lesson is chapter 9 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#state-representation-one-hot-encoding","title":"State Representation: One-hot Encoding","text":""},{"location":"unit4/lesson13/lesson13.html#vectorising-grid-world-with-one-hot-encoding","title":"Vectorising Grid World with One-hot Encoding","text":"<p>In this section we aim to vectroise the grid world that we developed in the early lessons of a previous unit. The idea is to move gradually away from the tabular representation towards state-value function approximation where we approximate each state as a vector of features. Note that we where estimating the state-value function in the tabular form. So given that we assume limited states, it is possible, at least in theory to reach an exact value for all state-values or the action-values. When we deal with vector representation of a state, we depart from having finite number of states to the possibility of having infinite number of states in the world. Therefore, in this case we will be approximating the state representation and in turn we are estimating an approximate state-value and action-value functions and hence the name of our methods are Function Approximation Methods. </p> <p>To encode a state, we set to 1 the corresponding component while all other features are set to 0. </p> <pre><code>class vGrid(Grid):\n    def __init__(self, nF=None, **kw):\n        super().__init__( **kw)\n        # num of features to encode a state\n        self.nF = nF if nF is not None else self.nS \n        self.S = None\n\n    # vectorised state representation: one-hot encoding (1 component represents a state)\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        \u03c6[self.s] = 1 \n        return \u03c6\n\n    def S_(self):\n        if self.S is not None: return self.S\n        # S is a *matrix* that represents the full state space, this is only needed for Grid visualisation\n        sc = self.s  # store current state to be retrieved later\n        for self.s in range(self.nS): \n            self.S = np.c_[self.S, self.s_()] if self.s else self.s_()\n        self.s = sc \n        return self.S\n</code></pre> <p>Note that we need to pass 'vGrid' as the prefix for functions that deals with the different Grid types (such as maze and ranwalk ) to return a vGrid type instead of the usual Grid type in order to get the vectorised state representation that we defined above.</p> <pre><code>randwalk(vGrid).render()\nrandwalk(vGrid).s_()\n</code></pre> <p></p> <pre><code>array([0., 0., 0., 1., 0., 0., 0.])\n</code></pre> <pre><code>randwalk().render()\nrandwalk().s_()\n</code></pre> <p></p> <pre><code>3\n</code></pre> <p>Compare the return of the above calls, in the vGrid we obtained a vector that represents the current state while in the Grid we get the index of the current state. </p> <p>We can also define a vrandwalk to be a vectroised random walk grid as follows.</p> <pre><code>def vrandwalk(**kw):  return randwalk  (vGrid, **kw)\ndef vrandwalk_(**kw): return randwalk_ (vGrid, **kw)\ndef vgrid(**kw):      return grid      (vGrid, **kw)\ndef vmaze(**kw):      return maze      (vGrid, **kw)\ndef vcliffwalk(**kw): return cliffwalk (vGrid, **kw)\ndef vwindy(**kw):     return windy     (vGrid, **kw)\n</code></pre> <pre><code>vrandwalk().render()\nvrandwalk().s_()\n</code></pre> <p></p> <pre><code>array([0., 0., 0., 1., 0., 0., 0.])\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#prediction-with-function-approximation","title":"Prediction with Function Approximation","text":""},{"location":"unit4/lesson13/lesson13.html#mrp-with-linear-function-approximation","title":"MRP with Linear Function Approximation","text":"<p>Linear Feature Representation</p> <p>In a linear model we will devise, customary to linear regression models, a set of weights that correspond with each feature, so we have a weight vector that have the same size of the feature vector along with the bias, we will defer treating the bias to when it becomes necessary.</p> <p>Because we are dealing with linear models(regardless of the representation), the value function is given as</p> <p>\\(V(s) = w^\\top x\\)</p> <p>The update for the weights parameters will be applied similar to what we did for the tabular but this time we will use the dot product. </p> <p>In hot encoding, each component's weight is really the value function for the corresponding state. We should get identical results of those that we obtained for the problems that we tackled in the tabular form, ex. random walk, the maze, the cliff walking etc. </p> <pre><code>class vMRP(MRP):\n\n    # set up the weights, must be done whenever we train\n    def init(self):\n        self.w = np.ones(self.env.nF)*self.v0\n        self.V = self.V_ # this allows us to use a very similar syntax for our updates\n        self.S_= None\n\n    #-------------------------------------------buffer related-------------------------------------\n    # allocate a suitable buffer\n    def allocate(self): \n        super().allocate()\n        self.s = np.ones ((self.max_t, self.env.nF), dtype=np.uint32) *(self.env.nS+10)    \n\n    #---------------------------------------- retrieve Vs ------------------------------------------\n    def V_(self, s=None):\n        return self.w.dot(s) if s is not None else self.w.dot(self.env.S_()) \n\n    def \u0394V(self,s): # gradient: we should have used \u2207 but jupyter does not like it\n        return s\n</code></pre> <p>Note how we redefined the V as a function instead of as a array which allows us to use a very similar syntax for our updates, we will replace the squared brackets with rounded brackets and that's it!! thanks to the way we originally structured our MRP infrastructure. To appreciate this, let us see how we can redefine our offline MC algorithm along with the online TD update to deal with function approximation. Below we show how.</p>"},{"location":"unit4/lesson13/lesson13.html#gradient-mc-with-function-approximation","title":"Gradient MC with Function Approximation","text":"<pre><code>        -\u2207 Jt = -\u2207 1/2(\u03b4t^2) = \n        -\u2207 1/2(Gt - V(s))^2 = \n        -2(1/2)(Gt - V(s))*(\u2207 Gt - \u2207 V(s)) = \n        -1(Gt - V(s))*(0-s)=\n        (Gt - V(s))*s\n</code></pre> <pre><code>class MC(vMRP):\n    def __init__(self,  **kw):\n        super().__init__(**kw)\n        self.store = True \n\n    def init(self):\n        super().init() # this is needed to bring w to the scope of the child class\n        self.store = True \n\n    # ----------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning ----------------------    \n    def offline(self):\n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.w += self.\u03b1*(Gt - self.V(s))*self.\u0394V(s)\n</code></pre> <p>This definition is almost identical to the tabular definition except for the following: 1. we update now a set of weights instead of one entry in a table. So in the left hand side we see w instead of V[s]. 2. we use V(.) instead of V[.] on the right hand side (we could have used operator overloading to keep using the [.] but it is a bit more involving)  3. we multiply by the gradient \u2207V(s) that is given by the MRP parent class.</p> <p>Note that s here is the state representation of the actual state, i.e. it is a vector of components.</p> <p>Note also that although the parent class deals with linear function approximation, the MC class does not assume that, it just needs the gradient of the V. So, as along as we make sure the parent MRP class does provide this function, we can define other types of MRP that use other types of function approximation like the tile coding or neural networks and we would not need to change the TD definition.  Please refer to section 9.3 of the book for more details.</p> <p>Let us now apply this vectorised from of the MC, or more precisely the gradient MC algorithm, on the random walk problem that use one-hot encoding. </p> <pre><code>mcwalk = MC(env=vrandwalk(), \u03b1=.01, episodes=20, v0=.5, seed=0, **demoV()).interact(label='offline MC learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#online-semi-gradient-td-with-linear-function-approximation","title":"Online Semi-Gradient TD with Linear Function Approximation","text":"<pre><code>class TD(vMRP):\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        self.w += self.\u03b1*(rn + (1-done)*self.\u03b3*self.V(sn) - self.V(s))*self.\u0394V(s)\n</code></pre> <p>Again, note also that although the parent class deals with linear function approximation, the TD class does not assume that, it just needs the gradient of V. So, as along as we make sure the parent MRP class does provide this function, we can define other types of MRP that use other types of function approximation like the tile coding or neural networks and we would not need to change the TD definition. Therefore we can use a class factory to be able to change the parent class when we need to.</p> <p>We called this algorithmm a semi-gradient since we took the gradient of the value function V(s) and not the gradient of the target V(sn). The reason is to do be consistent with MC which is considered the theoretical baseline for TD. Nevertheless, there are algorithms that takes the gradient of both, although these are usually not as fast as semi-gradient TD, especially with linear function approximation, refer to section 9.3 of our book for more details.</p> <p>Let us now apply this vectorised from of the TD, or more precisely the semi-gradient TD algorithm. We call it semi-gradient because we to the gradient of one of the terms of TD error which is the estimation of the current state and we left the target estimation of the next state as is. Please refer to section 9.3 of the book.</p> <pre><code>TDwalk = TD(env=vrandwalk(), episodes=20, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>We need to pass the env explicitly because we have inherited the MRP and the new environment would have no took effect.</p>"},{"location":"unit4/lesson13/lesson13.html#offline-semi-gradient-td-with-function-approximation","title":"Offline Semi-Gradient TD with Function Approximation","text":"<pre><code>class TDf(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            sn = self.s[t+1]\n            rn = self.r[t+1]\n            done = self.done[t+1]\n\n            self.w += self.\u03b1*(rn + (1-done)*self.\u03b3*self.V(sn) - self.V(s))*self.\u0394V(s)\n</code></pre> <pre><code>tdwalk = TDf(env=vrandwalk(), \u03b1=.05, episodes=50, v0=.5, **demoV()).interact(label='offline TD learning')\n</code></pre> <p>Let us rerun example 6.2 to double check that our algorithm is working well. This time we are using a vectorised grid and a linear model for TD and MC.</p> <pre><code># runing the book example 6.2 which compare TD and MC on randome walk, but this time we use vector representation\nexample_6_2(env=vrandwalk(), alg1=TDf, alg2=MC)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#one-hot-encoding-with-redundant-features-for-prediction","title":"One-hot-encoding with redundant features for prediction","text":"<p>As an auxiliary step towards generalising our classes to deal with any linear function approximation, below we create a random walk problem with 1000 redundant features to test whether our infrastructure classes are working.</p> <pre><code>vTDwalk = TD(env=vrandwalk_(nF=100), episodes=10, v0=.5, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#state-representation-state-aggregation","title":"State Representation: State aggregation","text":"<p>\\(\\underbrace{\\{s_0\\}}_{F_0}, \\underbrace{\\{s_1, s_2, ..., s_{100}\\}}_{F_1}, \\underbrace{\\{s_{101}, s_{102}, ..., s_{200}\\}}_{F_2},...,  \\underbrace{\\{s_{901}, s_{902}, ..., s_{1000}\\}}_{F_{10}}, \\underbrace{\\{s_{1001}\\}}_{F_{11}},\\)</p> <p>For the state aggregation to work as intended, the goals must have a separate state representation from the groups; hence we dedicate the first and the last components for these two terminal states as we have shown above (or even put both of them in the same component). We then divide the non-goal states into 100, where each 100 are represented via a component and whenever the agent is at any of the 100 states the corresponding component will be on and all other components are off. </p> <p>Let us now move to a different state representation technique that generalizes the simple state-to-component one-hot encoding regime that we used earlier. We would like to move closer toward bridging continuous and discrete state space and so aggregation seems an obvious choice. In state aggregation, we group a set of states together and represent them all in one component(feature) and we use again one-hot encoding. This time one component represents a group of states instead of one state. Therefore, we turn a component(feature) on whenever the agent is at any of the group of states that corresponds to it. </p> <p>\\(F_i: \\underbrace{\\{s_0, s_1, ..., s_{99}\\}}_{F_0}, \\underbrace{\\{s_{100}, ..., s_{199}\\}}_{F_1},...,  \\underbrace{\\{s_{900}, ..., s_{999}\\}}_{F_9}\\)</p> <p>Therefore, if the agent is in state \\(s_{100}\\), in state \\(s_{199}\\) or in any state in between, the feature \\(F_1\\) will be on = 1 and the rest \\(F_0, F_2...F_9\\) are all \\(0\\)'s. The feature vector that represents the current state would be \\(F =[0,1,0,0,0,0,0,0,0,0]\\)</p> <p>This way also we treat the goal(terminal) states like other non-terminal states in terms of representation, and that is ok since we will leave the terminal state treatment to the learning algorithms. Recall that the return at time step \\(t\\) is the sum of expected future rewards from time step \\(t+1\\) to the end of an episode at time step \\(T\\) and is given by:  </p> <p>\\(G_t = R_{t+1} + R_{t+2} + ... + R_{T}\\)</p> <p>So the reward of current state does not participate in its return unlike the rewards of all future rewards that form the current state return. Hence the terminal state value (or expected return) is always set to 0 because the agent stays there and it will not obtain any future rewards from that state on. This is different than the terminal state reward itself given by the environment which may or may not be 0. Our algorithms do the following to treat terminal states: when the next state s' is a terminal state all of our algorithms will assign 0 to the terminal state's value estimation by multiplying it by (1-done), done is True on terminal state so (1-done)=0 at the terminal state. This alleviate us from having to designate a separate component to represent the terminal states and simplifies greatly our implementation.</p> <p>We call each group a tile, so a tile covers a group of states, think of a state in this context as the unit that we measure the tile's area with (hence the bridging of continuous and discrete state space). The above example has 10 tiles each of size 100 sates, while using tiles of size 200 mean that each covers or encompasses 200 states and will result in a feature vector of 5 components. </p> <p>Similar to vectors and matrices, tiles comes in different dimensions. When we deal with random walk problems we have 1-d tiles, while when we deal with a usual grid we will be dealing with 2-d tiles. The tile(group) size is stored in a variable called tilesize. Refer to example 9.2 in the book for more details. </p> <p>Below we provided an explanation of a precise process that we can use to achieve the above representation. </p> <pre><code>15//3\n</code></pre> <pre><code>5\n</code></pre> <pre><code>nS = 1002\nnS = 900\ngoals = [0, nS-1]\ntilesize = 200\n\n#------------calculating number of componenets(features)-----------\nnF =  -(-nS//tilesize)   # 1 in case of nS is not divisible by tilesize\nprint('number of groups = ', nF)\n\n#------------obtainng an index (feature)---------------------------\ns   =  100# goals[1] # goals[0]\nind =  s//tilesize\nprint('the goal\\'s index = ', ind)\n\n#------------assigning the feature---------------------------------\nw = np.zeros(nF)\nw[ind] = 1\nprint(w)\n</code></pre> <pre><code>number of groups =  5\nthe goal's index =  0\n[1. 0. 0. 0. 0.]\n</code></pre> <pre><code>#In python the operator // rounds up if we used a negative number\nprint(   1002//200 )\nprint(-(-1002//200))\n</code></pre> <pre><code>5\n6\n</code></pre> <pre><code>class aggGrid(vGrid):\n    def __init__(self, tilesize=1, **kw):\n        super().__init__(**kw)\n        self.tilesize = self.jump = tilesize\n        self.nF = -(-self.nS//self.tilesize)\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF) \n        \u03c6[self.s//self.tilesize] = 1 \n        return \u03c6\n</code></pre> <p>As we can see, we have encoded the states via our s_( ) function which uses one-hot encoding.  The index is specified via the aggregation which is achieved by using the //tilesize operation.</p>"},{"location":"unit4/lesson13/lesson13.html#1000-states-random-walk-with-jumps","title":"1000 states random walk with jumps","text":"<p>To shorten the time of transfer between the states and to adapt it to work well with state aggregation we inherited from the class vGrid which inherited from Grid, which in turn allows the agent to jump any number of states. vGrid also provide access to S_() function which is needed to obtain the V_ values of the random walk process.</p> <p>In the example below we choose to allow for a random jumps of up to 50 steps and we aggregate/group the states into 50 instead of 100 because this will result in around 20 feature similar to our 19-state random walk problem that we saw earlier in previous lessons (it is 19+2 states with terminal states).</p> <p>\\(\\underbrace{\\{s_0, s_1, ..., s_{49}\\}}_{F_0}, \\underbrace{\\{s_{50}, ..., s_{99}\\}}_{F_1},...,  \\underbrace{\\{s_{950}, ..., s_{999}\\}}_{F_{19}}\\)</p> <pre><code># assuming that vstar is a function that returns Vstar values\ndef aggrandwalk_(nS=1000, tilesize=50, vstar=None, **kw): \n    env = randwalk_(aggGrid, nS=nS, tilesize=tilesize, **kw)\n    if vstar is not None: env.Vstar = vstar(env) # vstar is a function\n    return env\n</code></pre> <pre><code>aggTDwalk = TD(env=aggrandwalk_(nS=12, tilesize=4, figsize=[40,.5], jump=4), \n                 episodes=100, v0=.5, seed=10, visual=True).interact(label='TD learning', pause=.5)\n</code></pre> <p></p> <p></p> <pre><code>env = aggrandwalk_(nS=23, tilesize=3, figsize=[40,.5])\nenv.render(pause=1)\nenv.jump=3\nenv.step(1)\nenv.render()\n</code></pre> <p></p> <p>Let us now apply our online TD on a 25 state aggregation problem with 5 groups.</p> <pre><code>aggTDwalk = TD(env=aggrandwalk_(nS=24+2, tilesize=4),\u03b1=.05, episodes=100, v0=.0, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#n-step-td-with-linear-function-approximation","title":"n-step TD with linear function approximation","text":"<pre><code>class TDn(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True # there is a way to save storage by using t%(self.n+1) but we left it for clarity\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n        \u03c41 = \u03c4+1\n\n        s\u03c4 = self.s[\u03c4 ]\n        sn = self.s[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n</code></pre> <pre><code># try increase nS to 102+2 to see the effect\naggTDwalk = TDn(env=aggrandwalk_(nS=100, tilesize=10), \u03b1=.02, n=4, episodes=200, v0=.0, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <pre><code>aggTDwalk = TDn(env=aggrandwalk_(), \u03b1=.01, n=10, episodes=200, v0=.0, seed=0, **demoV()).interact(label='TD learning')\n</code></pre> <p>Note that we had to reduce the learning rate \u03b1 because the increased number of steps entails more updates and hence larger update magnitude.</p>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-random-walk-via-dynamic-programmingdp","title":"Solving the 1000 Random Walk via Dynamic Programming(DP)","text":"<p>It might be hard to notice that the stairs looks a bit off (bottom steps start over the straight line and top ones appears under the line). The problem is in fact not in our TDn algorithm, instead it is in the Vstar solution (the straight line). Because we add the ability for the agent to jump, the old solution is not valid any more albeit very close.</p> <p>To arrive to a more accurate solution, we can hand in the problem to a dynamic programming algorithm to solve it for us. Below we show a solution for the 1000 random walk with jumps based on Dynamic Programming techniques that we covered in lesson 3. Particularly we use the policy evaluation method since we are dealing with prediction and the policy is stationary (agent moves either to the left or to the right with equal .5 probabilities.)</p> <p>DP will help us to see how far the initial guess (the one similar to 19-states but with 1000 states random walk without the jumps) from the actual solution of the 1000-states random walk with the jumps. </p> <pre><code>def DP(env=aggrandwalk_(), compare=False, \u03b8=1e-2):\n    \u03c0 = np.ones((env.nS, env.nA), dtype=np.uint32)*.5\n    Vstar = Policy_evaluation(env=env, \u03c0=\u03c0, V0=env.Vstar, \u03b8=\u03b8, show=False)\n    print('V* obtained')\n    if compare:\n        plt.plot(env.Vstar,   label='solution for 1000-random walk without jumps')\n        plt.plot(Vstar[1:-1], label='solution for 1000-random walk with    jumps')\n        plt.legend()\n    return Vstar\n</code></pre> <p>Let us compare the default straight line solution to the V* solution for the random walk problem when we employ the jumping procedure. This will take a couple of minutes so please wait for it.</p> <pre><code>aggVstar = DP(env=aggrandwalk_(), compare=True)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.14it/s]\n\n\nV* obtained\n</code></pre> <p></p> <p>This shows that the initial guess and the DP solution are close but not quite the same. Note that if we decrease \u03b8 the algorithm will get a more accurate estimation (the blue line will bend further) but it will take longer to run, the length of the run shows one of the issues of DP compared to more resilient and faster RL algorithms such as TD. Trye set  \u03b8=1e-3 to see how long it will take and share the length within the group discussion.</p>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-random-walk-with-online-n-step-td-with-linear-function-approximation-and-state-aggregation","title":"Solving the 1000 Random Walk with Online n-step TD with linear function approximation and state aggregation","text":"<p>We can integrate finding a DP solution with the comparison as below, but since we have already found a solution in the previous steps we can simply also utilise the solution for comparison directly as we do in subsequent cells. Note that the level of accuracy, specified in \u03b8, can be adjusted. </p> <pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(vstar=DP), \\\n                                   algorithm=TDn, runs=10, envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.30it/s]\n\n\nV* obtained\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 39min 16s, sys: 18min 26s, total: 57min 43s\nWall time: 8min 32s\n</code></pre> <p></p> <pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(Vstar=aggVstar), \\\n                                   algorithm=TDn, runs=10, envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 39min 59s, sys: 20min 33s, total: 1h 33s\nWall time: 8min 2s\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#offline-n-step-td-with-linear-function-approximation","title":"Offline n-step TD with linear function approximation","text":"<pre><code>class TDnf(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True # offline method we need to store anyway\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        n=self.n        \n        for t in range(self.t+n): # T+n to reach T+n-1\n            \u03c4  = t - (n-1)\n            if \u03c4&lt;0: continue\n\n            # we take the min so that we do not exceed the episode limit (last step+1)\n            \u03c41 = \u03c4+1\n            \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1)\n\n            s\u03c4 = self.s[\u03c4 ]\n            sn = self.s[\u03c4n]\n            done = self.done[\u03c4n]\n\n            # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n            self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-with-offline-n-step-td-with-linear-function-approximation-and-state-aggregation","title":"Solving the 1000 with Offline n-step TD with linear function approximation and state aggregation","text":"<pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(Vstar=aggVstar),\\\n                                   algorithm=TDnf, runs=10, alglabel='offline TD', envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n15%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|5/32\n\n/var/folders/5s/t5nyc5mx7gd0_0g56wdmpgmrq78pqs/T/ipykernel_13119/3845626547.py:23: RuntimeWarning: invalid value encountered in multiply\n  self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n\n\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 35min 31s, sys: 18min 28s, total: 54min\nWall time: 7min 13s\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#state-representation-tile-coding","title":"State Representation: Tile Coding","text":"<p>Tile coding is a powerful state representation, it takes the idea of state aggregation one step ahead. It is assumed that the state space is continuous and we aim to discretize it by partitioning it several times. In state aggregation that is what we have done. Effectively we have partitioned the state space into tiles. Here, we will do it for more than 1 time. </p> <p>We call each state partition that covers a specific area of the space as a tiling and we have \\(d\\) tilings. Each tiling has \\(n\\) tiles as we saw earlier in state aggregation. Each component of a tiling vector is called a tile.  The length of the tile encoding vector is \\(d\\times n\\).</p> <p>Therefore, instead of having one component on as in the state aggregation, we design a larger vector where we have more components that are turned on to help us differentiate further between the states. Note that the early grouping that we did in the previous section is actually a tile encoding with one tiling.</p> <p>When an agent is in state \\(s\\) we will have exactly \\(d\\) components turned on and the rest \\(d\\times n - d = d \\times (n-1)\\) are \\(0\\)s (while in state aggregation we had \\(1\\) component on and the rest of the \\(n-1\\) components were \\(0\\)). What is left is to understand how to construct and turn these \\(d\\) components on. For construction we simply look at the state space and cover it with a set of tiles (tiling) so that when an agent in a state, one of the tiles will be on. This can be simply a state partitioning. Then we lay another tiling, this time we shift (or offset/stride similar to what we do in CNN!) them so that there is some level of overlapping between the two tiling and so on. When we have two tilings, we will have two tiles on.</p> <p>\\(\\underbrace{\\{s_0, s_1, ..., s_{199}\\}}_{F_{0,0}}, \\underbrace{\\{s_{200}, ..., s_{399}\\}}_{F_{1,0}},...,  \\underbrace{\\{s_{800}, ..., s_{999}\\}}_{F_{4,0}}\\)</p> <p>\\(\\underbrace{\\{s_1, s_2, ..., s_{200}\\}}_{F_{0,1}}, \\underbrace{\\{s_{201}, ..., s_{400}\\}}_{F_{1,1}},...,  \\underbrace{\\{s_{801}, ..., s_{999}\\}}_{F_{4,1}}\\)</p> <p>\\(\\underbrace{\\{s_2, s_3, ..., s_{201}\\}}_{F_{0,2}}, \\underbrace{\\{s_{202}, ..., s_{401}\\}}_{F_{1,2}},...,  \\underbrace{\\{s_{802}, ..., s_{999}\\}}_{F_{4,2}}\\)</p> <p>So if the agent in stat \\(s_{200}\\) then its state tile coding representation would be:</p> <p>$F = [0, 1, 0, 0, 0, \\quad 1, 0, 0, 0, 0, \\quad 1, 0, 0, 0, 0] $</p> <p>Finally, if the tiling creates a strain on the memory requirement, we combine it with hashing. Hashing can save us lots of space and computation time. Note that when we use hashing we would need to construct a state vector that is of the size of the hashed features which is smaller than that of a \\(d\\times n\\) . Note that although we assumed that the state space is continuous, we can usually treat the state space as if it is continuous. See section 9.4.5 along with 9.4.4.</p> <pre><code>class tiledGrid(vGrid):\n    def __init__(self, ntilings, offset=4, tilesize=50, **kw):\n        super().__init__(**kw)\n        self.tilesize = self.jump = tilesize\n        self.ntilings = ntilings\n        self.offset = offset\n        self.ntiles = -(-self.nS//self.tilesize) \n        self.nF = self.ntiles*self.ntilings\n\n    def s_(self):\n        \u03c6 = np.zeros((self.ntilings, self.ntiles))\n\n        for tiling in range(self.ntilings):\n            ind = min((self.s + tiling*self.offset)//self.tilesize, self.ntiles-1)\n            \u03c6[tiling, ind] = 1\n\n        return \u03c6.flatten()\n</code></pre> <p>Let us now define a tiled random walk environment with 1000 states handy to be used in our next set of experiments. As usual it has a rewards of (-1,1) for the far left and far right states while it has a tile size of 200. We can use multiple tilings for it to cover its states. We will use our original guess as the optimal Vstar estimation for the state values,however we will allow Vstar to be assigned a more correct values based on DP.</p> <pre><code>def tiledrandwalk_(nS=1000, ntilings=1, tilesize=200, vstar=None, **kw):\n    env = randwalk_(tiledGrid, nS=nS, ntilings=ntilings, tilesize=tilesize,  **kw)\n    if vstar is not None: env.Vstar = vstar(env) \n    return env\n    #return randwalk_(tiledGrid, nS=nS, Vstar=Vstar, ntilings=ntilings, tilesize=tilesize,  **kw)\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#studying-the-effect-of-number-of-tilings","title":"Studying the Effect of Number of Tilings","text":"<p>Below we run the tiled random walk problem with different number of tilings to show there effect.</p> <pre><code>def TDtiledwalk(ntilings):\n    env=tiledrandwalk_(nS=20, tilesize=4, offset=1, ntilings=ntilings)\n    TD(env=env, \u03b1=.02, episodes=200, **demoV()).interact(label='TD learning, %d tilings'%ntilings)\n</code></pre> <pre><code>TDtiledwalk(ntilings=1)\n</code></pre> <p></p> <p>TDtiledwalk(ntilings=2)</p> <pre><code>TDtiledwalk(ntilings=3)\n</code></pre> <p></p> <p>TDtiledwalk(ntilings=4)</p> <pre><code>TDtiledwalk(ntilings=5)\n</code></pre> <p></p> <p>Note how the increased number of tilings enhanced the estimation and reduced the error and the values become smoother with less stair-style effect. So in fact ntilings has a smoothening effect on the value function and helps to improve the estimation of our algorithms. </p>"},{"location":"unit4/lesson13/lesson13.html#td-on-1000-tiled-coded-random-walk","title":"TD on 1000 Tiled Coded Random Walk","text":"<pre><code>TDwalk = TD(env=tiledrandwalk_(ntilings=8, Vstar=DP(tiledrandwalk_())),\u03b1=.005, episodes=200, **demoV()).interact(label='True Online TD(%.2f) learning')\n</code></pre> <p>When using function approximation, the objective function needs to be differentiable and we used the sum of squared error (SE) from which we obtained the gradient. This makes sense since it is easily differentiable objective function instead of the harder RMSE (root of the mean squared error). However we can still use our RMSE to measure the performance as we did in previous lesson that we have used previously. </p> <p>Below we compare between 1 tiling and 50 tilings for the tiled random walk problem to generate figure 9.10 of the book. We used Monte Carlo because it is a full gradient algorithm since the target does not involve a next step estimate, but we can use TD as well.</p> <p>First let us find the Vstar for a 200 jumps problem (the previous one was for 50 so we cannot use it).</p> <pre><code>def MCtiltingsRuns():\n    Vstar=DP(tiledrandwalk_(),\u03b8=1e-3)\n    for ntilings in [1, 50]:\n        env=tiledrandwalk_(ntilings=ntilings, tilesize=200, Vstar=Vstar)\n        \u03b1 =.001/ntilings\n        mcs = Runs(algorithm=MC(env=env,\u03b1=\u03b1, episodes=5000), v0=0, \n                   runs=30, plotE=True).interact(label='MC with %d tilings'%ntilings)\n    plt.ylim(0,.45)\n\nfigure_9_10=MCtiltingsRuns \n</code></pre> <pre><code>figure_9_10()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.82s/it]\n\n\nV* obtained\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n</code></pre> <p></p> <p>This figure takes a long time to produce due to the extensivity of the experiments. As we can see, adding more tilings further reduced the error of the value function estimation and hence enhanced the algorithm's performance.</p>"},{"location":"unit4/lesson13/lesson13.html#conclusion","title":"Conclusion","text":"<p>In this lesson we have covered the different aspects of using function approximation in the context of reinforcement learning methods We have seen how to generalize the ideas covered in previous lessons to parametric models via semi-gradient methods including Semi-gradient Sarsa.</p>"},{"location":"unit4/lesson13/lesson13.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulation of RL which assumed that we use a parametric representation for our state space using function approximation instead of a table. In the next unit, we continue on that front to cover control algorithms that use function approximation and we will see a set of applications of RL in various domains.</p>"},{"location":"unit4/lesson13/lesson13.html#your-turn","title":"Your turn","text":"<ol> <li>Use tile coding on a grid world problem with Sarsa and see its effect.</li> <li>Sutton referred to idea of tile coding and that it is almost trivial to update the weights and rather than doing the dot product we can just pick the components that are active and update accordingly. Can you think of a way to implement such strategy for the state aggregation case? what kind of update we will end up with?</li> <li>When we varies ntilings in TDtiledwalk() function we more fluctuate in the error. Can you think of ways to counter this undesirable effect.</li> <li>In TDtiledwalk() try to reduce the learning rate \\(\\alpha\\) when we increase ntilings ex.: \\(\\alpha\\)=.02/ntilings. Adjust the code and see its effect.</li> <li>Prove the above state form of equation 13.9.</li> <li>Have a look at the following code and see if this implementation for aggGrid makes any difference to the random walk results obtained earlier. Note how this divides the non-terminal states by subtracting the 2 goal states form the count to define nF and then to obtain the hot-encoding it tests whether the state is a goal state and it always subtract 1.  $ i = (s-1)//200$ gives \\(F_i: \\underbrace{\\{s_1, s_2, ..., s_{200}\\}}_{F_0}, \\underbrace{\\{s_{201}, ..., s_{400}\\}}_{F_1},...,  \\underbrace{\\{s_{801}, ..., s_{1000}\\}}_{F_4}, \\underbrace{\\{s_{1001}, s_0\\}}_{F_5}\\). This implementation is more complicated but it will show us a more uniformed division for the non-terminal states. Note that you would need to add 2 also to the nS in randwalk_() so that you can visually see the desired effect of evenly dividing the non-terminal states equally.</li> </ol>"},{"location":"unit4/lesson14/lesson14.html","title":"14. Linear Approximation for Control","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit4/lesson14/lesson14.html#lesson-13-action-value-approximation-and-policy-gradient-methods-for-control","title":"Lesson 13: Action-Value Approximation and Policy Gradient Methods for Control","text":"<p>Learning outcomes 1. understand how to generalize tabular control methods to function approximation methods 1. understand how to generalize tabular policy methods to policy approximation methods 1. understand how to using tile coding for a continuous control problem 1. understand the difference between additive and multiplicative representation 1. understand how to take advantage of binary representation to come up with a binary control algorithm-namely Binary Sarsa 1. understand the benefit of using hashing along the side with tile coding and the added benefit of using index hash table</p> <p>Reading: The accompanying reading of this lesson is chapters 10 and 12 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>In this lesson, we continue our coverage of algorithms that use function approximation, this time for control. We use tile coding again, but this time in the context of an underpowered car that needs to climb a mountain. In susequent lessons, we then move to a set of powerful techniques that utilise the idea of eligibility traces, which allow an algorithm to perform updates that mimic the effect of a set of n-steps updates without having to wait for n-steps! We then move to cover RL algorithms that are suitable for Robotics and Games, which utilise non-linear function approximation, particularly neural networks shallow and deep, such as DQN and DDQN.</p> <p>So far, we have not tackled any task that has a continuous state space; rather all the state spaces that we have come across were discrete. In this lesson, we will see how to apply the tile coding technique we covered in the previous lesson on a continuous control task. The task will be controlling an underpowered vehicle stuck between two hills. We want our car to successfully negotiate this terrain and reach the right hilltop. Along the way, we will develop a new binary algorithm, Binary Sarsa, that is suitable for dealing with binary encoding, and we will study its performance. We present three representations of the problem. The first just discretised the space and used a vector representation that corresponds with this discretisation. This representation is equivalent to using one tiling in tile coding. Then we develop this representation to use multiple tiling that offset each other to enrich the representation capability for our continuous space. We then try to reduce the extra overhead introduced by the tile coding using hashing. We show two types of hashing, one that uses raw hashing via the modulus % operator and one that uses an index hashing table, which guarantees correspondence with non-hashed tile coding when the table is large enough. Ok let us get started...</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rlln import *\nfrom math import floor\n</code></pre> <p>Note that we imported the Grid and the other environments from the Function approximation lesson because we are dealing with vectorised environment form now on.</p>"},{"location":"unit4/lesson14/lesson14.html#control-with-function-approximation","title":"Control with Function approximation","text":""},{"location":"unit4/lesson14/lesson14.html#mdp-with-linear-function-approximation","title":"MDP with Linear Function Approximation","text":"<p>Below we show the implementation of the new MDP class. We have set it up in a way that allows us to maintain the same structure of the different updates that uses Q values. Note that we assign the new self.Q to the function self.Q_ which makes self.Q a function and then we can call self.Q(s,a) to obtain the Q values for state s and action a.</p> <pre><code>class vMDP(MDP(vMRP)):\n\n    def init(self):\n        super().init()\n        self.W = np.ones((self.env.nA, self.env.nF))*self.q0\n        self.Q = self.Q_\n\n    def Q_(self, s=None, a=None):\n        #print(s.shape)\n        W = self.W if a is None else self.W[a]\n        return W.dot(s) if s is not None else np.matmul(W, self.env.S_()).T \n\n    # we should have used \u2207 but python does not like it\n    def \u0394Q(self,s): \n        return s\n</code></pre> <p>Below we make sure that the classes hierarchy is correct by double-checking that the policy of an MDP object is \u03b5greedy.</p> <pre><code>vMDP().policy\n</code></pre> <pre><code>&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.vMDP object at 0x127e58b60&gt;&gt;\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#offline-mcc-with-any-function-approximation","title":"Offline MCC with Any Function Approximation","text":"<pre><code>class MCC(vMDP):\n\n    def init(self):\n        super().init()\n        self.store = True\n\n    # ---------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning-----------------------    \n    def offline(self):  \n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            a = self.a[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.W[a] += self.\u03b1*(Gt - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>mc = MCC(env=vgrid(reward='reward100'), \u03b1=.5, episodes=20, seed=10, **demoQ()).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#online-sarsa-with-any-function-approximation","title":"Online Sarsa with Any Function Approximation","text":"<pre><code>class Sarsa(vMDP):\n\n    def init(self): #\u03b1=.8\n        super().init()\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(reward='reward1'), \u03b1=.8, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.Q_(10,1)\n</code></pre> <pre><code>array([-32.256     , -33.9072    , -27.2128    , -24.5888    ,\n       -16.32      ,  -8.        ,  -8.        ,   0.        ,\n         0.        ,   0.        , -41.189888  , -33.6       ,\n       -27.4432    , -24.64      , -17.6       ,  -9.6       ,\n        -8.        ,  -8.        ,   0.        ,  -8.        ,\n       -39.7592576 , -35.354112  , -29.29706435, -19.989784  ,\n        -9.99998879,  -0.32      ,   0.        ,   0.        ,\n         0.        ,   0.        , -41.7846272 , -46.20985139,\n       -27.0091264 , -24.335872  , -16.144384  ,  10.        ,\n         0.        ,  -8.        ,  -8.        ,  -8.        ,\n       -39.0272    , -37.9008    , -24.256     , -18.88      ,\n       -11.586048  ,  -9.6       ,  -8.        ,   0.        ,\n         0.        ,   0.        , -33.28      , -32.83456   ,\n       -28.46976   , -24.97024   , -20.1728    , -11.2       ,\n        -8.        ,   0.        ,   0.        ,   0.        ,\n       -34.0992    , -26.6368    , -22.976     , -24.512     ,\n       -25.6128    , -28.16      ,  -8.        ,   0.        ,\n         0.        ,   0.        , -28.3648    , -32.4608    ,\n       -26.18624   , -27.0976    , -18.88      , -16.        ,\n       -14.4       ,   0.        ,   0.        ,   0.        ])\n</code></pre> <pre><code>def example_6_5():\n    return Sarsa(env=vwindy(reward='reward1'), \u03b1=.5, episodes=170, seed=100, **demoQ()).interact(label='TD on Windy')\n\ntrainedV = example_6_5()\n\nplt.subplot(133).plot(trainedV.Ts.cumsum(), range(trainedV.episodes),'-r', label='cumulative steps')\nplt.show()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#q-learning-with-function-approximation","title":"Q-learning with Function Approximation","text":"<pre><code>class Qlearn(vMDP):\n# \ud83d\udd79\ufe0f \ud81a\udc44 \n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn).max() - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>qlearn = Qlearn(env=vgrid(), \u03b1=.8, \u03b3=1, episodes=40, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>qlearn = Qlearn(env=vgrid(reward='reward1'), \u03b3=1, \u03b1=.8, episodes=40, seed=10, **demoQ()).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#sarsa-and-q-learning-on-the-cliff","title":"Sarsa and Q-Learning on the Cliff!","text":"<pre><code>sarsa = Sarsa(env=vcliffwalk(), \u03b1=.5, episodes=500, seed=10, **demoR()).interact()\n</code></pre> <pre><code>sarsa = Qlearn(env=vcliffwalk(), \u03b1=.5, episodes=500, seed=10, **demoR()).interact()\n</code></pre> <pre><code>SarsaCliff, QlearnCliff = example_6_6(runs=20, env=vcliffwalk(), alg1=Sarsa, alg2=Qlearn)# runs=500\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <pre><code>class XSarsa(vMDP):\n\n    # ------------------------------------- \ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):      \n        # obtain the \u03b5-greedy policy probabilities, then obtain the expecation via a dot product for efficiency\n        \u03c0 = self.\u03c0(sn)\n        v = self.Q(sn).dot(\u03c0)\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*v - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>xsarsa = XSarsa(env=vmaze(), \u03b1=.5, \u03b3=1, episodes=100, seed=1, **demoQ()).interact()\n</code></pre> <pre><code>xsarsa = XSarsa(env=vcliffwalk(), \u03b1=.5, \u03b3=1, episodes=100, seed=1, plotT=True).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#one-hot-encoding-with-redundant-features-for-control","title":"One-hot-encoding with redundant features for control","text":"<pre><code>sarsa = Sarsa(env=vmaze(nF=160, reward='reward1'), episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.store=True\nsarsa.interact(seed=10)\n</code></pre> <pre><code>&lt;__main__.Sarsa at 0x129fabd70&gt;\n</code></pre> <pre><code>sarsa.s[0]\n</code></pre> <pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0], dtype=uint32)\n</code></pre> <p>Note how we have almost 100 extra redundant features that will never be turned on. The point of doing such thing is to double check that our infrastructure is capable of accommodating different features sizes and is not tied to a feature representation that has the same size of the state space.</p>"},{"location":"unit4/lesson14/lesson14.html#policy-gradient-methods-with-function-approximation","title":"Policy Gradient Methods with Function Approximation","text":"<p>We turn out attention now to develop an actor-critic algorithm that uses function approximation. Note that as we said earlier the actor part uses control updates similar to Sarsa or Q learning while the critic part uses TD prediction updates. Hence, writing our algorithm is straightforward because we have already developed the infra-structure for both prediction and control.</p> <p>Note that if you have not read the policy gradient methods in lesson 4 and 5. It is now time to go back and read the corresponding sections. The ideas of policy gradient method are covered in chapter 13 of our book.</p> <pre><code>class Actor_Critic(PG(vMDP)):\n\n    def step0(self):\n        self.\u03b3t = 1 # powers of \u03b3\n\n    # -------------------------------------- \ud83c\udf16 online learning ------------------------------\n    def online(self, s, rn,sn, done, a,_): \n        \u03c0, \u03b3, \u03b3t, \u03b1, \u03c4, t, \u0394V, \u0394Q = self.\u03c0, self.\u03b3, self.\u03b3t, self.\u03b1, self.\u03c4, self.t, self.\u0394V, self.\u0394Q\n\n        \u03b4 = (1- done)*\u03b3*self.V(sn) + rn - self.V(s)    # TD error is based on the critic estimate\n\n        self.w    += \u03b1*\u03b4*\u0394V(s)                         # critic v\n        self.W[a] += \u03b1*\u03b4*\u0394Q(s)*(1 - \u03c0(s,a))*\u03b3t/\u03c4       # actor  \n        self.\u03b3t *= \u03b3  \n</code></pre> <p>Note that since we use a separate \\(\\theta_a\\) for each action, where we have that </p> <p>\\(Q(s,a) = \\phi^\\top \\theta_a \\qquad\\) \\(\\pi(a|s,\\theta_a) = \\frac{e^{\\phi^\\top \\theta_a}}{\\sum_{b}{e^{\\phi^\\top \\theta_b}}}\\)</p> <p>then equation 13.9 becomes as follows</p> <p>\\(\\nabla_{\\theta_a} ln \\pi(a|s,\\theta_a) = \\phi (1-  \\pi(a))\\)</p> <p>In the book authors assumes that we are using a concatenated vector \\(\\theta\\) for all actions where each n weights represents one of the actions.</p> <pre><code>ac = Actor_Critic(env=vgrid(), \u03b1=1, \u03c4=.1, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>In the below (disabled cell) we can see that the agent arrives at a good policy but it takes long to reach the goal. The reason is because the action-values differences is not high enough to converge to a close to greedy policy. Enable the cell by pressing escape then y and run the cell to see the lengthy run despite an apparent and sound policy shown by the arrows.</p> <p>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03b3=.98, episodes=30, seed=0 , **demoQ()).interact()</p> <p>Below we solve the problem by decaying the exploration rate \u03c4 exponentially.</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03c4min=.05, d\u03c4=.5,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac.\u03c4\n</code></pre> <pre><code>0.05\n</code></pre> <p>we can also decay linearly \u03c4 as follows</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03c4min=.01, T\u03c4=500,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac.\u03c4\n</code></pre> <pre><code>0.01\n</code></pre> <p>However we can achieve a better results by using a combination of intermediate reward and higher learning rate. Let us start by setting a different reward.</p> <pre><code>ac = Actor_Critic(env=vmaze(reward='reward0'), \u03b1=.1, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Now let us increase the learning rate.</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.8, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Now let us do both</p> <pre><code>ac = Actor_Critic(env=vmaze(reward='reward0'), \u03b1=.8, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>When we increase exploration for this setting we get </p>"},{"location":"unit4/lesson14/lesson14.html#mountain-car-problem","title":"Mountain Car Problem","text":"<p>We tackle the mountain car problem. This is problem is continuous and attention is needed for the way we represent the states. It has a boundaries for the x position as [-1.2,  .5]. In order to deal with it we need to discretize the X space by dividing it into a number of intervals each has a size of \u03c9.</p>"},{"location":"unit4/lesson14/lesson14.html#continuous-problem-and-state-space-discretisation","title":"Continuous Problem and State Space Discretisation","text":"<p>Below we show how we can discretise the problem by rescaling its range. We show the boundaries as well as some intermediate x positions. Note that we have always nS-2 as the max of the discretized s where the goal location is.</p> <pre><code>def ind(x=.1, printX=False):\n    X0, Xn  = -1.2, .5 \n    ntiles = 16\n    nS = ntiles +1\n    X = np.linspace(X0, Xn, ntiles)\n    scX = (ntiles)/(Xn - X0) # scaled range\n\n    s = int(scX*(x - X0)) \n\n    if printX: print(np.round(X,2))\n    print('at x=%.2f we have, s=%d:'%(x,s))\n    print()\n\nind(x=-1.2, printX=True)  # boundary case: far left\nind(x=.5)                 # boundary case: far right\nind(x=.5-.001)             # boundary case: far right\nind(x=.1)                 # mid state\n</code></pre> <pre><code>[-1.2  -1.09 -0.97 -0.86 -0.75 -0.63 -0.52 -0.41 -0.29 -0.18 -0.07  0.05\n  0.16  0.27  0.39  0.5 ]\nat x=-1.20 we have, s=0:\n\nat x=0.50 we have, s=16:\n\nat x=0.50 we have, s=15:\n\nat x=0.10 we have, s=12:\n</code></pre> <p>Note how the index for the far end ==ntiles which means that the number of states is actually ntiles+1.</p>"},{"location":"unit4/lesson14/lesson14.html#additive-state-space","title":"Additive State Space","text":"<p>We have two continuous spaces that specify the position and the velocity state of our car so we need to find a way to combine the two discretised spaces. We start by developing a form of binary encoding that is additive. We will call the set of partitions\u2019 tiles to help the consistency of the cover. But bear in mind that additive spaces are not called tilings. This will help us gradually move towards tile coding to appreciate its work and why. Below is the code for animating and dealing with this problem. We have used an additive state space. So, we have concatenated the discrete representation of the position (nS) with the discrete representation for the velocity(ntiles) to make up one vector for representing the position and the velocity of the car where the vector has a size of nS+ntiles.</p> <p>Ok let us get started...</p> <pre><code>print(floor(9.5/3.1))\nprint(9.5//3.1)\n</code></pre> <pre><code>3\n3.0\n</code></pre> <pre><code>class MountainCar:\n    def __init__(self, ntiles=8,  **kw):   # ntiles: number of tiles \n        # constants                          \n        self.X0,  self.Xn  = -1.2, .5       # position range\n        self.Xv0, self.Xvn = -.07, .07      # velocity range\n        self.\u03b7 = 3                          # we rescale by 3 to get the wavy valley/hill\n\n        # for render()\n        self.X  = np.linspace(self.X0,  self.Xn, 100)     # car's position\n        self.Xv = np.linspace(self.Xv0, self.Xvn, 100)    # car's speed\n        self.Y  = np.sin(self.X*self.\u03b7)\n\n        # for state encoding (indexes)\n        self.ntiles  = ntiles\n        # number of states is nS*nSd but number of features is nS+nSd with an econdoing power of 2^(nS+nSd)&gt;&gt;nS*nSd!\n        self.nF = self.nS = 2*(self.ntiles+1)\n\n        self.nA = 3\n        # for compatability\n        self.Vstar = None\n        self.nsubplots=3\n\n        # reset\n        self.x  = -.6 + rand()*(-.4+.6)\n        self.xv = 0\n\n        # figure setup\n        self.figsize0 = (12, 2) # figsize0 is used for compatibility\n\n\n    # get the descritized position and velocity\n    def s(self, tilings=1):\n        return floor(tilings*self.ntiles*(self.x  - self.X0 )/(self.Xn  - self.X0 ))\n\n    def sv(self, tilings=1):\n        return floor(tilings*self.ntiles*(self.xv - self.Xv0)/(self.Xvn - self.Xv0))\n\n    def reset(self):\n        #self.goals = self.nF -1 # to make sure that it is updated if we update nF\n        self.x  = -.6 + rand()*(-.4+.6)\n        self.xv = 0\n        return self.s_()\n\n\n    def s_(self):       \n        \u03c6 = np.zeros(self.nF) \n        \u03c6[self.s()] = 1 \n        \u03c6[self.sv() + self.ntiles + 1] = 1 \n        return \u03c6\n\n    # for compatibility\n    def S_(self):\n        return np.eye(self.nF)\n\n    def isatgoal(self):\n        return self.x==self.Xn\n\n    def step(self,a):\n        a-=1       # to map from 0,1,2 to -1,0,+1\n        self.xv += .001*a - .0025*np.cos(self.\u03b7*self.x); self.xv = max(min(self.xv, self.Xvn), self.Xv0)\n        self.x  += self.xv;                              self.x  = max(min(self.x,  self.Xn ), self.X0 )\n\n        # reset speed to 0 when reaching far left\n        if self.x&lt;=self.X0:  self.xv = 0\n\n        return self.s_(), -1.0, self.isatgoal(), {}\n\n\n    def render(self,  visible=True, pause=0, subplot=131, animate=True, **kw):\n        if not visible: return\n\n        self.ax0 = plt.subplot(subplot)\n        plt.gcf().set_size_inches(self.figsize0[0], self.figsize0[1])\n\n        car = '\\\u014d\u0361\u2261o\u02de\u0336' # fastemoji\n        bbox = {'fc': '1','pad': -5}\n\n        X = self.X\n        Y = self.Y\n        \u03b7 = self.\u03b7\n\n        plt.plot(X+.1,Y, 'k')\n        plt.plot(X[-1]+.1,Y[-1]-.05,'sg')\n        plt.text(X[-1],Y[-1]+.2,'Goal', color='g', size=14)\n        plt.title('Mountain Car', size=20)\n        plt.axis(\"off\")\n\n        # plot the mountain car \n        # take the derivative of the terrain to know the rotation of the car to make it more realistic\n        rotation = np.arctan(np.cos(self.x*\u03b7))*90  \n        plt.text(self.x, np.sin(self.x*\u03b7)+.05, car, va='center', rotation=rotation,  size=13, fontweight='bold', bbox=bbox)\n\n        if animate: clear_output(wait=True); plt.show(); time.sleep(pause)\n</code></pre> <pre><code>mcar = MountainCar()\nmcar.render()\n</code></pre> <p></p> <pre><code>mcar = MountainCar()\nmcar.x  = mcar.X0\nmcar.xd = 0\n\nwhile not mcar.isatgoal():\n    mcar.step(2)\n    mcar.render()\n    #print(np.sin(mcar.x))\n</code></pre> <p></p> <pre><code>mcar.xd\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#studying-the-behaviour-of-the-car","title":"Studying the behaviour of the car","text":"<p>Let us see how the car behaves if we do not accelerated forward throttle=0, the car would depend on it mass and the inclination of the terrain and it would keep oscillating backwards and forward.</p> <pre><code>mcar.x  = mcar.X0\nmcar.xd = 0\n\nfor _ in range(200):\n    mcar.step(1)\n    mcar.render()\n</code></pre> <p></p> <p>The above was for starting on top of the left hill, but this is not the starting position of the car. The car always start at the bottom of the valley. Let us see how the car behaves if we just accelerated forward throttle=+1 when we start at the bottom of the valley.</p> <pre><code>mcar.reset()\nfor _ in range(200):\n    mcar.step(2)\n    mcar.render()\n</code></pre> <p></p> <p>Let us see how the car behaves when we start at the top of the right hill and when we just accelerated backwards throttle=-1.</p> <pre><code>mcar.x  = mcar.X[-2]\nmcar.xd = -1\n\nfor _ in range(200):\n    mcar.step(0)\n    mcar.render()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#optimal-solution","title":"Optimal solution","text":"<p>Obviously the optimal policy would be to accelerate forward and then backward to gain momentum to be able to reach the top since the car has not enough engine power to reach it by simple forward acceleration.</p> <pre><code>mcar.reset()\n\n# swing forward and backward to gain momentum and then go full speed on\nswing=24 # try 22 or less to see what happen\n# 1. accelerate forward  \nfor _ in range(swing):\n    mcar.step(2)\n    mcar.render()\n\n# 2. accelerate backward to take advantage from the momentum\nfor _ in range(swing):\n    mcar.step(0)\n    mcar.render()\n\n# 3. now full throttle forward to reach the goal\nfor _ in range(100):\n    mcar.step(2)\n    mcar.render()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#optimal-policy","title":"Optimal Policy","text":"<p>As we can see, the car must intelligently swing itself forward and then backwards before reaching the top. This kind of problem is hard to find a policy for using traditional control algorithms because things need to get worse (away from the goals) before getting better. The credit assignment plays on the long run rather than on immediate improvements.</p> <p>Note that in all what will come below, we set the learning rate to be \u03b1=()/8 because the default number of tiles is 8. Please differentiate between this and the number of tilings which we will vary, but it will also be set to 8.</p> <p>Ok let us start training</p>"},{"location":"unit4/lesson14/lesson14.html#short-number-of-episodes","title":"Short number of episodes","text":"<pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=50, seed=1, **demoQ()).interact()\n</code></pre> <pre><code>qlearn = Qlearn(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=50, seed=1, **demoQ()).interact()\n</code></pre> <p>Longer episodes</p> <pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.5/8,  \u03b5=0.1, episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>qlearn = Qlearn(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <p>Note that there is uncontrollable element of randomness due to the random starting position of the car that is part of the hardness of the problem. So each time you run even with the same seed you will get a different learning curve.</p>"},{"location":"unit4/lesson14/lesson14.html#exploration-by-optimistic-initialisation","title":"Exploration by optimistic initialisation","text":"<p>We use a reward scheme of -1 in every step with an initial value function of 0. These initial values are optimistic, given that the agent will be punished for every step before reaching the goal. Such settings naturally encourage the agent to explore the early episodes because it will be disappointed by the reward that it gets, so we do not need to adopt an exploratory policy explicitly, and we can set \u03b5=0. Let us see how the Sarsa act on such a purely greedy policy with optimistic initialisation to encourage exploration.</p> <pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.1/8,   \u03b5=0,  episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#sarsa-for-binary-encoding","title":"Sarsa for Binary Encoding","text":"<p>Let us make our Sarsa implementation more efficient for a binary encoding where each component in the state vector is either 0 or 1. This simplifies the implementation of the dot product to be turned into summation. below we show the implementation.</p> <pre><code>s = np.array([0, 2 ,0, 1, 0])\n# s = np.array([0, 0 , 0, 0, 0])\nW = np.array([[.1,.2 ,.3, .4, .5], [1,2 ,3, 4, 5]])\nS = np.where(s!=0)[0]\nprint(len(S))\nfor i in S:\n    print(i)\n\nprint(W[0,np.where(s!=0)[0]].sum(0).round(2))\nprint(W[1,np.where(s!=0)[0]].sum(0).round(2))\n\nprint(W[:,np.where(s!=0)[0]].sum(1))\n</code></pre> <pre><code>2\n1\n3\n0.6\n6.0\n[0.6 6. ]\n</code></pre> <pre><code>class SarsaB(vMDP):  # Binary Sarsa that deals with binary state representation (later will have multiple tilings so we would have more than one hot encoding but each tilings is a one-hot encoding)\n\n    def init(self):\n        super().init() # \u03b1=.8\n        self.store = False       \n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def Q_(self, s=None, a=None):\n        if s is None: return np.matmul(self.W, self.S).T \n        s_ = np.where(s!=0)[0] # set of indexes that represents states because we use a binary representation\n        Q = self.W[:,s_].sum(1) if a is None else self.W[a,s_].sum(0)\n        return Q if Q.size else 0\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        s_ = np.where(s!=0)[0]\n        self.W[a,s_] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#runs-on-mountain-car","title":"Runs on Mountain Car","text":"<p>Let us now see the Sarsa behaviour on several runs</p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=Sarsa(env=MountainCar(),\u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 32.7 s, sys: 77.5 ms, total: 32.8 s\nWall time: 32.8 s\n</code></pre> <p></p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=SarsaB(env=MountainCar(), \u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 1min 8s, sys: 843 ms, total: 1min 9s\nWall time: 1min 9s\n</code></pre> <p></p> <p>As we can see both runs plots are identical as expected. However, Sarsa is faster than the Binary Sarsa. The reason is as usual the dot product is highly optimised and unless we are dealing with a very large state space the usual Sarsa is more efficient.</p>"},{"location":"unit4/lesson14/lesson14.html#binary-mountain-car-class-and-binary-sarsa","title":"Binary Mountain Car Class and Binary Sarsa","text":"<p>If we guarantee that we send indices instead of a feature vector we might be able to reduce the overhead. Let us see how.</p> <pre><code>class SarsaB(vMDP):  # Binary Sarsa that deals with binary state representation (later will have multiple tilings so we would have more than one hot encoding but each tilings is a one-hot encoding)\n\n    def init(self):\n        super().init() # \u03b1=.8\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def Q_(self, s=None, a=None):\n        if s is None: return np.matmul(self.W, self.S).T \n        Q = self.W[:,s].sum(1) if a is None else self.W[a,s].sum(0)\n        return Q if Q.size else 0\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.W[a,s] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))\n</code></pre> <p>As we can see we got rid of the s_ = np.where(s!=0)[0] statement in both the Q_() and online() functions. This statements obtain the indexes of the features that are on. To guarantee that s contain the indexes we need to adjust the environment as follows.</p> <pre><code>class MountainCarB(MountainCar):\n    def __init__(self, **kw):\n        super().__init__(**kw)\n\n    def s_(self):\n        return [self.s(), self.sv()+ self.ntiles + 1]\n</code></pre> <p>Now let us run the same experiments on the new two classes MountainCarB and SarsaB.</p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=SarsaB(env=MountainCarB(), \u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 1min, sys: 550 ms, total: 1min 1s\nWall time: 1min 1s\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#mountain-car-runs","title":"Mountain Car Runs","text":"<p>Let us now develop a function to conduct several runs on the mountain car problem to see how the different state representations perform with different learning rates.</p> <pre><code>def MountainCarRuns(runs=20, algo=Sarsa, env=MountainCar(), label='', \u03b5=0):\n    for \u03b1 in [.1, .2, .5]:\n        sarsaRuns = Runs(algorithm=algo(env=env, \u03b1=\u03b1/8, episodes=500, \u03b5=\u03b5),\n                         runs=runs, seed=1, plotT=True).interact(label='\u03b1=%.2f/8'%\u03b1)\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.title('Semi Gradient ' + algo.__name__  +' on Mountain Car '+label)\n\nfig_10_2 = MountainCarRuns\n</code></pre> <pre><code>%time MountainCarRuns()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 2min 49s, sys: 470 ms, total: 2min 49s\nWall time: 2min 50s\n</code></pre> <p></p> <p>As we can see, the algorithm shows some signs of underperformance and instability. This is a common problem when we have less representational capabilities in our state space due to the additivity nature of our space.</p> <p>The adopted additive representation is ok, but it has its limitations. The main issue is that the total number of possible states is \\(ntiles \\times ntiles\\), but the dimensionality of our vector is far less than that it is ntiles+ntiles. It is not a one-hot encoding, so each component does not correspond to one state. Instead, two components correspond to one state. These two issues combined reduced the capability of our algorithms to generalise to nearby states.</p>"},{"location":"unit4/lesson14/lesson14.html#multiplicative-state-space-tile-coding-for-mountain-car","title":"Multiplicative State Space: Tile Coding for Mountain Car","text":"<p>In this section, we alter the state representation to have multiplicative space with a vector of size \\(ntiles \\times ntiles\\) to have a state dimensionality that matches the discrete space dimensionality. In effect, we return to one-hot encoding with each component corresponding with one state (of position and velocity). </p> <p>Hence, we will cover the space with a grid of 2-d instead of the two 1-d arrays of position and velocity we concatenated earlier. This will make the space dimension \\(ntiles \\times ntiles\\). The benefit is that we would have one hot encoding instead of the 2-hot encoding. Usually, in RL, one-hot encoding outperforms other encoding and is tried and tested in many applications. </p> <p>When we use multiple tilings, the space is \\(ntiles \\times ntiles \\times ntilings\\) where ntilings is the number of tilings. When we move to multiple tilings, we may have abandoned the one-hot encoding. However, we have made the state vector's total representation power multiple times what the discretised space is. This redundancy is crucial and works best for RL. </p> <p>This is similar to the idea of increasing the dimensionality in kernel methods which allowed us to gain more power in state representation, and then we found computationally convenient ways of reducing the computing demands via the kernel trick. Here, we also increase the space dimensionality via adopting multiple tilings, but then we reduce its computing demands by the hashing trick. So, we hash the bigger multiplicative state space into a fixed table which improves the efficiency of our algorithms and guarantees bounded memory and time complexities.</p> <p>Below we show how to change the representation of our space to a 2-d which greatly increases its dimensionality due to multiplication. But first, we start off by showing the effect of the rescaling, and then we show the implementation.</p> <pre><code>ntilings = ntiles = 5\n\nX0, Xn  = -1.2, .5\nscaling = (ntilings*ntiles)/(Xn-X0)\n\nx = X0                # left boundary state\nx = X0 + (Xn- X0)/2   # mid state\n# x = Xn                # right boundary state\n\ntiling = 0  # any value between 0..ntilings-1\nprint(int((x-X0)*scaling + tiling )//ntilings)\n</code></pre> <pre><code>2\n</code></pre> <p>Let us see also how the tilings and tiles indexes works together.</p> <pre><code>inds = []\n# ntilings = ntiles = 3\nds_s = 1  # displacement for s\nds_sv = 3 # displacement for sv\nfor tiling in range(ntilings):\n    s =  int((x-X0)*scaling + ds_s*tiling )//ntilings\n    sv = int((x-X0)*scaling + ds_sv*tiling )//ntilings\n    inds.append((tiling,s,sv))\nprint(inds)\n</code></pre> <pre><code>[(0, 2, 2), (1, 2, 3), (2, 2, 3), (3, 3, 4), (4, 3, 4)]\n</code></pre> <p>ok let us construct a feature vector and turn it corresponding indices</p> <pre><code>\u03c6 = np.zeros((ntilings, ntiles+ds_s, ntiles+ds_s+ds_sv))\nfor ind in inds: \n    \u03c6[ind]=1\n\n# we will print one tiling but feel free to print them all\ntiling=2 # any value between 0..ntilings-1\nprint(\u03c6[tiling])\n# print(\u03c6)\n</code></pre> <pre><code>[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n</code></pre> <p>As we can see the corresponding indices have been turned on. </p> <p>We can now flatten the feature vector \u03c6.</p> <pre><code>\u03c6.flatten()\n</code></pre> <pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n</code></pre> <p>Note how we have exactly ntilings features turned on and the rest are 0. </p> <p>Ok so now we are ready to develop our tiled Mountain Car class.</p> <pre><code>class tiledMountainCar(MountainCar):\n    def __init__(self, ntilings=1, **kw): # ntilings: is number of tiles\n        super().__init__(**kw)\n        self.ntilings = ntilings\n        self.dim = (self.ntilings, self.ntiles+1, self.ntiles+3) # the redundancy to mach the displacements of position(x) and velocity(xv)\n        self.nF = self.dim[0]*self.dim[1]*self.dim[2]\n\n\n    def inds(self):\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        inds = []\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n            inds.append((tiling,s,sv))\n\n        return inds\n\n    def s_(self):\n        \u03c6 = np.zeros(self.dim)\n        for ind in self.inds(): \u03c6[ind]=1\n        return \u03c6.flatten()\n</code></pre> <p>As we can see, the implementation is simple we rescale x and xv, add the current tiling and divide the result by the number of tilings. In effect, this creates multiple space partitions, each shifted by (1*\u03c9/n)*i for the car position x by (3*\u03c9v/n)*i for the car velocity xv. </p> <p>We have rescaled the entire position, and velocity ranges into ntiles \\(\\times\\) ntilings. You can think of the number of tiles as centimetres and the nilings as millimeters on a ruler. The ruler may be 16 tiles (centimetres) long. Each tile is divided into 10 tilings(millimetres), but we have 10 rulers (the number of tilings is the same as the number of pieces each tile is divided into). Then when we want to know the encoding, we measure on the ruler where the input x is to obtain its tile. The first ruler starts at the start of the x range. Then we pick another ruler and offset its starting position by 1 millimetre (1 tiling), and we check the measure of our x on the second ruler to obtain its tile on the second ruler (on the second tiling), and we repeat the same process for all 10 rulers. </p> <p>We get a set of 10 active tiles on the 10 rulers, these will be our indexes that will be turned on in our state vector, and the rest are 0s. Our state vector size is ntiles x ntiles x n. The 1 and 3 are displacements to make the offset asymmetrical, which helps the generalisation (see figure 9.11 in the book for more details).</p> <pre><code>def SarsaOnMountainCar(ntilings, env=tiledMountainCar):\n    sarsa = Sarsa(env=env(ntilings=ntilings), \u03b1=.5/ntilings, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='ntilings=%d'%ntilings)\n    plt.gcf().set_size_inches(20,4)\n    plt.ylim(100,1000)\n</code></pre> <pre><code>for n in trange(5):\n    SarsaOnMountainCar(ntilings=2**n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:15&lt;00:00,  3.00s/it]\n</code></pre> <p></p> <p>Note the difference between the max steps and the max steps when we did not use tile coding. We will demonstrate the  advantage of tile coding further in a more extensive experiments.</p>"},{"location":"unit4/lesson14/lesson14.html#tiled-coded-mountain-car-runs","title":"Tiled coded mountain car runs","text":"<p>Let us see how the new tile coding behaves with respect to different learning rates, to do we simply call the SarsaMCar() function but we pass the tiledMountainCar environment with n=8.</p> <pre><code>MountainCarRuns(env=tiledMountainCar(ntilings=8), \u03b5=0, label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <p>Note that since we set  \u03b5=0 and used optimistically initialisation, these settings made Sarsa closer to Q-learning in the sense that it is learning about a purely greedy policy (of course Q-learning is acting usually according to an exploratory policy  \u03b5-greedy). Let us see how Q-learning behaves if we also set \u03b5=0.</p> <pre><code>MountainCarRuns(env=tiledMountainCar(ntilings=8), \u03b5=0.1, label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>MountainCarRuns(algo=Qlearn, env=tiledMountainCar(ntilings=8), label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <p>Indeed as we can see .\u215d seems to be performing best for a additive one tile coding (the above class implemented one-hot coding which is a special case of tile coding where n=1). In the next section we implement a more generic state representation that allows for multiple tilings.</p>"},{"location":"unit4/lesson14/lesson14.html#binary-tiled-mountain-car","title":"Binary Tiled Mountain Car","text":"<pre><code>class tiledMountainCarB(tiledMountainCar):\n    def __init__(self, **kw): #ntilings: is number of tiles\n        super().__init__(**kw)\n\n    def s_(self):\n        inds=[]\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n            ind = (tiling*self.dim[1] + s)*self.dim[2] + sv\n            inds.append(ind)\n\n        return inds\n</code></pre> <pre><code>MountainCarRuns(algo=SarsaB, env=tiledMountainCarB(ntilings=8), label='Binary Mountain Car with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p>As we can see again the performance is the same with an extra cost overhead in terms of time.</p>"},{"location":"unit4/lesson14/lesson14.html#multiplicative-state-space-hashed-tile-coding-for-mountain-car","title":"Multiplicative State Space: Hashed Tile Coding for Mountain Car","text":"<p>This section draws on the idea of hashing a tile coding representation to reduce its dimensionality to make it more efficient. To solve the issues of high dimensionality when using multiple tilings, we can resort to the idea of hashing, which will help us to maintain a high resolution but reduce the space dimensionality. It does that by fixing the state space dimensionality regardless of the discretised space. </p> <p>It takes advantage of the fact that not all states will be visited as often, and when the agent starts exploring, the earlier states will be guaranteed to have unique encoding. Only later on, for less frequent states when the hashing table runs out of spaces, will there be some colliding (i.e. two states would have the same encoding). This violates a uniqueness of representation which is an explicit condition for several algorithms or similarity of representation for close states. However, such an effect is minimal compared to the benefits of redundancy of the original discretised space. The disadvantage can be ignored given that we made our table large enough.</p> <p>Below we show a basic implementation of hashing. The idea is very simple; we hash the index of the feature that we were turning on in the tiledMountainCar class. In other words, we take the two indexes of position and velocity, along with the tiling index, and we hash the three indexes as a tuple. Then we apply the modulus % with the hash_size (same as the number of features) to obtain the index of the feature that we want it to be active. Note that the hashing function gives us some number that represents the tuple. We will get the same number each time we pass the same tuple. That is all that is being provided by the built-in hashing function. However, some different tuples might have the same modulus, which will cause collision even when the table size is larger than the state space. We will try to avoid this collision issue later. At least, we will minimise it so that it occurs only when the state space exceeds the table size.</p> <pre><code>class hashedtiledMountainCar(tiledMountainCar):\n    def __init__(self, hash_size=1024,**kw): \n        super().__init__(**kw)\n        self.nF = hash_size # fixed size that does not vary with the ntilings* ntiles\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        for ind in self.inds():\n            \u03c6[hash(ind)%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>%time sarsa = Sarsa(env=hashedtiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='hased tiled')\n%time sarsa = Sarsa(env=      tiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='tiled')\n\nplt.gcf().set_size_inches(20,4)\nplt.ylim(100,1000)\n</code></pre> <pre><code>CPU times: user 3.03 s, sys: 8.76 ms, total: 3.04 s\nWall time: 3.05 s\n\n\n\n\n\n(100.0, 1000.0)\n</code></pre> <p></p> <p>Note that there are some differences in the results of the last two steps graphs due to the issue of collisions that was discussed above.</p> <pre><code>%time MountainCarRuns(env=hashedtiledMountainCar(ntilings=8, ntiles=8, hash_size=8*8*8), label='with hashed 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 3min 26s, sys: 518 ms, total: 3min 26s\nWall time: 3min 27s\n</code></pre> <p></p> <p>Let us reduce the hash table size (i.e. the number of features) used to represent a state. Once we have less representation power we expect deterioration in performance. Note that we still are dealing with 8x8x8 tiled coding we are reducing the capability of the hash table to accommodate all of the discretised states with no collision.</p> <pre><code>%time MountainCarRuns(env=hashedtiledMountainCar(ntilings=8, ntiles=8, hash_size=8*8*4), label='with hashed 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 4min 4s, sys: 821 ms, total: 4min 5s\nWall time: 4min 6s\n</code></pre> <p></p> <p>As we can see the performance of the model deteriorated a bit when we reduced the hash table size due to collision.</p>"},{"location":"unit4/lesson14/lesson14.html#index-hashed-table-iht","title":"Index Hashed Table (IHT)","text":"<p>Directly using the hash function on upcoming tuples of indexes is fine, but it loses an opportunity to store nearby states in nearby locations and to guarantee consistency in allowing for collision. A collision occurs when two tuples have the same index values. This means that two states will have the same representation, and the uniqueness of representation is violated. This can be a last resort for a very large state space, but it is better not to for small to medium spaces. </p> <p>The index hash table allows us to store the tuples as values (in a dictionary) and their indices as keys. Thanks to the dictionary structure in Python, we can retrieve the index of a tuple at any point with O(1) cost to do. What is left is then to only resort to hashing a key when we cannot store it anymore in the table, and in this case, we need to live with collision. Below we show a simplified and more concise implementation. For more details, refer to hash tile coding, described in our textbook. We built our code differently to avoid storing the indices explicitly in a table for more efficiency. The standard software has extra capabilities we do not need here (ex. adding the action index is unnecessary since we do not amalgamate all the weights of different actions in one large weight vector).</p> <pre><code>class IHTtiledMountainCar(tiledMountainCar):\n    def __init__(self, iht_size=1024, **kw): # by default we have 8*8*8 (position tiles * velocity tiles * tilings)\n        super().__init__(**kw)\n        self.nF = iht_size\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        inds = np.where(super().s_()!=0)[0]\n        \u03c6[inds%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>10*11*8\n</code></pre> <pre><code>880\n</code></pre> <pre><code>%time sarsa = Sarsa(env=IHTtiledMountainCar(ntilings=8, ntiles=8), \\\n                \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='IHT tilings')\n\n%time sarsa = Sarsa(env=tiledMountainCar(ntilings=8, ntiles=8), \\\n                \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='tilings')\n\n# %time sarsa = Sarsa(env=hashedtiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='hashed tilings')\n\nplt.ylim(100, 1000)\n</code></pre> <pre><code>CPU times: user 2.93 s, sys: 3.14 ms, total: 2.93 s\nWall time: 2.93 s\n\n\n\n\n\n(100.0, 1000.0)\n</code></pre> <p></p> <p>Note how the two graphs are identical, although we have used an index hashing table (IHT) in the second while we did not in the first. This is because IHT maps the indices of the visited states, and unless the number of states exceeds the IHT size, the results will be identical to those with no hashing. This is the major advantage of IHT over just hashing. The downside is that it can be computationally more expensive to use IHT, so the equivalence is with an overhead cost. However, in our implementation, we have avoided this overhead completely!</p> <pre><code>MountainCarRuns(runs=10, env=IHTtiledMountainCar(ntilings=8,ntiles=8), label='with index hashed table for 8*8*8 tiles')\nMountainCarRuns(runs=10, env=   tiledMountainCar(ntilings=8,ntiles=8), label='with 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#collisions-in-a-hash-representation","title":"Collisions in a Hash Representation","text":"<p>Note that the actual state dimension is tiles \\(\\times\\) tiles  \\(\\times\\) number of tilings: since we have the 8 tiles for the positions and 8 for the velocity (8 by 8 grid) and 8 tilings:</p> <pre><code>8*8*8\n</code></pre> <pre><code>512\n</code></pre> <p>So we could get away with 512 without collision in the hash table. Collision occurs whenever we use the same key for more than one state. We scarify some accuracy due this collision. So unless it is necessary we try to avoid it by being generous in the memory allocation for the hash table (same as our feature vector). We may allow  some collision for larger space on the base that important states will be visited more often and hence their representation will overshadow other less frequent states.</p>"},{"location":"unit4/lesson14/lesson14.html#tilings-comparison","title":"Tilings Comparison","text":"<p>In this section we compare between the performance of Sarsa on different tilings to see the effect of changing the power of state representation.</p> <pre><code>def MountainCarTilings(runs=20, \u03b1=.3, algo=Sarsa, env=tiledMountainCar):\n\n    plt.title('Sarsa on mountain car: comparison of different tilings with \u03b1=%.2f/8'%\u03b1)\n    for ntilings in [2, 4, 8, 16, 32]:\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=ntilings),\u03b1=\u03b1/ntilings,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='%d tilings'%ntilings)\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\n</code></pre> <pre><code>MountainCarTilings()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#comparing-n-step-sarsa-on-mountain-car-with-different","title":"Comparing n-Step Sarsa on Mountain Car with different  \u03b1","text":"<p>Let us compare systematically how different number of steps plays with the learning rate \u03b1 for the mountain car problem. We will use hashed tilings with 8 tiles and 8 tilings. The goal is to confirm whether the behaviour of n-step Sarsa for control is compatible with the behaviour of n-step TD for prediction. </p> <p>Below we show the implementation of n-step Sarsa with function approximation. It is identical to the tabular form except we use the function Q(sn,an) instead of Q[sn,an].</p> <pre><code>class Sarsan(vMDP):\n\n    def init(self):\n        super().init()\n        self.store = True        # although online but we need to access *some* of earlier steps,\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, *args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c41 = \u03c4+1\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n\n        s\u03c4 = self.s[\u03c4];  a\u03c4 = self.a[\u03c4]\n        sn = self.s[\u03c4n]; an = self.a[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.W[a\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1-done)*self.\u03b3**n *self.Q(sn,an) - self.Q(s\u03c4,a\u03c4))*self.\u0394Q(s\u03c4)\n</code></pre> <p>Below we see that the an intermediate n (number of steps in n-step Sarsa) acts best in similar to the effect of the number of steps on random walk and that control algorithms with intermediate bootstrapping usually performs best.</p> <pre><code>def MountainCarTiledRuns_n(runs=20, algo=Sarsan, env=tiledMountainCar):\n\n    plt.title(algo.__name__+' on mountain car: comparison of n-steps with the same 8x8x8 tilings')\n    for n, \u03b1 in zip([1, 8], [.5, .3]):\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=8), n=n, \u03b1=\u03b1/8,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='%d step-Sarsa, \u03b1=%.2f/8'%(n,\u03b1))\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\nfigure_10_3_n = MountainCarTiledRuns_n\n</code></pre> <pre><code>%time figure_10_3_n(env=tiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 1s, sys: 231 ms, total: 2min 1s\nWall time: 2min 1s\n</code></pre> <pre><code>%time figure_10_3_n(env=hashedtiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 10s, sys: 249 ms, total: 2min 11s\nWall time: 2min 11s\n</code></pre> <pre><code>%time figure_10_3_n(env=IHTtiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 28s, sys: 270 ms, total: 2min 28s\nWall time: 2min 28s\n</code></pre> <p>As we can see the IHT performed exactly the same as the tiledMountainCar representation since we are using a large enough table_size, it gave a boost to the one-step Sarsa to become closer to the 8-step Sarsa.</p>"},{"location":"unit4/lesson14/lesson14.html#model-selection-for-n_step-sarsa","title":"Model Selection for n_step Sarsa","text":"<p>In this section we conduct an extensive set of experiments on Mountain Car with different learning rates in a similar manner to the study that we conducted for TD with random walk problem.</p> <pre><code>def MountainCarTiledCompare_n(runs=5, ntilings=8,  env=IHTtiledMountainCar): # 10\n\n    xsticks = np.array([0, .5 , 1, 1.5, 2, 2.3])/ntilings\n    plt.xticks(ticks=xsticks, labels=xsticks*ntilings)\n    plt.yticks([220, 240, 260, 280, 300])\n    plt.ylim(210, 300)\n    plt.title('Steps per episode averaged over first 50 episodes')\n\n    for n in range(5):\n        if n==0: \u03b1s = np.arange(.4,  1.8,  .1)\n        if n==1: \u03b1s = np.arange(.2,  1.8,  .1)\n        if n==2: \u03b1s = np.arange(.1,  1.8,  .1)\n        if n==3: \u03b1s = np.arange(.1,  1.2,  .07)\n        if n==4: \u03b1s = np.arange(.1,  1.0,  .07)\n\n        Compare(algorithm=Sarsan(env=env(ntiles=8, ntilings=ntilings), n=2**n, episodes=50, \u03b5=0), runs=runs, \n                                  hyper={'\u03b1':\u03b1s/ntilings}, \n                                  plotT=True).compare(label='%d-step Sarsa'%2**n)\n    plt.xlabel(r'$\\alpha \\times 8$ since we used 8 tiles for each tilings')\n    plt.show()\n\n\nfigure_10_4_n = MountainCarTiledCompare_n\n</code></pre> <p>Note that we always divided the learning rates \u03b1s by ntilings to match the amount of changes for different number of tilings so that we end up with an update magnitude that is compatible with a one-hot encoding.</p> <pre><code>%time MountainCarTiledCompare_n()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|14/14\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|13/13\n</code></pre> <p></p> <pre><code>CPU times: user 4min 51s, sys: 588 ms, total: 4min 52s\nWall time: 4min 52s\n</code></pre> <p>Note that we have not used hashing and hence our range is different from the one stated in the book. Nevertheless, the pattern is maintained.</p>"},{"location":"unit4/lesson14/lesson14.html#conclusion","title":"Conclusion","text":"<p>In this lesson, you saw how to deal with a continuous state space using function approximation and how to apply previous concepts to a more difficult control problem. We saw how to apply the tile coding technique we covered in the previous lesson on a continuous control Mountain car task. Along the way, we developed a new binary algorithm, Binary Sarsa, that is suitable for dealing with binary encoding, and we will study its performance. We present three representations of the problem. The first just discretised the space and used a vector representation that corresponds with this discretisation. This representation is equivalent to using one tiling in tile coding. Then we developed this representation to use multiple tiling that offset each other to enrich the representation capability for our continuous space. We then reduced the extra overhead introduced by the tile coding using hashing. We show that hashing is a powerful technique, and we studied a simple and efficient implementation of it. We concluded by updating our Sarsa(n) algorithm and applying it to the mountain car to compare its performance with different n. In the next lesson, we will show a new set of algorithms that achieve a similar or better performance without waiting for n steps.</p>"},{"location":"unit4/lesson14/lesson14.html#your-turn","title":"Your turn","text":"<ol> <li>run the MountainCarTilings but this time fixed the divider on 8 (instead of dividing by ntilings), observe the results and compare them to the original experiments, post what you can infer in the group discussion.</li> </ol>"},{"location":"unit4/lesson14/lesson14.html#challenge","title":"Challenge","text":"<ol> <li>Implement a state rep that directly use the software library provided by the book here.</li> <li>Can you think of a way to make the Mountain Car class works for the tabular case(assume that we have one tiling )</li> </ol>"},{"location":"unit5/lesson15/lesson15.html","title":"15. Linear Approximation with Eligibility Traces(prediction and control)","text":"<p>Unit 5: Learning Outcomes By the end of this unit, you will be able to:  </p> <ol> <li>Predict the value function for a policy using function approximation techniques.  </li> <li>Explain eligibility traces and the trade-offs associated with their depth.  </li> <li>Implement control methods that infer an agent\u2019s policy from an action-value function with function approximation.  </li> <li>Apply RL techniques to control a robotic system.  </li> </ol> <p>In this unit we continue our coverage of function approximation to try to achieve muti-step update effect by performing one update only via the nifty ideas of elegibility traces. We then move into utilising nonlinear function approximation to perfrom prediction and control in RL settings.</p>"},{"location":"unit5/lesson15/lesson15.html#lesson-14-eligibility-traces-for-approximate-prediction-and-control","title":"Lesson 14: Eligibility Traces for Approximate Prediction and Control","text":"<p>Learning outcomes</p> <ol> <li>understand the basic idea of the forward offline \u03bb-return algorithm and its average all possible n-step updates</li> <li>understand the basic idea of a backward implementation of \u03bb-return in an online algorithm via eligibility traces </li> <li>understand how eligibility traces achieve a trade-off between full bootstrapping TD(0) and no bootstrapping MC=TD(1)</li> <li>appreciate that eligibility traces are done on the component level when dealing with function approximation</li> </ol> <p>Reading: The accompanying reading of this lesson is chapters 10 and 12 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson tackles eligibility traces as a new algorithmic mechanism to achieve more in our online updates. This mechanism allows us to efficiently implement RL algorithms that perform n-step bootstrapped learning without waiting for or accessing the previous n steps. Instead, we maintain a set of parameters updated regularly online each time an agent visits a state. Then this vector representing recently visited states will be sued to update the weights instead of the simple state vector. Eligibility traces are a powerful trick often used in classical RL. Ok, let us get started...</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rlln import *\n</code></pre> <p>Note that we imported the Grid and the other environments from the Function approximation lesson because we are dealing with vectorised environment form now on.</p>"},{"location":"unit5/lesson15/lesson15.html#prediction-with-eligibility-traces","title":"Prediction with Eligibility Traces","text":""},{"location":"unit5/lesson15/lesson15.html#td","title":"TD(\u03bb)","text":"<p>TD(\u03bb) uses eligibility traces which allow us to implement the \u03bb-return algorithm more efficiently in a straightforward and online manner! Refer to section 12.2. This is an online algorithm. Surprisingly, we can obtain the average of an offline set of all n-step TD updates. The equivalence is only guaranteed for the offline case. Nevertheless, the online case works just fine. The next section will tackle the true online, which addresses this shortcoming.  The accumulating eligibility trace takes the form. </p> <p>\\(z_{t} = \\gamma\\lambda z_{t-1} + \u2207\\hat{v}(S_t, w_t)\\)</p> <p>while for the update, we replace the gradient with the eligibility vector. Note that this type of eligibility trace has the same gradient requirement regarding storage and processing time.</p> <p>\\(\\delta_{t} = R_{t+1} + \\gamma \\hat{v}(S_{t+1},w_t) - \\hat{v}(S_t,w_t)\\)</p> <p>\\(w_{t+1} = w_{t} + \\alpha\\delta_{t}z_{t}\\)</p> <p>The eligibility traces need to be initialised at the start of each episode, and we achieve that by using our step0 function.</p> <pre><code>class TD\u03bb(vMRP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n\n    def step0(self):\n        self.z = self.w*0\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n        self.z = \u03bb*\u03b3*self.z + self.\u0394V(s)\n        self.w += \u03b1*(rn + (1-done)*\u03b3*self.V(sn) - self.V(s))*self.z\n</code></pre> <pre><code>TD\u03bbwalk = TD\u03bb(env=vrandwalk(), \u03bb=.9, \u03b1=.03, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>TDwalk = TD(env=vrandwalk(), \u03b1=.03, episodes=100,  v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>As we can see TD(\u03bb) can converge faster than TD as it compresses several TD updates in each of its updates. This leads to the jittery behaviour that usually makes us reduce the learning rate but we can achieve more updates in less episodes in general. The other downside is that we have to tune the hyper parameter \u03bb which can take more effort. </p>"},{"location":"unit5/lesson15/lesson15.html#true-online-td-and-dutch-traces","title":"True Online TD(\u03bb) and Dutch Traces","text":"<p>We implement the True online TD(\u03bb) algorithm in this section. This algorithm guarantees an equivalence between the online and the offline cases, and it was hailed as an important step towards implementing a multi-step prediction algorithm that is both efficient and sound. in particular, it guarantees an equivalence between the offline \u03bb-return algorithm (which takes the average of all possible n-step updates) and the true online TD(\u03bb)! The justification or proof is quite involved. Please refer to sections 12.5 and 12.6 of the book. you can also have a look at the original true TD paper and subsequent paper the latter is packed with useful information.</p> <p>The drawback is that it can only be applied on a linear approximation and not on any general one as TD(\u03bb), but it is a step in the right direction until researchers find a way to do it also for the nonlinear function approximator as well (such as neural networks). </p> <p>We need to handle an eligibility trace variable (Dutch trace), the size of which is identical to the weights. The update rule is given in equation 12.11, and its justification/deduction can be read from section 12.5. which shows the formula for Dutch traces.</p> <p>Dutch traces are available when we use linear function approximation. They perform better than the classical accumulated eligibility traces. They take the form of </p> <p>\\(z_{t} = \\gamma\\lambda z_{t-1} + (1-\\alpha\\gamma\\lambda z_{t-1}^\\top x_{t})x_{t}\\)</p> <p>The update takes the form:</p> <p>\\(\\delta_{t} = R_{t+1} + \\gamma \\hat{v}(S_{t+1},w_t) - \\hat{v}(S_t,w_t)\\)</p> <p>\\(w_{t+1} = w_{t} + \\alpha\\delta_{t}z_{t} + \\alpha( w_{t}^\\top x_{t} - w_{t-1}^\\top x_{t})(z_{t} - x_{t})\\)</p> <p>When we implement the true online TD(\u03bb) algorithm, we save computation by saving the result of \\(vo=w_{t-1}^\\top x_{t}\\) instead of storing \\(w_{t-1}\\).</p> <pre><code>class trueTD\u03bb(vMRP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n\n    def step0(self):\n        self.z = self.w*0\n        self.vo = 0\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n\n        self.v = self.V(s)\n        self.vn= self.V(sn)*(1-done)\n        \u03b4 = rn + \u03b3*self.vn - self.v\n        self.z = \u03bb*\u03b3*self.z + (1-\u03b1*\u03bb*\u03b3*self.z.dot(s))*s\n\n        self.w += \u03b1*(\u03b4 + self.v - self.vo )*self.z - \u03b1*(self.v - self.vo)*s\n        self.vo = self.vn\n</code></pre> <pre><code>trueTD\u03bbwalk = trueTD\u03bb(env=vrandwalk(), \u03b1=.03, \u03bb=.8, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>trueTD\u03bbwalk = trueTD\u03bb(env=vrandwalk(), \u03b1=.03, \u03bb=.4, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>TDwalk = TD(env=vrandwalk(), episodes=100, \u03b1=.03,  v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>As we can see true online TD(\u03bb) can converge faster than TD. Again, similar to TD(\u03bb) this leads to the jittery behaviour that usually entails reducing the learning rate, but we can achieve more updates in less episodes in general. The other downside is that we have to tune the hyper parameter \u03bb which can take more effort. This leads to our next set of experiments in order to study how the algorithms reacts to both \u03bb and \u03b1 and whether there is a sweet spot that one can aim for.</p>"},{"location":"unit5/lesson15/lesson15.html#model-selection-for-truetd-tuning-and","title":"Model selection for trueTD\u03bb: Tuning \u03bb and \u03b1","text":"<p>We will test our true online TD on the random walk with function approximation. Due to the dot product, some of the learning rate \u03b1 might be too much for the algorithm when using high \u03bb values, and they will cause overflow. This is not a problem as we are just testing the limits of the algorithm, but to avoid doing so, we added a conditional statement to limit the values of \u03b1 for high \u03bb values.</p> <pre><code>def TD\u03bb_MC_Walk_\u03b1compare(algorithm=TD\u03bb, label='TD(\u03bb)', runs=10):\n\n    steps0 = list(np.arange(.001,.01,.001))\n    steps1 = list(np.arange(.011,.2,.02))\n    steps2 = list(np.arange(.25,1.,.05))\n\n    \u03b1s = np.round(steps0 +steps1 + steps2, 2)\n    #\u03b1s = np.arange(0,1.05,.1) # quick testing\n\n    plt.xlim(-.02, 1)\n    plt.ylim(.24, .56)\n    plt.title('%s RMS error Average over 19 states and first 10 episodes'%label)\n    for \u03bb in [0, .1, .4, .8, .9, .95, .975, .99, 1]:\n        end=34 if \u03bb&lt;.975 else (-3 if \u03bb&lt;.99 else -10)\n        compare = Compare(algorithm=algorithm(env=vrandwalk_(), v0=0, \u03bb=\u03bb, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s[:end]}, \n                                  plotE=True).compare(label='\u03bb=%.3f'%\u03bb)\n\n    if algorithm==trueTD\u03bb:\n        compare = Compare(algorithm=MC(env=vrandwalk_(), v0=0, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s}, \n                                  plotE=True).compare(label='MC \u2261 TD(\u03bb=1)', frmt='-.')\n</code></pre> <p>Ok let us now apply the TD(\u03bb) on our usual random walk to see if it behaves as expected. We are using the default vectroised Grid which is equivalent to the tabular case. </p> <pre><code>TD\u03bb_MC_Walk_\u03b1compare(TD\u03bb, runs=5)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n</code></pre> <p></p> <p>Ok that seems good and is follows to the usual pattern of n-step algorithm. Note that we have executed for a couple of runs which explains the overshooting of the figure boundaries. Let us now contrast TD(\u03bb) and true online TD(\u03bb) on the same random walk problem. </p> <pre><code>def TD\u03bb_vs_trueonlineTD\u03bb(runs):\n    plt.figure(figsize=[15,4]).tight_layout()\n\n    plt.subplot(121); TD\u03bb_MC_Walk_\u03b1compare(TD\u03bb, label='TD(\u03bb)', runs=runs)\n    plt.subplot(122); TD\u03bb_MC_Walk_\u03b1compare(trueTD\u03bb, label='true online TD(\u03bb)', runs=runs)\n</code></pre> <p>This time we will run both for longer runs.</p> <pre><code>TD\u03bb_vs_trueonlineTD\u03bb(runs=5)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n</code></pre> <p></p>"},{"location":"unit5/lesson15/lesson15.html#notes-on-the-convergence-of-td-with-function-approximation","title":"Notes on the convergence of TD with Function Approximation","text":"<p>As we can see, the true online has more stretched curves indicating that they are more stable with a wider range of learning rates. This is due to the equivalence between the online and offline cases, which guarantees convergence and more stability than TD(\u03bb). </p> <p>Note that TD(\u03bb)is proven to converge for the offline case, not the online case. True online TD(\u03bb) also performs slightly better than TD(\u03bb). Note, of course, that when \u03bb=0, we go back to our usual TD algorithm. Finally, note that TD(0) with linear function approximation is proven to converge under the usual conditions of stochastic approximation theory 2.7 the learning rate \u03b1 ($\\sum_{i=1}^{\\infty}\\alpha_i = \\infty $ and $\\sum_{i=1}<sup>{\\infty}\\alpha_i</sup>2 &lt;\\infty $), see page 206 Proof of Convergence of Linear TD(0) in the book.</p>"},{"location":"unit5/lesson15/lesson15.html#control-with-eligibility-traces","title":"Control with Eligibility Traces","text":"<p>We move next to the control realm and write a Sarsa(\u03bb) with an approximate value function. The idea is similar to Sarsa(0) that we have written in the previous lesson (we called is just Sarsa there). We need, however, this time to take care of an extra variable that holds the eligibility traces, which has a size identical to the weights, so each action has its own eligibility trace. </p> <p>The eligibility traces need to be initialised at the start of each episode, and we achieve that by using our step0 function. The update is per equation 12.5 in the book, but we must apply it for the eligibility trace corresponding to the current action.</p> <p>Note that the book assumes that we concatenate all the eligibility traces by encoding a large state representation that assigns 0 to all components that do not correspond with the current action. We felt that this was vague and made the implementation unclear.</p>"},{"location":"unit5/lesson15/lesson15.html#sarsa","title":"Sarsa(\u03bb)","text":"<pre><code>class Sarsa\u03bb(vMDP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def step0(self):\n        self.Z = self.W*0\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.Z[a] = self.\u03bb*self.\u03b3*self.Z[a] + self.\u0394Q(s)\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an)- self.Q(s,a))*self.Z[a]\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(), \u03b1=.05, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.underhood = 'maxQ'\nsarsa.render()\n</code></pre> <pre><code>sarsa.Q_(10,1)\n</code></pre> <pre><code>array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 2.50000000e-02, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 6.00209380e-05, 6.10730839e-03,\n       5.14226101e-02, 3.25250643e-01, 1.58015574e+00, 5.59873331e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(reward='reward1'), \u03b1=0.5, episodes=200, seed=0, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vgrid(reward='reward1'), episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.1, episodes=200, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.5, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.9, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <p>Note how some of the cells arrows that represent the policy is being updated without being visited in the current time step this is due to the effect of eligibility traces accumulating states in previous steps that are eligible for updating.</p>"},{"location":"unit5/lesson15/lesson15.html#true-online-sarsa","title":"True Online Sarsa(\u03bb)","text":"<p>We implement the True online Sarsa(\u03bb) algorithm in this section. This algorithm guarantees an equivalence between the online and the offline cases, and it is hailed as an important step towards implementing a sound multi-step control algorithm that is both efficient and sound. The main drawback that we have is that it can only be applied to a linear approximation, not to any general one as Sarsa(\u03bb), but it is a step in the right direction until researchers find a way to do it also for the nonlinear functions as well (such as neural networks). We need to handle an eligibility trace variable, the size of which is identical to the weights, so each action has its own eligibility trace. The update rule is given in equation 12.11, and its justification/deduction can be read from section 12.5. Note that in our implementation. </p> <ul> <li>qo is an old action-value estimation for the current state</li> <li>qn is the action-value estimation for the next    state</li> <li>q  is the action-value estimation for the current state</li> </ul> <pre><code>class trueSarsa\u03bb(vMDP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def step0(self):\n        self.Z = self.W*0\n        self.qo = 0\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n\n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n\n        self.q = self.Q(s,a)\n        self.qn= self.Q(sn,an)*(1-done)\n        \u03b4 = rn + \u03b3*self.qn - self.q\n        self.Z[a] = \u03bb*\u03b3*self.Z[a] + (1-\u03b1*\u03bb*\u03b3*self.Z[a].dot(s))*s\n\n        self.W[a] += \u03b1*(\u03b4 + self.q - self.qo )*self.Z[a] - \u03b1*(self.q - self.qo)*s\n        self.qo = self.qn\n</code></pre> <pre><code>truesarsa\u03bb = trueSarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.99, \u03b1=.1, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.99,  \u03b1=.1, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how true online Sarsa(\u03bb) performs a bit better than Sarsa(\u03bb) which in turn performs better than Sarsa(0) which has been covered in previous lessons.</p> <pre><code>sarsa = Sarsa(env=vmaze(reward='reward1'), episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>However, this can be compensated by increasing \u03b1 from .1 (the default value) to a higher value, such as .8 or .9. Note that \u03bb also affects the best value for \u03b1; often, we cannot set both to high values as we saw in the hyperparameter study that we performed on the random walk problem. </p> <pre><code>sarsa = Sarsa(env=vmaze(reward='reward1'), \u03b1=.9, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>Let us now apply the MountainCarTiledRuns on the trueSarsa\u03bb to see how it behaves with different \u03bb values.</p> <pre><code>def MountainCarTiledRuns_\u03bb(runs=20, algo=trueSarsa\u03bb, env=tiledMountainCar):\n\n    plt.title(algo.__name__+' on mountain car: comparison of \u03bb and \u03b1 with the same 8x8x8 tilings')\n\n    for \u03bb, \u03b1 in zip([0, .8], [.5, .3]):\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=8), \u03bb=\u03bb, \u03b1=\u03b1/8,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='\u03bb=%.2f, \u03b1=%.2f/8'%(\u03b1,\u03b1))\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\n\nfigure_10_3_\u03bb = MountainCarTiledRuns_\u03bb\n</code></pre> <pre><code>%time figure_10_3_\u03bb(env=tiledMountainCar, algo=trueSarsa\u03bb)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 22s, sys: 527 ms, total: 2min 23s\nWall time: 2min 23s\n</code></pre> <p>As we can see both n-step TD and true online TD(\u03bb)  have perform very similarly which confirms our starting premise, which is to design an algorithm that can perform n-step TD updates but which does not requires the steps delay.</p> <pre><code>def MountainCarTiledCompare_\u03bb(runs=3,  algo=trueSarsa\u03bb, env=IHTtiledMountainCar): # 10\n\n    xsticks = np.array([0, .5 , 1, 1.5, 2, 2.3])/8\n    plt.xticks(ticks=xsticks, labels=xsticks*8)\n    plt.yticks([180, 200, 220, 240, 260, 280, 300])\n    plt.ylim(170, 300)\n    plt.title('Steps per episode averaged over first 50 episodes for '+algo.__name__)\n\n    for \u03bb in [0, .68, .84, .92]:#, .96, .98, .99]:\n        if \u03bb&gt;=.0: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.6: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.8: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.9: \u03b1s = np.arange(.1,  1.8,  .15)\n        if \u03bb&gt;=.98: \u03b1s = np.arange(.1,  .7,  .15)\n        if \u03bb&gt;=.98: \u03b1s = np.arange(.1,  .7,  .07)\n\n        Compare(algorithm=algo(env=env(ntilings=8, ntiles=8), \u03bb=\u03bb, episodes=50, \u03b5=0), runs=runs, \n                                  hyper={'\u03b1':\u03b1s/8}, \n                                  plotT=True).compare(label='\u03bb=%.2f'%\u03bb)\n    plt.xlabel(r'$\\alpha \\times 8$ since we used 8 tiles for each tilings')\n    plt.show()\n\n\nfigure_12_10 = MountainCarTiledCompare_\u03bb\n</code></pre>"},{"location":"unit5/lesson15/lesson15.html#comparing-sarsa-with-n-step-sarsa","title":"Comparing Sarsa(\u03bb) with n-step Sarsa","text":"<pre><code>%time MountainCarTiledCompare_\u03bb()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|12/12\n</code></pre> <pre><code>CPU times: user 2min 23s, sys: 658 ms, total: 2min 24s\nWall time: 2min 25s\n</code></pre> <pre><code>from RLvec import *\n</code></pre> <pre><code>%time MountainCarTiledCompare_n()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|14/14\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|13/13\n</code></pre> <pre><code>CPU times: user 5min 6s, sys: 707 ms, total: 5min 7s\nWall time: 5min 7s\n</code></pre> <p>Note that we have not used hashing and hence our range is different from the one stated in the book. Nevertheless, the pattern is maintained.</p>"},{"location":"unit5/lesson15/lesson15.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we covered an important mechanism in RL that allows us to efficiently implement n-step methods online without waiting for n-steps, as we did earlier in the tabular case. Eligibility traces are very useful when we use the approximate function to estimate the value function in RL, but they can also be used in the tabular case. We have seen the classic Sarsa() for control, which can be used with any function approximation, including a neural network. Finally, we have seen how True Online Sarsa(\u03bb) behaves and that it is more efficient and holds a guarantee of the online and offline equivalency of TD(), which took several years to achieve in the RL development. True online TD(\u03bb) only applies to the linear function approximation, and there is no counterpart for it when we move to neural networks. In this lesson, you also saw how to deal with a continuous state space using function approximation and how to apply previous concepts to a more difficult control problem.</p>"},{"location":"unit5/lesson15/lesson15.html#your-turn","title":"Your turn","text":"<ol> <li>apply TD(\u03bb) and true online TD(\u03bb) on the 1000 tiled random walk environment (below we show you how to do that on the 1000 random walk), make sure to set \u03b1 Appropriately around .002.</li> <li>apply Sarsa(\u03bb) and true online Sarsa(\u03bb) on the tiled mountain car environment and confirm that its behaviour is similar to the n-step Sarsa.</li> </ol>"},{"location":"unit5/lesson15/lesson15.html#challenge","title":"Challenge","text":"<ol> <li>Implement a true online Q-learning algorithm. Note that you would need to restart the traces when the max action is not the same as the selected action since you cannot blame the pure greedy policy (the one Q-learning is learning about) for what the e-greedy policy took to explore, refer to the paper</li> <li>Can you think of a reason why we did not implement an offline version of the trueTD\u03bb?</li> <li>Implement and offline of trueTD\u03bb and TD\u03bb and compare between them and their online counterparts do you see a difference between them.</li> </ol>"},{"location":"unit5/lesson16/lesson16.html","title":"16. Nonlinear Approximation for Control","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit5/lesson16/lesson16.html#lesson-15-non-linear-action-value-approximation-and-policy-gradient-methods-for-control","title":"Lesson 15: Non-linear Action-Value Approximation and Policy Gradient Methods for Control","text":"<p>Learning outcomes 1. build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand 1. understand how to combine reinforcement learning with non-linear function approximators such as a neurla network to create a powerful framework that allows automatic agent learning by observation or self-play. 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</p> <p>Reading: The accompanying reading of this lesson is chapter 16 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson deals with non-linear function approximation and RL, particularly with neural networks combined with RL. We have seen earlier how to deal with linear function approximations and the benefits that they bring in terms of a richer state representation. Now we move to an even richer but integrated and automatic feature extraction via a neural network. We can of course do feature extraction separate from training an RL, but integrating these two stages brings the benefit of extracting features that are particularly useful for the RL algorithm. One of the earliest examples of successfully using a neural network is Tesauro Backgammon TDGammon. Although the theoretical convergence guarantees do not extend from linear network cases to the non-linear function approximation, however, that did stop researchers from integrating both albeit in a few examples prior to the deep learning era, after which a vast number of models that uses both deep learning and reinforcement learning emerges with impressive results.</p> <p>We will utilise the idea of an experience replay buffer. We have already seen how to benefit from past experience on a large scale in the planning lesson, where we build a model of the environment. Here, we will not build a model, so we are still in the vicinity of model-free RL, but we will see how to execute a batch of updates instead of one update at a time. Training a neural network is an important addition to our arsenal of techniques because it brings RL closer to how we train supervised learning models, which is useful in two folds. The first is to train based on a mix of old and new experiences is useful for incorporating new experiences without forgetting old experiences. The second is to benefit from the built-in parallelisation of neural network training, which is greatly useful for more complex domains such as games and robotics. </p> <p>Note that the replay buffer dictates the choice of an off-policy algorithm, i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.</p>"},{"location":"unit5/lesson16/lesson16.html#dependencies","title":"Dependencies","text":"<p>Please refer to libraries installation in the Introduction to find a list of libraries that you will need to install. If you are using the Azure VM then these packages will be already there, so you can get started and jum to the next section. If you cannot find a list of pip3 install in the IntroductionTOC notebook then re-download the notebook from minerva.</p> <p>Let us test if it is working</p> <pre><code># !conda list -f tensorflow\n# print('-------------check that the two commands give you the same version--------------------')\n# print('-------------otherwise it means you are using a kernel without a GPU------------------')\n# !pip3 show tensorflow \n</code></pre> <p>We can also check if our GPU is in use as follows.</p> <pre><code>from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n# or\n# !python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <pre><code>2025-02-23 16:56:04.777721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 12904346926301707512\nxla_global_id: -1\n]\n</code></pre> <pre><code>from tensorflow.python.platform import build_info as tf_build_info\n# print(\"cudnn_version\",tf_build_info.build_info['cudnn_version'])\n# print(\"cuda_version\",tf_build_info.build_info['cuda_version'])\n</code></pre> <p>Ok, we are ready, let us get started...!</p> <p>To run this notebook on a remote Azure lab server without using remote desktop check this link</p>"},{"location":"unit5/lesson16/lesson16.html#non-linear-funciton-approximation-reinforcement-learning","title":"Non-linear Funciton Approximation Reinforcement Learning","text":"<p>First let us import the necessary libraries</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre> <pre><code>import time\nimport os\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom numpy.random import rand\nfrom collections import deque\nfrom itertools import islice\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.animation as animation\n</code></pre> <pre><code>import tensorflow as tf\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model\n# from tensorflow.keras.callbacks import ProgbarLogger\n\nfrom IPython.display import clear_output\n\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n</code></pre> <p>Note that we are importing the basic MRP and MDP classes because this is suitable to create a non-linear MRP and MDP as we do not want to mix up non-linear solutions with linear solutions produced in previous lesson.</p>"},{"location":"unit5/lesson16/lesson16.html#rl-with-neural-networks","title":"RL with Neural Networks","text":"<p>It is time to extend our basic MRP class to handle function approximation using neural networks. Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the application under consideration.</p>"},{"location":"unit5/lesson16/lesson16.html#buffer-implementation","title":"Buffer Implementation","text":"<p>If you are familiar with the concepts of deque then please skip to the next section. </p> <p>It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly.</p> <pre><code>from collections import deque\nbuffer = deque(maxlen=5)\nbuffer.append(1)\nbuffer.append(2)\nbuffer.append(3)\nbuffer.append(4)\nprint(buffer)\n</code></pre> <pre><code>deque([1, 2, 3, 4], maxlen=5)\n</code></pre> <pre><code>buffer.append(5)\nbuffer.append(6)\nbuffer.append(7)\nprint(buffer)\n</code></pre> <pre><code>deque([3, 4, 5, 6, 7], maxlen=5)\n</code></pre> <p>To access the last item we use the usual indexing.</p> <pre><code>print(buffer[-1])\nbuffer.append(8)\nprint(buffer[-1])\n</code></pre> <pre><code>7\n8\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#random-sampling-form-the-buffer","title":"Random Sampling form the Buffer","text":"<p>Let us now take few random samples of size 2 from the buffer.</p> <pre><code>from random import sample\nfor i in range(3):\n    print(sample(buffer,2))\n</code></pre> <pre><code>[4, 8]\n[7, 8]\n[4, 6]\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#buffer-with-complex-element-tuples","title":"Buffer with Complex Element (Tuples)","text":"<p>Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images.</p> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\n\nprint(buffer)\n</code></pre> <pre><code>deque([('2', 1, '3'), ('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4')], maxlen=4)\n</code></pre> <p>Now in order to sample we can directly sample from the buffer </p> <pre><code>batch = sample(buffer,3)\nprint(batch)\n</code></pre> <pre><code>[('2', 1, '3'), ('5', 1, '4'), ('3', 2, '4')]\n</code></pre> <p>However, the above is not useful, usually we want to place all the actions and states and next states in their own list to feed them as a batch into a neural network. To put all states and actions together each in its own list we can use zip</p> <pre><code>for i in zip(*batch): print(i)\n</code></pre> <pre><code>('2', '5', '3')\n(1, 1, 2)\n('3', '4', '4')\n</code></pre> <p>We can also convert them into a numpy array directly.</p> <pre><code>[np.array(i) for i in zip(*batch)]\n</code></pre> <pre><code>[array(['2', '5', '3'], dtype='&lt;U1'),\n array([1, 1, 2]),\n array(['3', '4', '4'], dtype='&lt;U1')]\n</code></pre> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\nbuffer.append(('7',8,'6'))\n\nprint(buffer)\nsamples = [np.array(item) for item in zip(*sample(buffer,3))]\nprint(samples)\n</code></pre> <pre><code>deque([('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4'), ('7', 8, '6')], maxlen=4)\n[array(['5', '7', '4'], dtype='&lt;U1'), array([1, 8, 2]), array(['4', '6', '5'], dtype='&lt;U1')]\n</code></pre> <p>sampling an empty batch</p> <pre><code>nbatch = 1\nsample(buffer, nbatch-1)\n</code></pre> <pre><code>[]\n</code></pre> <p>sampling last nbatch elements from the buffer</p> <pre><code>nbatch = 3\ndef slice_(buffer):\n    return list(islice(buffer,len(buffer)-nbatch,len(buffer)))\n</code></pre> <pre><code>slice_(buffer)\n</code></pre> <pre><code>[('4', 2, '5'), ('5', 1, '4'), ('7', 8, '6')]\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#neural-network-based-mrp","title":"Neural Network Based MRP","text":"<p>In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_. We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific total number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y&lt;x. Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class.</p> <pre><code>class nnMRP(MRP):\n    def __init__(self, \u03b3=0.99, nF=512, nbuffer=10000, nbatch=32, rndbatch=True,\n                 save_weights=1000,     # save weights every now and then\n                 load_weights=False,\n                 print_=True,\n                 **kw):\n\n        super().__init__(\u03b3=\u03b3, **kw)\n        self.nF = nF   # feature extraction is integrated within the neural network model not the env\n\n        self.nbuffer  = nbuffer\n        self.nbatch   = nbatch\n        self.rndbatch = rndbatch\n\n        self.load_weights_= load_weights\n        self.save_weights_= save_weights # used to save the target net every now and then\n\n        self.update_msg = 'update %s network weights...........! %d'\n        self.saving_msg = 'saving %s network weights to disk...! %d'\n        self.loading_msg = 'loading %s network weights from disk...!'\n\n    def init(self):\n        self.vN = self.create_model('V')                      # create V deep network\n        if self.load_weights_: self.load_weights(self.vN,'V.weights.h5') # from earlier training proces   \n        self.V = self.V_\n\n    #-------------------------------------------Deep model related---------------------------\n    def create_model(self, net_str):\n        x0 = Input(self.env.reset().shape)#(84,84,1))#self.env.frame_size_)\n        x = Conv2D(32, 8, 4, activation='relu')(x0)\n        x = Conv2D(64, 4, 2, activation='relu')(x)\n        x = Conv2D(64, 3, 1, activation='relu')(x)\n        x = Flatten()(x)\n        x = Dense(self.nF, 'relu')(x)\n        x = Dense(1 if net_str=='V' else self.env.nA)(x) \n        model = Model(x0, x)\n        model.compile(Adam(self.\u03b1), loss='mse')\n        model.summary() if net_str != 'Qn' else None\n        model.net_str = net_str\n        return model\n\n    def load_weights(self, net, net_str ):\n        print(self.loading_msg%net_str)\n        loaded_weights = net.load_weights(net_str)\n        loaded_weights.assert_consumed()\n\n    def save_weights(self):\n        print(self.saving_msg%('V ',self.t_))\n        self.vN.save_weights('V.weights.h5')\n\n    #------------------------------------- value related \ud83e\udde0-----------------------------------\n    def V_(self, s, Vs=None):\n\n        # update the V network if Vs is passed\n        if Vs is not None: self.vN.fit(s, Vs, verbose=False); return None\n\n        # predict for one state for \u03b5greedy, or predict for a batch of states, copy to avoid auto-grad issues\n        return self.vN.predict(np.expand_dims(s, 0), verbose=0)[0] if len(s.shape)!=4 else np.copy(self.vN.predict(s, verbose=0)) \n\n    #-------------------------------------------buffer related----------------------------------\n    def allocate(self):\n        self.buffer = deque(maxlen=self.nbuffer)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        self.save_weights() if self.save_weights_ and self.t_%self.save_weights_==0 else None\n        self.buffer.append((s, a, rn, sn, done))\n\n    # deque slicing, cannot use buffer[-nbatch:]\n    def slice_(self, buffer, nbatch):\n        return list(islice(buffer, len(buffer)-nbatch, len(buffer)))\n\n    def batch(self):\n        # if nbatch==nbuffer==1 then (this should give the usual qlearning without replay buffer)\n        # sample nbatch tuples (each tuple has 5 items) without replacement or obtain latest nbatch from the buffer\n        # zip the tuples into one tuple of 5 items and convert each item into a np array of size nbatch \n\n        samples = sample(self.buffer, self.nbatch) if self.rndbatch else self.slice_(self.buffer, self.nbatch)\n        samples = [np.array(experience) for experience in zip(*samples)]\n\n        # generate a set of indices handy for filtering, to be used in online()\n        inds = np.arange(self.nbatch)\n\n        return samples, inds\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#neural-network-based-mdp","title":"Neural Network Based MDP","text":"<p>Now we create the MDP class which implements policy related functionality</p> <pre><code>class nnMDP(MDP(nnMRP)):\n\n    # update the target network every t_qNn steps\n    def __init__(self, create_vN=False, **kw):\n        super().__init__(**kw)\n        self.create_vN = create_vN\n\n    def init(self):\n        super().init() if self.create_vN else None                     # to create also vN, suitable for actor-critic\n        self.qN  = self.create_model('Q')                              # create main policy network\n        self.qNn = self.create_model('Qn')                             # create target network to estimate Q(sn)\n\n        self.load_weights(self.qN,'Q.weights.h5') if self.load_weights_ else None # from earlier training proces\n        self.load_weights(self.qNn,'Q.weights.h5') if self.load_weights_ else None # from earlier training proces\n\n        self.Q = self.Q_\n\n    def save_weights(self):\n        super().save_weights() if self.create_vN else None             # save vN weights, for actor-critic\n        print(self.saving_msg%('Q', self.t_))\n        self.qN.save_weights('Q.weights.h5')\n\n    def set_weights(self, net):\n        print(self.update_msg%(net.net_str, self.t_))\n        net.set_weights(self.qN.get_weights())\n\n    #------------------------------------- policies related \ud83e\udde0-----------------------------------\n    def Q_(self, s, Qs=None):\n        # update the Q networks if Qs is passed\n        if Qs is not None: self.qN.fit(s, Qs, verbose=0); return None\n\n        # predict for one state for \u03b5greedy, or predict for a batch of states, \n        # copy to avoid auto-grad issues\n        return self.qN.predict(np.expand_dims(s, 0), verbose=0)[0] if len(s.shape)!=4 \\\n    else np.copy(self.qN.predict(s, verbose=0))\n\n    def Qn(self, sn, update=False):\n        # update the Qn networks if Qn is passed\n        if update: self.set_weights(self.qNn); return None\n        return self.qNn.predict(sn, verbose=0)\n</code></pre> <p>Bellow we double-check that the policies assigned via class inheritance is suitable. MRP should have a stationary policy while MDP has an \u03b5greedy policy.</p> <pre><code>print(nnMRP().policy)\nprint(nnMDP().policy)\n</code></pre> <pre><code>&lt;bound method MRP.stationary of &lt;__main__.nnMRP object at 0x14c455b80&gt;&gt;\n&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.nnMDP object at 0x14c455b50&gt;&gt;\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#deep-q-learning-architecture","title":"Deep Q-Learning Architecture","text":"<p>Note that we need to set \u03b5 here otherwise it will be set by default to .1 in the parent class.</p> <pre><code>class DQN(nnMDP):\n    def __init__(self, \u03b1=1e-4, t_Qn=1000, **kw): \n        print('--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.store = True\n        self.t_Qn = t_Qn\n\n    #------------------------------- \ud83c\udf16 online learning ---------------------------------\n    # update the online network in every step using a batch\n    def online(self, *args):\n        # no updates unless the buffer has enough samples\n        if len(self.buffer) &lt; self.nbuffer: return\n\n        # sample a tuple batch: each component is a batch of items \n        # (s and a are sets of states and actions)\n        (s, a, rn, sn, dones), inds = self.batch() \n\n        # obtain the action-values estimation from the two networks and \n        # ensure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the Q-learning update rule\n        Qs[inds, a] = self.\u03b3*Qn.max(1) + rn\n\n        # then update both\n        self.Q(s, Qs)\n        self.Qn(sn, self.t_%self.t_Qn==0)\n</code></pre> <p>Let us now deal with Grid as a game and state as images. We will try first just to run as usual without sampling but with randomisation form the buffer. To do so, we simply assign the same number for the nbuffer and nbatch which will force the algorithm to pass all of what it has in the buffer albeit randomised in terms of order.</p> <p>To properly force the algorithm not to randomise the samples, we can pass this flag explicitly.</p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n# please be patient as it takes much longer to learn from pixles\n\n\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, episodes=100, \\\n                    rndbatch=False, t_Qn=500, nbuffer=32, nbatch=32, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 3h 32min 14s, sys: 44min 36s, total: 4h 16min 50s\nWall time: 3h 11min 34s\n</code></pre> <p></p> <pre><code>nqlearn.interact(resume=True, episodes=103, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x14a0f16d0&gt;\n</code></pre> <p></p> <pre><code>nqlearn.interact(resume=True, episodes=110, view=10, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x14a0f16d0&gt;\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, \\\n                    episodes=100, nbuffer=1, nbatch=1, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 6h 47min 50s, sys: 58min 54s, total: 7h 46min 45s\nWall time: 6h 44min 8s\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, \\\n                    episodes=100, nbuffer=8, nbatch=8, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 4h 57min 59s, sys: 49min 30s, total: 5h 47min 30s\nWall time: 4h 39min 39s\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), \\\n                    seed=10, episodes=100, nbuffer=1000, nbatch=32, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 1h 49min 58s, sys: 15min 27s, total: 2h 5min 26s\nWall time: 1h 33min 50s\n</code></pre> <p></p> <pre><code>nqlearn.policy\n</code></pre> <pre><code>&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.DQN object at 0x19819fc20&gt;&gt;\n</code></pre> <pre><code>print(nqlearn.env.img.shape)\n</code></pre> <pre><code>(50, 84, 1)\n</code></pre> <pre><code>%time nqlearn.interact(resume=True, episodes=151, **demoGame())\n</code></pre> <pre><code>CPU times: user 23min 48s, sys: 3min 35s, total: 27min 24s\nWall time: 19min 47s\n\n\n\n\n\n&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p> <pre><code>nqlearn.env.render__()\n</code></pre> <p></p> <pre><code># nqlearn.ep -=1\n# nqlearn.plotT = False\n# nqlearn.visual = True\n# nqlearn.underhood='maxQ' # uncomment to see also the policy\nnqlearn.interact(resume=True, episodes=159, plotT=True)\n</code></pre> <pre><code>&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p> <pre><code>plt.imread('img/img0.png').shape\n</code></pre> <pre><code>(231, 387, 4)\n</code></pre> <pre><code>plt.imshow(nqlearn.env.img)\n</code></pre> <pre><code>&lt;matplotlib.image.AxesImage at 0x197c04ec0&gt;\n</code></pre> <p></p> <pre><code>163*278*4\n</code></pre> <pre><code>181256\n</code></pre> <pre><code>nqlearn.env.img=None\nnqlearn.env.i=0\nnqlearn.ep -=1\nnqlearn.plotT = False\nnqlearn.visual = True\n# nqlearn.underhood='maxQ' # uncomment to see also the policy\nnqlearn.interact(train=False, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p>"},{"location":"unit5/lesson16/lesson16.html#double-dqn-learning","title":"Double DQN Learning","text":"<pre><code>class DDQN(DQN):\n    def __init__(self, \u03b1=1e-4, **kw):\n        print('----------- \ud83e\udde0 Double DQN is being set up \ud83e\udde0 ---------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.store = True\n    #--------------------------- \ud83c\udf16 online learning -----------------------------\n    def online(self, *args):\n        # sample a tuple batch: each component is a batch of items \n        #(ex. s is a set of states, a is a set of actions) \n        (s, a, rn, sn, dones), inds = self.batch()\n\n        # obtain the action-values estimation from the two networks \n        # and make sure the target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the *Double* Q-learning \n        # update rule, this is where the max estimations are decoupled from the max \n        # action selections\n        an_max = self.Q(sn).argmax(1)\n        Qs[inds, a] = self.\u03b3*Qn[inds, an_max] + rn\n\n        # update\n        self.Q(s, Qs) \n        self.Qn(sn, self.t_%self.t_Qn==0)\n</code></pre> <pre><code># %time ddqlearn = DDQN(env=Gridi(style='maze'), seed=10, episodes=2, max_t=200, \\\n#                       max_t_exp=2000, nbuffer=100, nbatch=32, **demoGame()).interact() \n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#conclusion","title":"Conclusion","text":"<p>In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. </p> <p>You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. </p>"},{"location":"unit5/lesson16/lesson16.html#discussion-and-activity","title":"Discussion and Activity","text":"<p>Read the following classic Nips paper and Nature paper and discuss it in the discussion forum.</p>"},{"location":"unit5/lesson16/lesson16.html#your-turn","title":"Your turn","text":"<ol> <li> <p>try to apply the same concept on other simple environments provided by Gym such as the acrobot.</p> </li> <li> <p>apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum.</p> </li> </ol>"},{"location":"unit5/lesson16/lesson16.html#challenge","title":"Challenge++","text":"<ol> <li>check the implementation of the deep network in the tutorial and try to integrate it into the provided infrastructure.</li> </ol>"},{"location":"unit5/lesson17/lesson17.html","title":"17. Application on Robot Navigation","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit5/lesson17/lesson17.html#lesson-16-rl-on-robotics","title":"Lesson 16: RL on Robotics","text":"<p>Learning outcomes 1. understand how to create a simple Robot environment that links to Gazebo 1. understand how to deal with the simulated environment in a grid world fashion 1. appreciate the intricacy of applying RL to the robotics domain 1. build on previous concepts to come up with a suitable solution to a problem at hand 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence 1. understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play. 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</p> <p>Reading: We cover applications of RL on robotics based on previouse units which you can refere to.</p> <p>In this notebook, we deal with how to set up a robot environment class that can handle the publish-subscribe on topics and deal with services in ROS. We must have ROS and Gazebo installed and set up on our machine. The code is a starting point and is not fully developed. You will need to write the necessary functionality to address a specific requirement. The main idea of tackling robotics applications in a Jupyter notebook is to utilise the provided infrastructure and libraries of code we covered in earlier units.</p>"},{"location":"unit5/lesson17/lesson17.html#instructions-for-running-experiments-on-azure-vm","title":"Instructions for running Experiments on Azure VM","text":"<p>The VM usage limit is set to 40 hours. Please turn off the machine when not using it to preserve your time. The VM is not set to disconnect you automatically so that you can leave it training the robot continuously for assessment 2.</p> <p>Please turn off the screen save and screen lock in Xfce(Applications-&gt;Settings-&gt;Light Locker) as it may cause the machine to become not responsive, which in turn, causes Azure to stop it automatically.</p> <p>If the VM becomes corrupted for some reason, then you can reimage it by going to Azure Lab page and selecting the three dots, then reimage. Reimage will reset the VM to its initial settings but it causes all data you have on the machine to be lost. You are advised to backup your data, you may want to use OneDrive or other backup methods.</p> <p>Note If Gazebo stops for any reason, the provided code has a try-except statement (in lesson 4 Monte Carlo) that you can activate (comment in). It allows you to continue training even if the robot becomes not responsive without having to restart the experiment.</p> <p>If you are running out of time, please let your tutor know in advance and they will try to increase your VM time allowance.</p>"},{"location":"unit5/lesson17/lesson17.html#turtlebot3","title":"Turtlebot3","text":"<p>Install turtlebot3 packages. If you are in our VM, they would have been already installed.</p>"},{"location":"unit5/lesson17/lesson17.html#more-realistic-simulation-running-gazebo","title":"More realistic Simulation: Running Gazebo","text":"<p>You will need to launch a gazebo environment with Turtlebot3 in it. So long as the /scan(LaserScan), /odom (Odometry) and /cmd_vel(Twist) topics are available, the environment should work fine. Our target is to build an environment that will allow us to use the algorithms we developed in earlier units directly.</p> <p>To launch an environment, you should open a terminal and run the following command</p> <p>ros2 launch turtlebot3_gazebo turtlebot3_house.launch</p> <p>Note that you cannot do that here because that will block the notebook from executing other code.  You must select restart your notebook kernel, ex. Kernel-&gt; Restart and Run ALL, whenever you want to re-establish a connection with the environment.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>import rclpy as ros\nfrom rclpy.node import Node\n\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg  import Odometry\nfrom sensor_msgs.msg import LaserScan\nfrom std_srvs.srv import Empty\nfrom gazebo_msgs.srv import SpawnModel\n\nimport numpy as np\nfrom numpy import Inf\nfrom random import randint\nfrom math import atan2, atan, pi\nimport matplotlib.pyplot as plt\nros.init()\n</code></pre> <pre><code># !export PATH=\"home/rl/.local/bin:/opt/ros/humble/share:$PATH\"\n# !python3 -c \"import rclpy as ros; ros.init()\"\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#moving-the-robotactions","title":"Moving the robot(Actions)","text":"<p>We start by controlling the robot only</p> <p>Go to /opt/ros/humble/share/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf and adjust the 30 to 5 for the odomotry turtlebot3_diff_drive. This will change its frequency from 30 to 5 so that it aligns with the scanner frequency.</p> <pre><code>def name(): return 'node'+str(randint(1,1000))\n</code></pre> <pre><code>class Env(Node):\n\n# initialisation--------------------------------------------------------------\n    # frequency: how many often (in seconds) the spin_once is invoked, or the publisher is publishing to the /cmd_vel\n    def __init__(self, name=name(), \n                 freq=1/20, n=28, \n                 speed=.5, \u03b8speed=pi/5, \n                 rewards=[30, -10, 0, -1],\n                 verbose=False):\n        super().__init__(name)\n\n        self.freq = freq\n        self.n = n\n\n        self.speed = speed\n        self.\u03b8speed = round(\u03b8speed,2)\n\n        self.robot = Twist()\n        self.rewards = rewards\n        self.verbose = verbose\n\n        # do not change----------------------------------------------------\n        self.x = 0 # initial x position\n        self.y = 0 # initial y position\n        self.\u03b8 = 0 # initial \u03b8 angle\n        self.scans = np.zeros(60) # change to how many beams you are using\n        self.t = 0\n\n        self.tol = .6  # meter from goal as per the requirement (tolerance)\n        self.goals =  [[2.0, 2.0], [-2.0, -2.0]]\n        # -----------------------------------------------------------------\n\n        self.controller = self.create_publisher(Twist, '/cmd_vel', 0)\n        self.timer = self.create_timer(self.freq, self.control)\n\n        self.scanner = self.create_subscription( LaserScan, '/scan', self.scan, 0)\n        self.odometr = self.create_subscription( Odometry, '/odom', self.odom, 0)\n\n        self.range_max = 3.5\n        self.range_min = .28               # change as you see fit\n\n\n        # establish a reset client \n        self.reset_world = self.create_client(Empty, '/reset_world')\n        while not self.reset_world.wait_for_service(timeout_sec=2.0):\n            print('world client service...')\n\n\n        # compatibility----------------------------------------------\n        nturns = 15 # number of turns robot takes to complete a full circle\n        resol = speed/2\n\n        \u03b8resol = 2*pi/nturns\n        dims = [4,4]\n        self.xdim = dims[0]  # realted to the size of the environment\n        self.ydim = dims[1]  # realted to the size of the environment\n\n        self.resol = round(resol,2)\n        self.\u03b8resol = round(\u03b8resol,2)\n\n        self.cols  = int(self.xdim//self.resol) +1   # number of grid columns, related to linear speed\n        self.rows  = int(self.ydim//self.resol) +1   # number of grid rows,    related to linear speed\n        self.orts  = int(2*pi//self.\u03b8resol)     +1   # number of angles,       related to angular speed\n\n        self.nC = self.rows*self.cols              # Grid size\n        self.nS = self.rows*self.cols*self.orts # State space size\n        self.nA = 3\n\n\n        self.Vstar = None # for compatibility\n        # --------------------------------------------------------------- \n        # self.rate = self.create_rate(30)\n        self.reset()\n\n        print('speed  = ', self.speed)\n        print('\u03b8speed = ', self.\u03b8speed)\n        print('freq   = ', self.freq)\n\n# sensing--------------------------------------------------------------\n    # odometry (position and orientation) readings\n    def odom(self, odoms):\n        self.x = round(odoms.pose.pose.position.x, 1)\n        self.y = round(odoms.pose.pose.position.y, 1)\n        self.\u03b8 = round(self.yaw(odoms.pose.pose.orientation),2) \n        self.odom = np.array([self.x, self.y, self.\u03b8])\n        if self.verbose: print('odom = ',  self.odom )\n\n    # laser scanners readings\n    def scan(self, scans):\n        self.scans = np.array(scans.ranges)\n        self.scans[scans==Inf] = self.range_max\n        # if self.verbose: print('scan = ', self.scans[:10].round(2))\n        if self.verbose: print('scan = ', np.r_[self.scans[-5:], self.scans[:5]].round(2))\n\n    # converting to the quaternion self.z to Euler\n    # see https://www.allaboutcircuits.com/technical-articles/dont-get-lost-in-deep-space-understanding-quaternions/#\n    # see https://eater.net/quaternions/video/intro\n\n    def yaw(self, orient):\n        x, y, z, w = orient.x, orient.y, orient.z, orient.w\n        yaw = atan2(2.0*(x*y + w*z), w*w + x*x - y*y - z*z)\n        return yaw if yaw&gt;0 else yaw + 2*pi # in radians, [0, 2pi]\n\n    # angular distance of robot to a goal.............................................\n    def \u03b8goal(self, goal):\n        xgoal, ygoal = self.goals[goal] \n        x, y  = self.x, self.y\n        \u03b8goal = atan2(abs(xgoal-x), abs(ygoal-y)) # anglegoal\n        # if \u03b8goal&lt;=0  \u03b8goal += 2*pi\n        return round(\u03b8goal, 2) # in radians, [0, 2pi]\n\n    # Eucleadian distance of robot to nearest goal......................................   \n    def distgoal(self):\n        dists = [Inf, Inf]        # distances of robot to the two goals\n        for goal, (xgoal, ygoal) in enumerate(self.goals):\n            dists[goal] = (self.x - xgoal)**2 + (self.y - ygoal)**2\n\n        dist = min(dists)         # nearest goal distance\n        goal = dists.index(dist)  # nearest goal index\n\n        if self.verbose: print('seeking goal ____________________', goal)\n        return round(dist**.5, 2), goal\n\n    # robot reached goal ...............................................................\n    def atgoal(self):\n        tol, x, y = self.tol,  self.x, self.y\n        atgoal = False\n        for xgoal, ygoal in self.goals:\n            atgoal = xgoal + tol &gt; x &gt; xgoal - tol  and  \\\n                     ygoal + tol &gt; y &gt; ygoal - tol\n\n            if atgoal: print('Goal has been reached woohoooooooooooooooooooooooooooooo!!'); break\n        return atgoal\n\n    # robot hits a wall...................................................................\n    def atwall(self, rng=5):\n        # check only 2*rng front scans for collision, given the robot does not move backward\n        return np.r_[self.scans[-rng:], self.scans[:rng]].min() &lt; self.range_min \n        #return self.scans.min()&lt;self.range_min\n\n    # reward function to produce a suitable policy..........................................\n    def reward(self, a, imp=2):\n        stype = [self.atgoal(), self.atwall(), a==1, a!=1].index(True)\n\n        dist, goal = self.distgoal()\n        \u03b8goal = self.\u03b8goal(goal)\n\n        # get angular distance to reward/penalise robot relative to its orientation towards a goal\n        \u03b8dist = abs(self.\u03b8 - \u03b8goal)\n        if goal==1: \u03b8dist -= pi\n        \u03b8dist = round(abs(\u03b8dist),2)\n\n        reward = self.rewards[stype] \n        if stype: reward -= imp*(dist+\u03b8dist) \n\n        if self.verbose: \n            print('reward components=', \n                  'Total reward=', reward, \n                  'state reward=', self.rewards[stype],\n                  'goal dist=', dist, \n                  '|\u03b8-\u03b8goal|=', \u03b8dist)\n                #   '\u03b8robot=', self.\u03b8, \n                #   '\u03b8goal =', \u03b8goal, \n\n        # reset without restarting an episode if the robot hits a wall\n        if stype==1: self.reset() \n\n        return reward, stype==0, stype==1\n\n# State representation-------------------------------------------------\n   # change this to generate a suitable state representation\n    def s_(self):\n\n        self.xi = int((self.x+self.xdim/2)//self.resol)     # x index = col, assuming the grid middle is (0,0)\n        self.yi = int((self.y+self.ydim/2)//self.resol)     # y index = row, assuming the grid middle is (0,0)\n\n        # pi/2 to be superficially resilient to slight angle variation to keep \u03b8i unchanged\n        self.\u03b8i = int((self.\u03b8+pi/2)%(2*pi)//self.\u03b8resol)\n\n        self.si = self.xi + self.yi*self.cols     # position state in the grid\n        self.s = self.nC*(self.\u03b8i) + self.si      # position state with orientation\n        if self.verbose: print('grid cell= ', self.si, 'state = ', self.s)\n        return self.s \n\n\n# Control--------------------------------------------------------------    \n    def spin_n(self, n):\n        for _ in range(n): ros.spin_once(self)\n\n    def control(self): \n        self.controller.publish(self.robot) \n\n    # move then stop to get a defined action\n    def step(self, a=1, speed=None, \u03b8speed=None):\n        if speed is None: speed = self.speed\n        if \u03b8speed is None: \u03b8speed = self.\u03b8speed\n\n        self.t +=1\n        if self.verbose: print('step = ', self.t)\n\n        if  a==-1: self.robot.linear.x  = -speed  # backwards\n        elif a==1: self.robot.linear.x  =  speed  # forwards\n        elif a==0: self.robot.angular.z =  \u03b8speed # turn left\n        elif a==2: self.robot.angular.z = -\u03b8speed # turn right\n\n        # Now move and stop so that we can have a well defined actions  \n        self.spin_n(self.n) if a==1 else self.spin_n(self.n//2)\n        self.stop()\n\n        reward, done, wall = self.reward(a)\n        return self.s_(), reward, done, {}\n\n    def stop(self):\n        self.robot.linear.x = .0\n        self.robot.angular.z = .0\n        #  spin less so that we have smoother actions\n        self.spin_n(self.n//8)\n\n# reseting--------------------------------------------------------------\n    def reset(self):\n        print('resetting world..........................................')\n        # to ensure earlier queued actions are flushed, there are better ways to do this\n        for _ in range(1): self.reset_world.call_async(Empty.Request())\n        for _ in range(2): self.step(a=1, speed=0.001)  # move slightly forward to update the odometry to prevent repeating an episode unnecessary\n        for _ in range(1): self.reset_world.call_async(Empty.Request())\n\n        return self.s_()\n\n    # for compatibility, do not delete\n    def render(self, **kw):\n        pass\n</code></pre> <p>Ok, let us now test our little environment, to do so, open a terminal and launch the simple environment be executing the following command:</p> <p><code> ros2 launch turtlebot3_gazebo turtlebot3_simple.launch.py </code></p> <p>To make the testing smoother, you can right-click Gazebo and keep the window on top. You can also press ctrl+R to reset the environment.</p>"},{"location":"unit5/lesson17/lesson17.html#rotational-and-translational-calibration","title":"Rotational and Translational Calibration","text":"<p>Let us calibrate the rotational and translational movements of our robot settings. The idea here is to be able to get a consistent behaviour where a robot can consistently complete a full circle in a specified number of times most of the times. This is a trial and error process, we usually need to experiment with different settings, bearing in minde the accuracy and efficiency of the robot training that will take place later.</p> <p>The frequency plays an important role as it specifies how many times the velocity changes commands are going to be executed per seconds. This is via our subscription to the /cmd_vel topic and the create_timer() function of the Node class. The second important factor is the number of times the spin_once() is going to be executed. Spining a few times after publishing a command helps stablise the behaviour and gives us more consistency because it helps flush any delayed execution as well as any delayed subscription due to the robot hardware limitation which is simulated to an extent in Gazebo.</p>"},{"location":"unit5/lesson17/lesson17.html#rotation-in-place-to-form-a-full-2pi-circle","title":"Rotation in place to form a full \\(2\\pi\\) circle","text":"<p>You could try to increase the \u03b8speed but that will result in more slippage. It is also possible to increase the speed of execution (rather that the speed of the robot) by playing with n which is the number of times a spin_once() is executed. You could also speed up the clock by increasing the hz (frequency) of execution.</p> <pre><code>hz = 20       # increase to speed up, default is 20, max 30 to speed up\nn  = 28       # decrease to shorten the movements, default is 30, min 5 to speed up\n\nenv = Env(speed=.5, \u03b8speed=pi/5, freq=1/hz, n=n, verbose=True)\n</code></pre> <pre><code>resetting world..........................................\nstep =  1\nscan =  [1.19 2.51 2.43 2.41 2.39 2.38 2.39 1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [0. 0. 0.]\nodom =  [0. 0. 0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.53 2.45 2.39 2.37 2.37 2.4  1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.53 2.45 2.39 2.38 2.38 2.4  1.52 1.49 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\ngrid cell=  144 state =  1011\nstep =  2\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.43 2.4  2.39 2.38 2.39 1.53 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.45 2.4  2.36 2.38 2.39 1.54 1.5  2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.4  2.39 2.39 2.38 1.54 1.53 2.61]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\ngrid cell=  144 state =  1011\ngrid cell=  144 state =  1011\nspeed  =  0.5\n\u03b8speed =  0.63\nfreq   =  0.05\n</code></pre> <pre><code>def rotate_test(env):\n    env.reset( )\n    for _ in range(1+39):\n        env.step(0)\n\n# %time rotate_test(env)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#translation-calibration-moving-in-a-straight-line","title":"Translation calibration, moving in a straight line","text":"<p>You could try to increase the speed but that will result in bending.</p> <pre><code>def forward_test(env):\n    env.reset( )\n    # for _ in range(2): env.step(0)\n    for _ in range(10):\n        env.step()\n\n# forward_test(env)\n</code></pre> <pre><code># env.reset()\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#manual-solution-policy-to-the-given-problem","title":"Manual solution (policy) to the given problem","text":"<pre><code>def optimal_policy1(env):\n    env.reset( )\n\n    for _ in range(3): env.step(0)\n    for _ in range(5): env.step()\n    for _ in range(3): env.step(0)\n    for _ in range(2): env.step()\n\n# %time optimal_policy1(env)\n</code></pre> <pre><code>def optimal_policy2(env):\n    env.reset( )\n\n    for _ in range(13): env.step(2)\n    for _ in range(6): env.step()\n    for _ in range(5): env.step(2)\n    for _ in range(2): env.step()\n\n# %time optimal_policy2(env)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#applying-an-rl-algorithms-to-train-a-turtlebot3-to-autonomously-reach-the-goals","title":"Applying an RL Algorithms to Train a Turtlebot3 to Autonomously Reach the Goals","text":"<p>Now let us apply Sarsa on this problem</p> <pre><code>from rl.rl import *\n</code></pre> <pre><code># env_slow = Env(speed=.5 , \u03b8speed=pi/2, verbose=False) # slower more thorough\n# env_fast = Env(speed= 1., \u03b8speed=.75*pi, verbose=False) # useful for testing\n</code></pre> <pre><code># env = env_fast\n# short max_t so that an episode does not take long\n# sarsa = Sarsa(env=env, \u03b1=.1, max_t=200, episodes=300, seed=0, **demoGame()).interact()\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#resume-training-and-extend-training","title":"Resume Training and Extend training","text":"<p>If training intrrupted for any reason (including finishing the assigned number of episodes), you can resume it by passing resume=True to the interact() function.</p> <pre><code># env.reset()\n</code></pre> <pre><code># sarsa.interact(resume=True)\n</code></pre> <pre><code># sarsa.episodes = 1000\n# # sarsa.rewards=[100, -10, 0, -1]\n# %time sarsa.interact(resume=True)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#vectorised-environment","title":"Vectorised Environment","text":"<p>Let us now try to changed teh states from a number/index into vector. We will simply utilise the laser scans. We can use them as is or try to turn them into some form of a hot encoding or tile coding. Below we show a simple implementaiton which you can build on. Note that we will import algorithms from RLv instead of RL so that we can use the vectorised linear model RL algorithms such Sarsa and Q_learning.</p> <pre><code>class vEnv(Env):\n    def __init__(self, nscans=60, **kw):\n        self.nF = nscans\n        super().__init__(**kw)\n\n\n    def s_(self):\n        max, min = self.range_max, self.range_min\n        \u03c6 = self.scans\n        \u03c6[\u03c6==Inf] = max\n        \u03c6[\u03c6==np.nan] = 0\n        \u03c6[\u03c6&lt;min] = 0\n        \u03c6 = 1 - \u03c6/max\n        return  \u03c6/\u03c6.sum() \n\n    # def s_(self):\n    #     nF, scans, range_max = self.nF, self.scans, self.range_max\n    #     \u03c6 = np.r_[scans[-nF//2:], scans[:nF//2]]\n    #     \u03c6[\u03c6==Inf] = range_max\n    #     # \u03c6[\u03c6 &gt; range_max/2] = 0\n    #     # \u03c6[\u03c6 != 0] = 1\n    #     # print(\u03c6)\n    #     return \u03c6/\u03c6.sum()\n</code></pre> <pre><code>venv = vEnv(speed= .5, \u03b8speed=pi/5, rewards=[30,-10,0,-1], verbose=True) # useful for testing\n</code></pre> <pre><code>[WARN] [1715334431.685891644] [rcl.logging_rosout]: Publisher already registered for provided node name. If this is due to multiple nodes with the same name then all logs for that logger name will go out over the existing publisher. As soon as any node with that name is destructed it will unregister the publisher, preventing any further logs for that name from being published on the rosout topic.\n\n\nresetting world..........................................\nstep =  1\nodom =  [0. 0. 0.]\nscan =  [1.23 2.52 2.44 2.4  2.38 2.38 2.4  1.53 1.52 2.63]\nodom =  [0.   0.   6.28]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.49 2.44 2.4  2.36 2.39 2.41 1.56 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.39 2.38 2.39 2.38 1.55 1.5  2.61]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nstep =  2\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.44 2.4  2.38 2.39 2.38 1.53 1.5  2.63]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.43 2.39 2.37 2.38 2.4  1.51 1.52 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.38 2.39 2.38 2.4  1.53 1.52 2.6 ]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nspeed  =  0.5\n\u03b8speed =  0.63\nfreq   =  0.05\n</code></pre> <p>Now it is time to apply Sarsa on robotics! Note that this might not generate a useful policy yet. You must adjust the above code and tune your RL method hyperparameters.</p> <pre><code>from rl.rlln import *\n</code></pre> <pre><code>%time vsarsa = Sarsa\u03bb(env=venv, \u03b1=.05, \u03b5=.1, max_t=1000, episodes=500, seed=1, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 35min 38s, sys: 1min 23s, total: 37min 2s\nWall time: 7h 53min 35s\n</code></pre> <p></p> <pre><code>%time vtruesarsa = trueSarsa\u03bb(env=venv, \u03b1=.05, \u03b5=.1, max_t=1000, episodes=500, seed=1, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 30min 2s, sys: 1min 9s, total: 31min 12s\nWall time: 7h 4min 40s\n</code></pre> <p></p> <pre><code>\n</code></pre> <pre><code># venv.\u03b8speed = pi/3 \n# that means we are changing the env dynamics which is more challenging for the agent\n# vsarsa.\u03b5 = .1\n# vsarsa.d\u03b5= 1\n# vsarsa.\u03b5min\n</code></pre> <pre><code>vsarsa.episodes = 540\n# sarsa.rewards=[100, -10, 0, -1]\n%time vsarsa.interact(env=venv, resume=True)\n</code></pre> <pre><code>CPU times: user 30.8 s, sys: 1.42 s, total: 32.3 s\nWall time: 6min 31s\n\n\n\n\n\n&lt;RLv.Sarsa\u03bb at 0x7f5ffd716f40&gt;\n</code></pre> <p></p> <pre><code>vtruesarsa.episodes = 530\n# sarsa.rewards=[100, -10, 0, -1]\n%time vtruesarsa.interact(env=venv, resume=True)\n</code></pre> <pre><code>CPU times: user 1min 25s, sys: 3.76 s, total: 1min 29s\nWall time: 18min 5s\n\n\n\n\n\n&lt;RLv.trueSarsa\u03bb at 0x7f60340968b0&gt;\n</code></pre> <p></p> <pre><code>venv.reset()\n</code></pre> <pre><code>resetting world..........................................\nstep =  116815\nscan =  [0.23 0.24 0.28 0.28 0.12 0.12 0.12 0.15 0.17 0.18]\nodom =  [2.3  1.6  0.57]\nscan =  [1.2  2.52 2.43 2.39 2.38 2.37 2.41 1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.45 2.4  2.39 2.39 2.4  1.54 1.51 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.44 2.39 2.38 2.4  2.39 1.54 1.52 2.61]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nstep =  116816\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.52 2.43 2.39 2.39 2.39 2.41 1.54 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.52 2.45 2.41 2.37 2.4  2.4  1.54 1.5  2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.52 2.44 2.39 2.37 2.38 2.39 1.53 1.49 2.61]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\n\n\n\n\n\narray([0.01292713, 0.01281209, 0.0227836 , 0.02324465, 0.01027762,\n       0.00855598, 0.00619121, 0.00852212, 0.01901032, 0.01732261,\n       0.01271966, 0.01092166, 0.0123869 , 0.01278423, 0.01308601,\n       0.02420145, 0.02379162, 0.02363867, 0.02289693, 0.02216109,\n       0.02112418, 0.0060394 , 0.0024645 , 0.00575699, 0.00831019,\n       0.02626327, 0.02679824, 0.02755685, 0.02754739, 0.02787054,\n       0.02779497, 0.02766693, 0.02734911, 0.01158986, 0.01050995,\n       0.01039502, 0.01457711, 0.01629029, 0.00589362, 0.00818161,\n       0.01017182, 0.02215024, 0.02146195, 0.01321665, 0.01336568,\n       0.01321665, 0.01286833, 0.01212773, 0.01087723, 0.00950818,\n       0.02127673, 0.02375081, 0.02529406, 0.02671511, 0.0276625 ,\n       0.02650686, 0.01138107, 0.01230194, 0.01287275, 0.01305804],\n      dtype=float32)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#training-headless","title":"Training Headless","text":"<p>To train more efficiently, turn off the gui in gazebo. To do so, go to the .launch file that you have launched gazebo with and comment out the follwoing lines:</p> <pre><code>   IncludeLaunchDescription(\n       PythonLaunchDescriptionSource(\n           os.path.join(pkg_gazebo_ros, 'launch', 'gzclient.launch.py')\n       ),\n   ),\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html","title":"18. Application on Games(optional)","text":""},{"location":"unit6/lesson18/lesson18.html#lesson-18-rl-application-on-games","title":"Lesson 18: RL Application on Games","text":"<p>Unit 6: Learning Outcomes </p> <ol> <li>understand how to create a simple wrapper for a Gym environment to take advantage of its provided functionality</li> <li>understand how to integrate our previous classes with Gym to combine them in a powerful way</li> <li>appreciate the intricacy of applying RL to different domains such as games and robotics</li> <li>build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand</li> <li>understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play.</li> <li>understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</li> </ol> <p>Reading: The accompanying reading of this lesson is chapter 16 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson deals with gym openai. OPenAI Gym provides a rich set of libraries and environments to try our algorithms on. These include the Atari games the DQN used in their Nature paper to show that Deep Reinforcement Learning can supersede human performance on Atari games with the same generic model that is used in all of the presented games. </p> <p>This was a big and important breakthrough since, previously, RL applications stayed largely within simpler control environments and required, as it was customary at that time, manual feature extraction. So, when this paper showed that it is feasible to build an end-to-end RL model that can automatically extract useful features that can be used directly in an RL training algorithm without any human engineering or intervention, it was an important landmark in our field. Prior to that, there was the Tesauro Backgammon TDGammon as another success story which used a neural network to train a model to learn to play backgammon (see the chapter for more details). </p> <p>From that moment on, DeepMind went into a progressive trajectory of successful research and applications in the domain of deep reinforcement learning that reached an unprecedented level of beating the world champion in the game of Go see AlphaGo paper and then moved on to create a generic architecture that is capable of self-playing to reach superhuman levels in chess, Go and shogi that they called AlphaZero. Contrary to AlphaGo, AlphaZero did not use any human knowledge to train the model, and it completely started from scratch and let the AI train itself by itself! see here. </p> <p>Note that even the replay idea is an old idea in RL it just got rehashed and done a bit differently inside a buffer that allows us to conduct learning in mini-batches similar to what we do in supervised learning (since this was tried and tested by the ML community and it is proven to work very well with backpropagation). Yet, this idea was not new since other researchers have tried batches with RL. The paper's main flair is the impressive performance level that could be achieved via an adequately long period of training, a foot that has not been achieved before. Note that the replay buffer dictates the choice of an off-policy algorithm i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.</p> <p>We expect this trajectory to continue and that RL with robotics will create the next wave of innovation that will hopefully change how we conduct our daily lives. We hope this will lead to positive changes and prosperity in the long run, but that does not prevent mistakes. You will tackle this ethical side in another module. For now, enjoy dealing with the revolutionary side of AI that will change the world!</p> <p>Ok, let us get started...!</p>"},{"location":"unit6/lesson18/lesson18.html#openai-gym-classical-environment","title":"OpenAI Gym Classical Environment","text":"<p>We first tackle classical environments in OpenAI Gym as useful training for dealing with the library. In particular, we will start with their mountain car environment. We have already created our own class for this problem in the previous lesson, however, here, we will inherit their class, and we will override their rendering mechanism to be able to embed its visualisation in our notebook. </p> <p>This is a useful exercise to familiarise ourselves with Gym and verify our findings in the previous lesson. Therefore, we will repeat some of the experiments we conducted in the previous lesson. No commentary is provided as it replicates previous experiments on our newly created class that depends on Gym. Note that we have already set up our classes in previous lessons to be ready to integrate easily with Gym. We will show this in the following example.</p>"},{"location":"unit6/lesson18/lesson18.html#useful-resources","title":"Useful Resources","text":"<p>There are plenty of videos and resources which you can search online for. - You may want to follow the following tutorial which shows how to deal with tensorflow on cart pole but the input is not the frames. - You may then follow this tutorial which uses deep learning on the frames of a cart pole problem.</p> <ul> <li>You may find it useful to watch this video for a Keras tutorial with RL, or this video for a pytorch tutorial with RL. </li> <li>You may want to have a look at some tutorials.</li> </ul> <p>We start by showing you how to inherit from gym environment. We will do exactly what we did in the previous lesson regarding training a mountain car. This time we will utilise the openai gym environment rendering and </p> <pre><code>class MountainCar(MountainCarEnv):\n    def __init__(self, ntiles=8, **kw):   #  \u03c9: position window, rd: velocity window\n        super().__init__(**kw)\n\n        # constants                          \n        self.X0,  self.Xn  = -1.2, .5       # position range\n        self.Xv0, self.Xvn = -.07, .07      # velocity range\n        self.\u03b7 = 3                          # we rescale by 3 to get the wavy valley/hill\n\n        # for render()\n        self.X  = np.linspace(self.X0,  self.Xn, 100)     # car's position\n        self.Xv = np.linspace(self.Xv0, self.Xvn, 100)    # car's speed\n        self.Y  = np.sin(self.X*self.\u03b7)\n\n        # for state encoding (indexes)\n        self.ntiles  = ntiles\n        # number of states is nS*nSd but number of features is nS+nSd with an econdoing power of 2^(nS+nSd)&gt;&gt;nS*nSd!\n        self.nF = self.nS = 2*(self.ntiles+1)\n        self.nA = 3\n        # for compatability\n        self.Vstar = None\n\n        # reset\n        self.x, _ = super().reset()\n        self.xv = 0\n\n        # figure setup\n        self.figsize = (17, 3)\n        #plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n\n        self.render_mode=\"rgb_array\"\n\n    def s(self, tilings=1):\n        return (tilings*self.ntiles*(self.x  - self.X0 )/(self.Xn  - self.X0 )).astype(int)\n\n    def sv(self, tilings=1):\n        return int(tilings*self.ntiles*(self.xv - self.Xv0)/(self.Xvn - self.Xv0))\n\n    def reset(self):\n        self.x, _ = super().reset(seed=0)\n        self.xv = 0\n        return self.s_()\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        \u03c6[self.s()] = 1\n        \u03c6[self.sv() + self.ntiles] = 1\n        return \u03c6\n\n    # for compatibility\n    def S_(self):\n        return np.eye(self.nF)\n\n    def isatgoal(self):\n        return self.x&gt;=self.Xn\n\n\n    def step(self,a):\n        obs, r, done, _,_ = super().step(a)\n        self.x, self.xv = obs[0], obs[1]\n        return self.s_(), r, done, {}\n\n    def render(self, visible=True, pause=0, subplot=131, animate=True, **kw):\n\n        if not visible: return\n        self.ax0 = plt.subplot(subplot)\n        self.figsize = (17, 3)\n        plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n        plt.imshow(super().render())\n        plt.axis('off')\n\n        if animate: \n            clear_output(wait=True)\n            plt.show(); time.sleep(pause)\n</code></pre> <pre><code>%time sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.1/8, episodes=45, seed=1, animate=True, **demoTR()).interact()\n</code></pre> <pre><code>CPU times: user 42.4 s, sys: 622 ms, total: 43 s\nWall time: 44.1 s\n</code></pre> <p></p> <p>Note that we were only showing the end state in each episode and not the whole training process. We can make the process faster by not showing(animating) the training progress as we do below.</p> <pre><code>%time sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.1/8, episodes=45, seed=1, **demoTR()).interact()\n</code></pre> <pre><code>CPU times: user 27.5 s, sys: 347 ms, total: 27.9 s\nWall time: 28.3 s\n</code></pre> <p></p> <p>No exploration</p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True, plotR=True).interact()\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=16), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True, plotR=True).interact()\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=32), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True).interact()\nplt.yscale('log')\n</code></pre> <p></p> <pre><code>MountainCarRuns()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:40&lt;00:00,  5.01s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:25&lt;00:00,  4.29s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:08&lt;00:00,  3.44s/it]\n</code></pre> <p></p> <p>To run similar experiments that we did in the previous lesson we need to use the tiledMountainCar class. Below we restate the tiledMountainCar class that we developed in the previous lesson. No changes here except that it inherits now from the new MountainCar cvlass that uses gym MountainCarEnv class. We could have avoided this by putting tiledMountainCar in a class factory function. This is left for you as an exercise.</p> <pre><code>class tiledMountainCar(MountainCar):\n    def __init__(self, ntilings=1, **kw): #ntilings: is number of tiles\n        super().__init__(**kw)\n        self.ntilings = ntilings\n        self.dim = (self.ntilings, self.ntiles+2, self.ntiles+3) # the redundancy to mach the displacements of position(x) and velocity(xv)\n        self.nF = self.dim[0]*self.dim[1]*self.dim[2]\n\n\n    def inds(self):\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        inds = []\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n\n            inds.append((tiling,s,sv))\n\n        return inds\n\n    def s_(self):\n        \u03c6 = np.zeros(self.dim)\n        for ind in self.inds(): \n            \u03c6[ind]=1\n\n        return \u03c6.flatten()\n</code></pre> <pre><code>for n in trange(5):\n    SarsaOnMountainCar(ntilings=2**n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.42s/it]\n</code></pre> <p></p> <pre><code>MountainCarRuns(runs=10, env=IHTtiledMountainCar(ntilings=8,ntiles=8), label='with index hashed table for 8*8*8 tiles')\nMountainCarRuns(runs=10, env=tiledMountainCar(ntilings=8,ntiles=8), label='with 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:12&lt;00:00,  7.21s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:00&lt;00:00,  6.00s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:50&lt;00:00,  5.10s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:42&lt;00:00, 10.27s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:25&lt;00:00,  8.56s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.72s/it]\n</code></pre> <p></p> <pre><code>class IHTtiledMountainCar(tiledMountainCar):\n    def __init__(self, iht_size=1024, **kw): # by default we have 8*8*8 (position tiles * velocity tiles * tilings)\n        super().__init__(**kw)\n        self.nF = iht_size\n\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        inds = np.where(super().s_()!=0)[0]\n        \u03c6[inds%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>MountainCarTilings()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:31&lt;00:00,  4.58s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:24&lt;00:00,  4.25s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:25&lt;00:00,  4.30s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:36&lt;00:00,  4.82s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:56&lt;00:00,  5.85s/it]\n</code></pre> <p></p> <p>As we can see we got identical results to the same experiments that we ran in the previous lesson.</p>"},{"location":"unit6/lesson18/lesson18.html#openai-gym-atari-games-environments","title":"OpenAI Gym Atari Games Environments","text":"<p>Now we are ready to move to our Atari environment. To be able to follow the logic it is better if you read the Nature paper since most tutorial follow the deep learning architecture that the paper presented. Note that the deep neural network architecture is just three CNNs, nevertheless the paper pioneered and showed the effectiveness of combing deep learning with reinforcement learning. The paper has few inventions such as the experience replay buffer and the batch training something which had not been done successfully before in RL in such a context. The main contribution is to prove that RL is generic enough to be applied to learn from images just as human learn to play a game. Note that the algorithm needed to be an off-policy because of the usage of the experience replay buffer. The use of experience replay means that the agent is learning its current policy from an old experience that stemmed from following an old policy (previous version of its current policy). </p> <p>Note that in terms of theory we are still lagging to prove convergence of such approaches. This is where we can be uncertain which shadow of doubt on the ethical controllability of our own creation.</p>"},{"location":"unit6/lesson18/lesson18.html#atari-gym-wrapper","title":"Atari Gym Wrapper","text":"<p>Let us now build our own wrapper for Atari game so that we are able to apply Deep Q-Learning on the screen pixels coming from the environment. We have equipped our environment with necessary video storing as well as with pre-processing and to be able to handle the frames of the Gym environment. Note that we are dealing with environments that do not perform skipping to avoid the stochasticity associated with randomly applying actions. Therefore, we had to do the frame skipping in the wrapper class. It is also sufficient to deal with grey scale images not RGB since this will reduce the processing demands on our machines. Feel free to experiments with the GymEnv class to change its underlying preprocessing and or its assumptions.</p> <pre><code>def GymEnvi(Wrapper=gym.Wrapper):\n\n    class GymEnvi(Wrapper):\n        def __init__(self, env_name='ALE/Pong-v5', seed=0,   # 'PongNoimgskip-v4'\n                     nimgs_skip=4, nimgs_stack=1, img_size=(84, 84), \n                     video=True, i=0, animate=False, saveimg=False):\n\n            # if Wrapper==gym.Wrapper:\n            super().__init__(gym.make(env_name, render_mode='rgb_array'))\n\n            self.nA = 3 if 'Pong' in env_name else self.action_space.n \n            self.env_name = env_name\n            # seeding the game for consistency when testing\n            # self.env.seed(seed)\n\n            self.animate = animate\n            self.saveimg = saveimg\n            self.nimgs_skip = nimgs_skip      # how many imgs will be skipped to form one step\n            self.imgs_skip  = deque(maxlen=2)   # a buffer to store last few imgs that will take their max\n\n            self.nimgs_stack = nimgs_stack    # how many imgs will be put together to form a one state (we use first and last only)\n            self.img_size    = img_size       # (w,h)how much reduction will be applied on the original imgs\n            self.img_size_   = (*img_size, max(1,nimgs_stack)) # extended dimension of state space\n            self.imgs_stack  = deque(maxlen=nimgs_stack)\n\n\n            self.nS = 10 # for compatibility\n            self.i = i   # video number\n\n            # ineffective for compatibility \n            self.Vstar = None\n\n            # for rendering and video\n            self.ax0 = None\n            self.figsize = (20, 4)\n            self.video = video\n            self.video_imgs  = []\n            self.video_imgs_ = []\n\n        # self.s holds the latest img *after* preprocessing (state/observation seen by the agent)\n        def preprocess(self, img):\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img = cv2.resize(img, self.img_size) / 255\n            self.s = np.expand_dims(img, -1)\n            return self.s \n\n        # self.img holds the latest img *before* preprocessing (will be used for videos)\n        def step(self, a):\n            if a and 'Pong' in self.env_name: a+=1 # suitable for pong only\n            self.a = a\n            self.r = 0\n            # skip 4 imgs (apply same action) and stack last 2 imgs then take their max\n            for i in range(self.nimgs_skip):\n                img, r, self.done, _, _ = self.env.step(a)\n                self.r += r\n                self.imgs_skip.append(img)\n                if self.done: break\n\n            img = self.preprocess(np.stack(self.imgs_skip).max(axis=0))\n            self.imgs_stack.append(img)\n\n            # stack only the first and last imgs to save computation and to convey movement direction to the model\n            img_inds = [0,-1] if self.nimgs_stack&gt;1 else [0]\n            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n\n            return self.img, self.r, self.done, {}\n\n        def reset(self):\n            self.imgs_skip.clear()\n            self.imgs_stack.clear()\n\n            # reset the environment and retain its initial state (image) \n            img = self.env.reset()[0]\n            self.imgs_skip.append(img)\n            img = self.preprocess(img)\n\n            # now stack the same img n times for consistency with step()\n            for _ in range(self.nimgs_stack):\n                self.imgs_stack.append(img)\n\n            img_inds = [0,-1] if self.nimgs_stack&gt;1 else [0]\n            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n\n            return self.img \n\n        #------------------------------------------render \u270d\ufe0f-------------------------------------------------\n        def render(self, visible=True, pause=0, subplot=131,  animate=True, **kw):\n\n            if not visible: return\n\n            self.ax0 = plt.subplot(subplot)\n            plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n\n            # saving it as a video if needed, note that we render only at the last few episode\n            if self.video and animate:\n                self.video_imgs.append(self.img)\n                plt.axis('off')\n                # create the video at the end of the set of episodes\n                if self.done:\n                    for obsv in self.video_imgs: self.video_imgs_.append([plt.imshow(obsv, cmap=cm.Greys_r, animated=True)])\n                    anim = animation.ArtistAnimation(self.ax0.figure, self.video_imgs_, interval=50, blit=True, repeat_delay=1000)\n                    anim.save('atari%d.mp4'%self.i)\n            img_inds = [0, int(self.nimgs_stack/2), -1] if self.nimgs_stack &gt; 2 else [0]\n            img = np.dstack([self.imgs_stack[i] for i in img_inds])\n\n            plt.imshow(img)\n            plt.axis('off')\n            if animate:\n                clear_output(wait=True)\n                plt.show()\n                time.sleep(pause)\n\n    return GymEnvi\n\nGymEnv = GymEnvi()\n</code></pre> <p>Let us create some handy function to play an Atari games and observe the states.</p> <pre><code>def play(ep=1, env=GymEnv(nimgs_stack=3), render=True): # try nframes_stack=5 it is fun!\n    for ep in range(ep):\n        env.reset()\n        done=False\n        for _ in range(50):\n            s,r, done, _ = env.step(randint(3))\n            if render: env.render(pause=0)\n    print(s.shape)\n    return s\n</code></pre> <pre><code>s = play()\n</code></pre> <p></p> <pre><code>(84, 84, 2)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#rl-with-deep-learning","title":"RL with Deep Learning","text":"<p>It is time to extend our basic MRP class to handle function approximation using deep neural networks. Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the game under consideration. For example, in Pong, we can set this to 18. This means that on average (for the last 100 or 200 games/episodes), the opponent could only score only a max of 3 goals, and our agent scores the wining 21 goals.</p>"},{"location":"unit6/lesson18/lesson18.html#buffer-implementation","title":"Buffer Implementation","text":"<p>It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly.</p> <pre><code>from collections import deque\nbuffer = deque(maxlen=5)\nbuffer.append(1)\nbuffer.append(2)\nbuffer.append(3)\nbuffer.append(4)\nprint(buffer)\n</code></pre> <pre><code>deque([1, 2, 3, 4], maxlen=5)\n</code></pre> <pre><code>buffer.append(5)\nbuffer.append(6)\nbuffer.append(7)\nprint(buffer)\n</code></pre> <pre><code>deque([3, 4, 5, 6, 7], maxlen=5)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#sampling-form-the-buffer","title":"Sampling form the buffer","text":"<pre><code>import random\n\nrandom.sample(buffer,2)\n</code></pre> <pre><code>[4, 6]\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#buffer-with-complex-element-tuples","title":"Buffer with Complex Element (Tuples)","text":"<p>Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images.</p> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\n\nprint(buffer)\n</code></pre> <pre><code>deque([('2', 1, '3'), ('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4')], maxlen=4)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-mrp","title":"Deep MRP","text":"<p>In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_. We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific total number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y&lt;x. Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class.</p> <pre><code>class Deep_MRP(MRP):\n    def __init__(self, \n                 env=GymEnv(), \n                 \u03b3=0.99,\n                 nF=512, \n                 R_star=None, \n                 last=40,\n                 buffer_size=10000, \n                 batch_size=32, \n                 max_t_exp=int(1e6),\n                 load_weights=False,\n                 print_=False,\n                 **kw):\n\n        super().__init__(env=env, \u03b3=\u03b3, last=last, print_=print_, **kw)\n        self.nF           = nF # feature extraction is integrated within the deep learning model not the env\n        self.R_star       = R_star\n        self.buffer_size  = buffer_size\n        self.batch_size   = batch_size\n        self.load_weights_= load_weights\n        self.max_t_exp    = max_t_exp # used to stop learning\n\n    def init(self):\n        self.vN = self.create_model('V')                      # create V deep network\n        if self.load_weights_: self.load_weights(self.vN,'V') # from earlier training proces\n        self.vN.summary()\n\n        self.V = self.V_\n\n    #-------------------------------------------Deep model related---------------------------\n    def create_model(self, net_str):\n        print(f'model for {net_str} network is being loaded from disk........!')\n        x0 = Input((84,84,1))#self.env.frame_size_)\n        x = Conv2D(32, 8, 4, activation='relu')(x0)\n        x = Conv2D(64, 4, 2, activation='relu')(x)\n        x = Conv2D(64, 3, 1, activation='relu')(x)\n        x = Flatten()(x)\n        x = Dense(self.nF, 'relu')(x)\n        x = Dense(1 if net_str=='V' else self.env.nA)(x) \n        model = Model(x0, x)\n        model.compile(Adam(self.\u03b1), loss='mse')\n        return model\n\n    def load_weights(self, net, net_str ):\n        print(f'weights for {net_str} network are being loaded from disk........!')\n        loaded_weights = net.load_weights(net_str)\n        loaded_weights.assert_consumed()\n\n    def save_weights(self):\n        print(f'weights for V network are being saved to disk........!')\n        self.vN.save_weights('V')\n\n    #------------------------------------- value related \ud83e\udde0-----------------------------------\n    def V_(self, s):\n        if len(s.shape)!=4: \n            # this needs to be tested to make sure it give us the required dim\n            return self.vN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for \u03b5greedy to work well\n        return np.copy(self.vN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n\n    # ------------------------------------ experiments related --------------------------------\n    # overriding: we do not specify a preset number of episodes to stop the experiments\n    def stop_exp(self):  \n\n        if self.ep and self.R_star is not None and self.Rs[:self.ep].mean()&gt;= self.R_star: \n            print(f'target reward is achieved in {self.t_} steps!')\n            return True\n\n        if self.t_ &gt; self.max_t_exp:\n            print(f'max steps of {self.t_} is reached and training is stopped, weights are being saved!')\n            self.save_weights()\n            return True\n\n        # save model's weights at least 10 times during the life of the agent\n        if (self.t_+1)%int(.1*self.max_t_exp)==0: self.save_weights()\n        return False\n\n    # stop_ep must be overriden because otherwise it will stope when max_t is reached\n    def stop_ep(self, done): return done\n\n    #-------------------------------------------buffer related----------------------------------\n    def allocate(self):\n        self.buffer = deque(maxlen=self.buffer_size)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        self.buffer.append((s, a, rn, sn, done))\n\n    def sample(self):\n        # sample a set of batch_size tuples (each tuple has 5 items) without replacement \n        # zip the tuples into one tuple of 5 items and convert each item into a np array \n        # of size batch_size   \n        samples = [np.array(experience) for experience in zip(*sample(self.buffer, self.batch_size))]\n\n        # generate a set of indices handy for filtering, to be used in online()\n        inds = np.arange(self.batch_size)\n\n        return samples, inds\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-mdp","title":"Deep MDP","text":"<p>Now we create the Deep_MDP class which implements policy related functionality</p> <pre><code>class Deep_MDP(Deep_MRP):\n    def __init__(self, \n                 \u03b5=1.0,  \n                 \u03b5min=0.01, \n                 \u03b5T=200000,    # linear decay by default\n                 t_qNn=1000,   # update the target network every t_qNn steps\n                 create_vN=False,\n                 **kw):\n\n        super().__init__(**kw) \n        self.t_qNn = t_qNn \n        self.create_vN = create_vN\n        self.\u03b5 = \u03b5\n        self.\u03b5min = \u03b5min\n        self.\u03b5T = \u03b5T\n\n    def init(self):\n        self.create_vN: super().init()                        # to create also vN, suitable for actor-critic\n\n        self.qN  = self.create_model('Q')                     # create main policy network\n        self.qNn = self.create_model('Q')                     # create target network to estimate Q(sn)\n        if self.load_weights_: self.load_weights(self.qN,'Q') # from earlier training proces\n        self.qNn.set_weights(self.qN.get_weights())\n\n        self.qN.summary()\n\n        self.Q = self.Q_\n\n    def save_weights(self):\n        if self.create_vN: super().save_weights()\n        print(f'weights for Q network are being saved to disk........!')\n        self.qN.save_weights('Q')\n    #------------------------------------- policies related \ud83e\udde0-----------------------------------\n    def Q_(self, s):\n        if len(s.shape)!=4: \n            return self.qN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for \u03b5greedy to work well\n        return np.copy(self.qN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n\n    def Qn(self, sn):\n        return self.qNn.predict(sn)    \n    #-------------------------------------- \ud83d\udd0d conditions -----------------------------------------\n\n    def online_cond(self):  return len(self.buffer) &gt;= self.buffer_size\n    def target_cond(self):  return self.t_ % self.t_qNn==0\n\n    #-------------------------------------- \ud83e\udde0 deep nets updates ----------------------------------- \n    def update_before(self, *args): pass\n    def update_after(self, *args): pass\n    def update_online_net(self): pass    \n\n    # update the target network every now and then\n    def update_target_net(self):\n        if self.target_cond(): \n            print('assigning weights for target network.....')\n            self.qNn.set_weights(self.qN.get_weights())\n\n\n    #------------------------------------- \ud83c\udf16 online learning --------------------------------------       \n    def online(self, *args):\n        # make sure the buffer is full before doing any update or sampling\n        if self.online_cond(): \n            self.update_before(*args) \n            self.update_target_net()\n            self.update_online_net()\n            self.update_after(*args)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-q-learning-architecture","title":"Deep Q-Learning Architecture","text":"<p>Note that we need to set \u03b5 here otherwise it will be set by default to .1 in the parent class.</p> <pre><code>class DQN(Deep_MDP):\n    def __init__(self, \u03b1=1e-4, **kw): \n        print('--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n\n    #------------------------------- \ud83c\udf16 online learning ---------------------------------\n    # update the online network in every step using a batch\n    def update_online_net(self):\n        # sample a tuple batch: each componenet is a batch of items \n        #(ex. s is a set of states, a is a set of actions)\n        (s, a, rn, sn, dones), inds = self.sample() \n\n        # obtain the action-values estimation from the two networks \n        # and make sure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the Q-learning update rule\n        Qs[inds, a] = self.\u03b3*Qn.max(1) + rn\n        self.qN.fit(s, Qs, verbose=False)\n</code></pre> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time deepqlearn = DQN(env=GymEnv(), episodes=5, max_t_exp=20000, \u03b5T=500, buffer_size=10000, batch_size=32).interact() \n</code></pre> <pre><code>--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------\nmodel for Q network is being loaded from disk........!\nmodel for Q network is being loaded from disk........!\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 84, 84, 1)]       0\n\n conv2d_12 (Conv2D)          (None, 20, 20, 32)        2080\n\n conv2d_13 (Conv2D)          (None, 9, 9, 64)          32832\n\n conv2d_14 (Conv2D)          (None, 7, 7, 64)          36928\n\n flatten_4 (Flatten)         (None, 3136)              0\n\n dense_8 (Dense)             (None, 512)               1606144\n\n dense_9 (Dense)             (None, 3)                 1539\n\n=================================================================\nTotal params: 1679523 (6.41 MB)\nTrainable params: 1679523 (6.41 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nassigning weights for target network.....\n1/1 [==============================] - 0s 80ms/step\n1/1 [==============================] - 0s 81ms/step\n...\n</code></pre> <pre><code># play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n%time deepqlearn = DQN(env=GymEnv(),R_star=10, episodes=50, max_t_exp=200000, \u03b5T=50000, buffer_size=10000, batch_size=32).interact() \n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#double-dqn-learning","title":"Double DQN Learning","text":"<pre><code>class DDQN(DQN):\n    def __init__(self, \u03b1=1e-4, **kw):\n        print('----------- \ud83e\udde0 Double DQN is being set up \ud83e\udde0 ---------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n    #--------------------------- \ud83c\udf16 online learning -----------------------------\n    def update_online_net(self):\n        # sample a tuple batch: each componenet is a batch of items \n        #(ex. s is a set of states, a is a set of actions) \n        (s, a, rn, sn, dones), inds = self.sample()\n        # obtain the action-values estimation from the two networks \n        # and make sure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the *Double* Q-learning update rule\n        # this is where the max estimations are decoupled from the max action selections\n        an_max = self.Q(sn).argmax(1)\n        Qs[inds, a] = self.\u03b3*Qn[inds, an_max] + rn\n        self.qN.fit(s, Qs, verbose=0)\n</code></pre> <pre><code># play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n%time doubledeepqlearn = DDQN(env=GymEnv(), episodes=20, buffer_size=10000, batch_size=32).interact() \n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#extracting-features-via-auto-encoders","title":"Extracting Features via Auto-Encoders","text":"<p>In this section we show how to use the latent variables of an auto-encoder in order to extract useful features from the frames grabbed from the games. This will allow us to apply previously covered algorithms that can be applied on linear models. For example we can take the latent variables and use them as the input for true online TD(\\(\\lambda\\)).  Refer to keras auto-encoder tutorial.</p>"},{"location":"unit6/lesson18/lesson18.html#build-a-dataset-from-a-game","title":"Build a dataset from a game","text":"<p>Let us collect a dataset from our game by assigning the max_t_exp to be as large as the buffer_size so that the games stops when the buffer is almost full.</p> <pre><code>dqn = DQN(env=GymEnv(), buffer_size=41000, max_t_exp=40000).interact() \n</code></pre> <p>Now extract the data frames from the buffer.</p> <pre><code>experience = [np.array(experience) for experience in zip(*dqn.buffer)]\nframes = experience[0]\nx_train, x_test = train_test_split(frames, test_size=0.3, random_state=42)\nprint('training data size', x_train.shape)\nprint('testing data size', x_test.shape)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#conclusion","title":"Conclusion","text":"<p>In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. </p> <p>You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. </p>"},{"location":"unit6/lesson18/lesson18.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulation of RL that we use a function approximation to represent the stare space which can be continuous and infinite.</p> <p>We only expect that the interest is going to continue to grow and that RL with robotics will create the next wave of innovation that will hopefully change the way we conduct our daily lives. We hope that this will lead to positive changes and to prosperity in the long run but that does not prevent mistakes. You will tackle this ethical side in another module, for now enjoy dealing with revolutionary side of AI that will change the world!</p> <p>Congratulations on completing this last unit on RL!</p>"},{"location":"unit6/lesson18/lesson18.html#discussion-and-activity","title":"Discussion and Activity","text":"<p>Read the following classic Nips paper and Nature paper and discuss it in the discussion forum.</p>"},{"location":"unit6/lesson18/lesson18.html#extra-resources","title":"Extra Resources","text":"<ul> <li>You may find see this series of talks about the future of AI by Stuart Russell interesting.</li> <li> <p>If you are intersted in self-driving cars, then:</p> <ul> <li>See the ALVIN paper for an early stage vehicle road control neural network, it is an early precursor of the current road systems that control autonomous vehicle. A lot of the new systems retain some similarities with this systems. </li> <li>See this video of the history of this system. </li> <li>See this video lecture of the topic autonomous vehicle driving.</li> </ul> </li> <li> <p>Carla Autonomous Vehicle Simulation</p> <ul> <li>See this video for Carla simulation tutorial 1</li> <li>See this video for Carla simulation tutorial 1</li> </ul> </li> </ul>"},{"location":"unit6/lesson18/lesson18.html#your-turn","title":"Your turn","text":"<ol> <li> <p>try to apply the same concept on other simple environments provided by Gym such as the acrobot.</p> </li> <li> <p>apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum.</p> </li> <li> <p>try to think of way to adopt a more complex neural network architecture that uses a more advanced CNN block in order to advance the state of the art in computer vision such as EfficentNetV2. As a starting point, we should note that this kind of neural networks is designed for classification. I.e. is there a way to deal with RL algorithms as a classification problem not a regression of estimating the value function v?.</p> </li> </ol>"},{"location":"unit6/lesson18/lesson18.html#challenge","title":"Challenge++","text":"<ol> <li>check the implementation of the deep network in the tutorial and try to integrate it into the provided infrastructure.</li> </ol>"}]}